<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Advanced Regression Topics | MBA 8350: Analyzing and Leveraging Data   The Course Companion</title>
  <meta name="description" content="This is a course companion for MBA 8350." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Advanced Regression Topics | MBA 8350: Analyzing and Leveraging Data   The Course Companion" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a course companion for MBA 8350." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Advanced Regression Topics | MBA 8350: Analyzing and Leveraging Data   The Course Companion" />
  
  <meta name="twitter:description" content="This is a course companion for MBA 8350." />
  

<meta name="author" content="Scott Dressler" />


<meta name="date" content="2021-06-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="MLR.html"/>

<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MBA 8350 Course Companion</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#about-this-book"><i class="fa fa-check"></i>About this book…</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#the-big-picture-of-statistics"><i class="fa fa-check"></i><b>1.1</b> The “Big Picture” of Statistics</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#the-vocabulary-of-statistics"><i class="fa fa-check"></i><b>1.2</b> The Vocabulary of Statistics</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#descriptive-measures"><i class="fa fa-check"></i><b>1.3</b> Descriptive Measures</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#central-tendency"><i class="fa fa-check"></i><b>1.3.1</b> Central Tendency</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#variation"><i class="fa fa-check"></i><b>1.3.2</b> Variation</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#measures-of-shape"><i class="fa fa-check"></i><b>1.3.3</b> Measures of shape</a></li>
<li class="chapter" data-level="1.3.4" data-path="intro.html"><a href="intro.html#covariance-and-correlation"><i class="fa fa-check"></i><b>1.3.4</b> Covariance and Correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>2</b> Data Collection and Sampling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="Data.html"><a href="Data.html#sampling-distributions"><i class="fa fa-check"></i><b>2.1</b> Sampling Distributions</a></li>
<li class="chapter" data-level="2.2" data-path="Data.html"><a href="Data.html#sampling-bias---two-examples"><i class="fa fa-check"></i><b>2.2</b> Sampling Bias - two examples</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="Data.html"><a href="Data.html#dewey-defeats-truman"><i class="fa fa-check"></i><b>2.2.1</b> Dewey Defeats Truman?</a></li>
<li class="chapter" data-level="2.2.2" data-path="Data.html"><a href="Data.html#section-1"><i class="fa fa-check"></i><b>2.2.2</b> 98.6?</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="Data.html"><a href="Data.html#sampling-methods"><i class="fa fa-check"></i><b>2.3</b> Sampling Methods</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="Data.html"><a href="Data.html#simple-random-sampling"><i class="fa fa-check"></i><b>2.3.1</b> Simple random sampling</a></li>
<li class="chapter" data-level="2.3.2" data-path="Data.html"><a href="Data.html#systematic-sampling"><i class="fa fa-check"></i><b>2.3.2</b> Systematic Sampling</a></li>
<li class="chapter" data-level="2.3.3" data-path="Data.html"><a href="Data.html#stratified-sampling"><i class="fa fa-check"></i><b>2.3.3</b> Stratified Sampling</a></li>
<li class="chapter" data-level="2.3.4" data-path="Data.html"><a href="Data.html#cluster-sampling"><i class="fa fa-check"></i><b>2.3.4</b> Cluster Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="Data.html"><a href="Data.html#sampling-in-practice"><i class="fa fa-check"></i><b>2.4</b> Sampling in Practice</a></li>
<li class="chapter" data-level="2.5" data-path="Data.html"><a href="Data.html#sampling-and-sampling-distributions"><i class="fa fa-check"></i><b>2.5</b> Sampling and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="Data.html"><a href="Data.html#an-application"><i class="fa fa-check"></i><b>2.5.1</b> An Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="R.html"><a href="R.html"><i class="fa fa-check"></i><b>3</b> Getting Started with R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="R.html"><a href="R.html#the-r-project-for-statistical-computing"><i class="fa fa-check"></i><b>3.1</b> The R Project for Statistical Computing</a></li>
<li class="chapter" data-level="3.2" data-path="R.html"><a href="R.html#downloading-and-installing-r"><i class="fa fa-check"></i><b>3.2</b> Downloading and installing R</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="R.html"><a href="R.html#choosing-a-mirror"><i class="fa fa-check"></i><b>3.2.1</b> Choosing a <em>Mirror</em></a></li>
<li class="chapter" data-level="3.2.2" data-path="R.html"><a href="R.html#download-and-install-the-correct-version"><i class="fa fa-check"></i><b>3.2.2</b> Download and install the correct version</a></li>
<li class="chapter" data-level="3.2.3" data-path="R.html"><a href="R.html#downloading-and-installing-rstudio"><i class="fa fa-check"></i><b>3.2.3</b> Downloading and installing RStudio</a></li>
<li class="chapter" data-level="3.2.4" data-path="R.html"><a href="R.html#taking-stock"><i class="fa fa-check"></i><b>3.2.4</b> Taking Stock</a></li>
<li class="chapter" data-level="3.2.5" data-path="R.html"><a href="R.html#installing-packages"><i class="fa fa-check"></i><b>3.2.5</b> Installing <em>Packages</em></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="R.html"><a href="R.html#coding-basics"><i class="fa fa-check"></i><b>3.3</b> Coding Basics</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="R.html"><a href="R.html#assigning-objects"><i class="fa fa-check"></i><b>3.3.1</b> Assigning Objects</a></li>
<li class="chapter" data-level="3.3.2" data-path="R.html"><a href="R.html#listing-adding-and-removing"><i class="fa fa-check"></i><b>3.3.2</b> Listing, Adding, and Removing</a></li>
<li class="chapter" data-level="3.3.3" data-path="R.html"><a href="R.html#loading-data"><i class="fa fa-check"></i><b>3.3.3</b> Loading Data</a></li>
<li class="chapter" data-level="3.3.4" data-path="R.html"><a href="R.html#manipulating-data"><i class="fa fa-check"></i><b>3.3.4</b> Manipulating Data</a></li>
<li class="chapter" data-level="3.3.5" data-path="R.html"><a href="R.html#subsetting-data"><i class="fa fa-check"></i><b>3.3.5</b> Subsetting Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="R.html"><a href="R.html#data-visualization"><i class="fa fa-check"></i><b>3.4</b> Data Visualization</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="R.html"><a href="R.html#histograms"><i class="fa fa-check"></i><b>3.4.1</b> Histograms</a></li>
<li class="chapter" data-level="3.4.2" data-path="R.html"><a href="R.html#line-bar-and-scatter-plots"><i class="fa fa-check"></i><b>3.4.2</b> Line, bar, and Scatter Plots</a></li>
<li class="chapter" data-level="3.4.3" data-path="R.html"><a href="R.html#boxplots"><i class="fa fa-check"></i><b>3.4.3</b> Boxplots</a></li>
<li class="chapter" data-level="3.4.4" data-path="R.html"><a href="R.html#much-more-out-there"><i class="fa fa-check"></i><b>3.4.4</b> Much more out there</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CLT.html"><a href="CLT.html"><i class="fa fa-check"></i><b>4</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="4.1" data-path="CLT.html"><a href="CLT.html#the-clt-formally"><i class="fa fa-check"></i><b>4.1</b> The CLT (Formally)</a></li>
<li class="chapter" data-level="4.2" data-path="CLT.html"><a href="CLT.html#application-1-a-sampling-distribution-with-a-known-population"><i class="fa fa-check"></i><b>4.2</b> Application 1: A Sampling Distribution with a Known Population</a></li>
<li class="chapter" data-level="4.3" data-path="CLT.html"><a href="CLT.html#application-2-a-sampling-distribution-with-an-unknown-population"><i class="fa fa-check"></i><b>4.3</b> Application 2: A Sampling Distribution with an Unknown Population</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="CLT.html"><a href="CLT.html#the-sample"><i class="fa fa-check"></i><b>4.3.1</b> The Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="CLT.html"><a href="CLT.html#the-punchline-1"><i class="fa fa-check"></i><b>4.4</b> The Punchline</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.1" data-path="CI.html"><a href="CI.html#a-refresher-on-probability"><i class="fa fa-check"></i><b>5.1</b> A Refresher on Probability</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="CI.html"><a href="CI.html#application-1"><i class="fa fa-check"></i><b>5.1.1</b> Application 1</a></li>
<li class="chapter" data-level="5.1.2" data-path="CI.html"><a href="CI.html#application-2"><i class="fa fa-check"></i><b>5.1.2</b> Application 2</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="CI.html"><a href="CI.html#deriving-a-confidence-interval"><i class="fa fa-check"></i><b>5.2</b> Deriving a Confidence Interval</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="CI.html"><a href="CI.html#application-3"><i class="fa fa-check"></i><b>5.2.1</b> Application 3</a></li>
<li class="chapter" data-level="5.2.2" data-path="CI.html"><a href="CI.html#what-if-we-want-to-change-confidence"><i class="fa fa-check"></i><b>5.2.2</b> What if we want to change confidence?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="CI.html"><a href="CI.html#what-to-do-when-we-do-not-know-sigma"><i class="fa fa-check"></i><b>5.3</b> What to do when we do not know <span class="math inline">\(\sigma\)</span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="CI.html"><a href="CI.html#t-distribution-versus-z-distribution"><i class="fa fa-check"></i><b>5.3.1</b> t distribution versus Z distribution…</a></li>
<li class="chapter" data-level="5.3.2" data-path="CI.html"><a href="CI.html#application-4"><i class="fa fa-check"></i><b>5.3.2</b> Application 4</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="CI.html"><a href="CI.html#determining-sample-size"><i class="fa fa-check"></i><b>5.4</b> Determining Sample Size</a></li>
<li class="chapter" data-level="5.5" data-path="CI.html"><a href="CI.html#concluding-applications"><i class="fa fa-check"></i><b>5.5</b> Concluding Applications</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="CI.html"><a href="CI.html#light-bulbs-last-time"><i class="fa fa-check"></i><b>5.5.1</b> Light Bulbs (Last Time)</a></li>
<li class="chapter" data-level="5.5.2" data-path="CI.html"><a href="CI.html#returning-to-the-philadelphia-school-policy-application"><i class="fa fa-check"></i><b>5.5.2</b> Returning to the Philadelphia School Policy Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="HT.html"><a href="HT.html"><i class="fa fa-check"></i><b>6</b> Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="6.1" data-path="HT.html"><a href="HT.html#anatomy-of-a-hypothesis-test"><i class="fa fa-check"></i><b>6.1</b> Anatomy of a Hypothesis Test</a></li>
<li class="chapter" data-level="6.2" data-path="HT.html"><a href="HT.html#two-methods-for-conducting-a-hypothesis-test-when-sigma-is-known"><i class="fa fa-check"></i><b>6.2</b> Two methods for conducting a hypothesis test (when <span class="math inline">\(\sigma\)</span> is known)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="HT.html"><a href="HT.html#rejection-region-method"><i class="fa fa-check"></i><b>6.2.1</b> Rejection Region Method</a></li>
<li class="chapter" data-level="6.2.2" data-path="HT.html"><a href="HT.html#p-value-approach"><i class="fa fa-check"></i><b>6.2.2</b> P-value Approach</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="HT.html"><a href="HT.html#two-sided-vs-one-sided-test"><i class="fa fa-check"></i><b>6.3</b> Two-sided vs One-sided Test</a></li>
<li class="chapter" data-level="6.4" data-path="HT.html"><a href="HT.html#conducting-a-hypothesis-test-when-sigma-is-unknown"><i class="fa fa-check"></i><b>6.4</b> Conducting a hypothesis test (when <span class="math inline">\(\sigma\)</span> is unknown)</a></li>
<li class="chapter" data-level="6.5" data-path="HT.html"><a href="HT.html#appendix-a-note-on-calculating-p-values"><i class="fa fa-check"></i><b>6.5</b> Appendix: A note on calculating P-values</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="HT.html"><a href="HT.html#the-problem"><i class="fa fa-check"></i><b>6.5.1</b> The Problem</a></li>
<li class="chapter" data-level="6.5.2" data-path="HT.html"><a href="HT.html#how-to-calculate-p-values"><i class="fa fa-check"></i><b>6.5.2</b> How to calculate p-values</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="SLR.html"><a href="SLR.html"><i class="fa fa-check"></i><b>7</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="SLR.html"><a href="SLR.html#a-simple-linear-regression-model"><i class="fa fa-check"></i><b>7.1</b> A Simple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="SLR.html"><a href="SLR.html#what-does-a-regression-model-imply"><i class="fa fa-check"></i><b>7.1.1</b> What does a regression model imply?</a></li>
<li class="chapter" data-level="7.1.2" data-path="SLR.html"><a href="SLR.html#the-real-simple-linear-regression-model"><i class="fa fa-check"></i><b>7.1.2</b> The <em>REAL</em> Simple Linear Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="SLR.html"><a href="SLR.html#application-predicting-house-price-based-on-house-size"><i class="fa fa-check"></i><b>7.2</b> Application: Predicting House Price Based on House Size</a></li>
<li class="chapter" data-level="7.3" data-path="SLR.html"><a href="SLR.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>7.3</b> Ordinary Least Squares (OLS)</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="SLR.html"><a href="SLR.html#b.l.u.e."><i class="fa fa-check"></i><b>7.3.1</b> B.L.U.E.</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="SLR.html"><a href="SLR.html#decomposition-of-variance"><i class="fa fa-check"></i><b>7.4</b> Decomposition of Variance</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="SLR.html"><a href="SLR.html#the-r2"><i class="fa fa-check"></i><b>7.4.1</b> The <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="7.4.2" data-path="SLR.html"><a href="SLR.html#what-is-a-good-r2"><i class="fa fa-check"></i><b>7.4.2</b> What is a <em>good</em> <span class="math inline">\(R^2\)</span>?</a></li>
<li class="chapter" data-level="7.4.3" data-path="SLR.html"><a href="SLR.html#standard-error-of-the-estimate"><i class="fa fa-check"></i><b>7.4.3</b> Standard Error of the Estimate</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="SLR.html"><a href="SLR.html#assumptions-of-the-linear-regression-model"><i class="fa fa-check"></i><b>7.5</b> Assumptions of the Linear Regression Model</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="SLR.html"><a href="SLR.html#linearity"><i class="fa fa-check"></i><b>7.5.1</b> Linearity</a></li>
<li class="chapter" data-level="7.5.2" data-path="SLR.html"><a href="SLR.html#independence-of-errors"><i class="fa fa-check"></i><b>7.5.2</b> Independence of Errors</a></li>
<li class="chapter" data-level="7.5.3" data-path="SLR.html"><a href="SLR.html#equal-variance"><i class="fa fa-check"></i><b>7.5.3</b> Equal Variance</a></li>
<li class="chapter" data-level="7.5.4" data-path="SLR.html"><a href="SLR.html#normality-of-errors"><i class="fa fa-check"></i><b>7.5.4</b> Normality of Errors</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="SLR.html"><a href="SLR.html#statistical-inference"><i class="fa fa-check"></i><b>7.6</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="SLR.html"><a href="SLR.html#confidence-intervals-around-population-parameters"><i class="fa fa-check"></i><b>7.6.1</b> Confidence Intervals (around population parameters)</a></li>
<li class="chapter" data-level="7.6.2" data-path="SLR.html"><a href="SLR.html#hypothesis-tests"><i class="fa fa-check"></i><b>7.6.2</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="7.6.3" data-path="SLR.html"><a href="SLR.html#confidence-intervals-around-forecasts"><i class="fa fa-check"></i><b>7.6.3</b> Confidence Intervals (around forecasts)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="MLR.html"><a href="MLR.html"><i class="fa fa-check"></i><b>8</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="MLR.html"><a href="MLR.html#application-explaining-house-price-in-a-multiple-regression"><i class="fa fa-check"></i><b>8.1</b> Application: Explaining house price in a multiple regression</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="MLR.html"><a href="MLR.html#the-importance-of-controls"><i class="fa fa-check"></i><b>8.1.1</b> The Importance of “Controls”</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="MLR.html"><a href="MLR.html#adjusted-r2"><i class="fa fa-check"></i><b>8.2</b> Adjusted <span class="math inline">\(R^2\)</span></a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="MLR.html"><a href="MLR.html#abusing-an-r2"><i class="fa fa-check"></i><b>8.2.1</b> Abusing an <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="8.2.2" data-path="MLR.html"><a href="MLR.html#an-adjusted-r2"><i class="fa fa-check"></i><b>8.2.2</b> An <em>Adjusted</em> <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="MLR.html"><a href="MLR.html#qualitative-dummy-variables"><i class="fa fa-check"></i><b>8.3</b> Qualitative (Dummy) Variables</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="MLR.html"><a href="MLR.html#intercept-dummy-variable"><i class="fa fa-check"></i><b>8.3.1</b> Intercept dummy variable</a></li>
<li class="chapter" data-level="8.3.2" data-path="MLR.html"><a href="MLR.html#slope-dummy-variable"><i class="fa fa-check"></i><b>8.3.2</b> Slope dummy variable</a></li>
<li class="chapter" data-level="8.3.3" data-path="MLR.html"><a href="MLR.html#what-if-there-are-more-than-two-categories"><i class="fa fa-check"></i><b>8.3.3</b> What if there are more than two categories?</a></li>
<li class="chapter" data-level="8.3.4" data-path="MLR.html"><a href="MLR.html#a-final-application"><i class="fa fa-check"></i><b>8.3.4</b> A Final Application</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="MLR.html"><a href="MLR.html#joint-hypothesis-tests"><i class="fa fa-check"></i><b>8.4</b> Joint Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="MLR.html"><a href="MLR.html#simple-hypothesis-tests"><i class="fa fa-check"></i><b>8.4.1</b> Simple Hypothesis Tests</a></li>
<li class="chapter" data-level="8.4.2" data-path="MLR.html"><a href="MLR.html#simple-versus-joint-tests"><i class="fa fa-check"></i><b>8.4.2</b> Simple versus Joint Tests</a></li>
<li class="chapter" data-level="8.4.3" data-path="MLR.html"><a href="MLR.html#applications"><i class="fa fa-check"></i><b>8.4.3</b> Applications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Advanced.html"><a href="Advanced.html"><i class="fa fa-check"></i><b>9</b> Advanced Regression Topics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="Advanced.html"><a href="Advanced.html#nonlinear-models"><i class="fa fa-check"></i><b>9.1</b> Nonlinear Models</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="Advanced.html"><a href="Advanced.html#derivatives"><i class="fa fa-check"></i><b>9.1.1</b> Derivatives</a></li>
<li class="chapter" data-level="9.1.2" data-path="Advanced.html"><a href="Advanced.html#why-consider-non-linear-relationships"><i class="fa fa-check"></i><b>9.1.2</b> Why consider non-linear relationships?</a></li>
<li class="chapter" data-level="9.1.3" data-path="Advanced.html"><a href="Advanced.html#functional-forms"><i class="fa fa-check"></i><b>9.1.3</b> Functional Forms</a></li>
<li class="chapter" data-level="9.1.4" data-path="Advanced.html"><a href="Advanced.html#the-log-transformation"><i class="fa fa-check"></i><b>9.1.4</b> The Log transformation</a></li>
<li class="chapter" data-level="9.1.5" data-path="Advanced.html"><a href="Advanced.html#the-quadratic-transformation"><i class="fa fa-check"></i><b>9.1.5</b> The Quadratic transformation</a></li>
<li class="chapter" data-level="9.1.6" data-path="Advanced.html"><a href="Advanced.html#the-reciprocal-transformation"><i class="fa fa-check"></i><b>9.1.6</b> The Reciprocal transformation</a></li>
<li class="chapter" data-level="9.1.7" data-path="Advanced.html"><a href="Advanced.html#conclusion"><i class="fa fa-check"></i><b>9.1.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="Advanced.html"><a href="Advanced.html#collinearity"><i class="fa fa-check"></i><b>9.2</b> Collinearity</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="Advanced.html"><a href="Advanced.html#an-application-1"><i class="fa fa-check"></i><b>9.2.1</b> An Application</a></li>
<li class="chapter" data-level="9.2.2" data-path="Advanced.html"><a href="Advanced.html#what-does-collinearity-do-to-our-regression"><i class="fa fa-check"></i><b>9.2.2</b> What does Collinearity do to our regression?</a></li>
<li class="chapter" data-level="9.2.3" data-path="Advanced.html"><a href="Advanced.html#how-to-test-for-collinearity"><i class="fa fa-check"></i><b>9.2.3</b> How to test for Collinearity?</a></li>
<li class="chapter" data-level="9.2.4" data-path="Advanced.html"><a href="Advanced.html#an-application-2"><i class="fa fa-check"></i><b>9.2.4</b> An Application:</a></li>
<li class="chapter" data-level="9.2.5" data-path="Advanced.html"><a href="Advanced.html#how-do-we-remove-collinearity"><i class="fa fa-check"></i><b>9.2.5</b> How do we remove Collinearity?</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="Advanced.html"><a href="Advanced.html#heteroskedasticity"><i class="fa fa-check"></i><b>9.3</b> Heteroskedasticity</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="Advanced.html"><a href="Advanced.html#pure-versus-impure-heteroskedasticity"><i class="fa fa-check"></i><b>9.3.1</b> Pure versus Impure Heteroskedasticity</a></li>
<li class="chapter" data-level="9.3.2" data-path="Advanced.html"><a href="Advanced.html#consequences-of-heteroskedasticity"><i class="fa fa-check"></i><b>9.3.2</b> Consequences of Heteroskedasticity</a></li>
<li class="chapter" data-level="9.3.3" data-path="Advanced.html"><a href="Advanced.html#detection"><i class="fa fa-check"></i><b>9.3.3</b> Detection</a></li>
<li class="chapter" data-level="9.3.4" data-path="Advanced.html"><a href="Advanced.html#remedies"><i class="fa fa-check"></i><b>9.3.4</b> Remedies</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MBA 8350: Analyzing and Leveraging Data <br> The Course Companion</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Advanced" class="section level1" number="9">
<h1><span class="header-section-number">Chapter 9</span> Advanced Regression Topics</h1>
<p>This final chapter of the course companion deals with three advanced topics which will be addressed in turn.</p>
<ol style="list-style-type: decimal">
<li><p>Nonlinear Models</p></li>
<li><p>Collinearity</p></li>
<li><p>Heteroskedasticity</p></li>
</ol>
<div id="nonlinear-models" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Nonlinear Models</h2>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \varepsilon_i\]</span></p>
<p>The regression model we have studied thus far has two main features. First, the model is linear in the coefficients. This important property allows us to estimate the model using OLS. Second the model is linear in the variables. This property imposes a linear relationship between the dependent and independent variables. In other words, the relationship is a straight line.</p>
<p>A linear relationship between the independent and a dependent variable results in a slope that is <em>constant</em>.</p>
<p><span class="math display">\[\beta_1 = \frac{\Delta Y_i}{\Delta X_{1i}}\]</span></p>
<p>This means that (holding <span class="math inline">\(X_{2i}\)</span> constant, of course) the expected value of <span class="math inline">\(Y\)</span> will increase by <span class="math inline">\(\beta_1\)</span> units in response to any unit-increase in <span class="math inline">\(X_{1i}\)</span>. The same increase occurs on average no matter where in the range of the independent variable you are. Sometimes this assumption is valid if the range of the independent variable is small enough such that a constant slope is appropriate. Sometimes it isn’t. If this assumption is not valid, then we are committing a specification error even if we have included all of the necessary independent variables. The specification error involves the assumption of a linear model.</p>
<p>The types of models we will consider here will be non-linear in the variables but will still be linear in the coefficients. This means that we can still estimate the models using OLS, but we will be extending the model to uncover some highly non-linear relationships between the dependent and independent variables. We will do this by transforming the variables prior to estimation, and the names of these models are given by the types of transformations we perform. We then run a simple OLS estimation on the transformed variables, and <em>back-out</em> the non-linear relationships afterwards. This last bit is what will be new to us, but you will see that it only involves a brief refresher of… calculus.</p>
<div id="derivatives" class="section level3" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Derivatives</h3>
<p>In calculus, the <em>slope</em> of a function is a simplistic term for it’s <em>derivative</em>. Take for example the very general function <span class="math inline">\(f(X) = aX^b\)</span>. This function uses two parameters (<span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>) and one variable (<span class="math inline">\(X\)</span>) to return a number or function value <span class="math inline">\(f(X)\)</span>. If you think about it, this is exactly what the deterministic component of our regression does. When <span class="math inline">\(b\neq1\)</span>, this function is non-linear. Therefore, to determine the slope - the increase in the function value given a unit-increase in <span class="math inline">\(X\)</span> - we need to take the derivative. The general formula for a derivative is given by</p>
<p><span class="math display">\[\frac{\Delta f(X)}{\Delta X}=abX^{b-1}\]</span></p>
<p>Note that we have used this derivative formula before, only we used it when <span class="math inline">\(f(X)=Y\)</span> and the formula was linear (i.e. <span class="math inline">\(b=1\)</span>) and <span class="math inline">\(a = \beta\)</span>.</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i\]</span></p>
<p><span class="math display">\[\frac{\Delta Y_i}{\Delta X_i}=\beta_1\]</span></p>
</div>
<div id="why-consider-non-linear-relationships" class="section level3" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Why consider non-linear relationships?</h3>
<p>The models we will consider below will generally use this extension (and a few other calculus tools) to transform our variables and get at specific non-linear relationships. The reason we do this is to get at the <em>true</em> relationship between the dependent and independent variables. If the relationship is in fact linear, then none of these sophisticated models are necessary. If the relationship is not linear, then linear models are by definition incorrect and will deliver misleading results. The models in this section are therefore used only when the data requires them. Nobody wants to make a model more complicated than it needs to be.</p>
</div>
<div id="functional-forms" class="section level3" number="9.1.3">
<h3><span class="header-section-number">9.1.3</span> Functional Forms</h3>
<p>The sole purpose of introducing a sophisticated functional form into an otherwise straight-forward regression model is because the relationship between a dependent an independent variable is not linear <em>in the data</em>. Recall that a linear relationship is one where the slope is constant. If the slope is constant (say, <span class="math inline">\(\beta\)</span>), then a one-unit increase in <span class="math inline">\(X\)</span> will deliver a <span class="math inline">\(\beta\)</span> unit increase in <span class="math inline">\(Y\)</span> on average <em>no matter where in the range of X you are</em>. There are a lot of instances in the real world where this doesn’t make sense. Take for example the impact of apartment rental price on its size. One can imagine that if an apartment is small (e.g. 50 sq ft), then one might be willing to be a lot more for a slight increase in size. If an apartment is ridiculously huge (e.g. 5000 sq ft), then one might not be willing to pay anything for an increase in size. This means that the relationship between apartment rental price and size are conceptually non-linear - the slope is dependent upon the actual size of the apartment. However, if your data has a small range (e.g. between 300 and 400 sq ft), then you might never need to consider a non-linear relationship because a linear one does a good job of approximating the relationship that you observe. This chapter deals with situations where you observe a non-linear relationship in your actual data, so it needs to be modeled.</p>
<p>Once we have established that a relationship is non-linear, we next need to take a stand on what <em>type</em> of non-linear relationship we are attempting to uncover. Answering this question depends upon how you think the slope is going to behave.</p>
<ul>
<li><p>Is the slope not constant in unit changes, but constant in percentage changes?</p></li>
<li><p>Does the slope qualitatively change? In other words, is there a relationship between a dependent and independent variable that starts out as positive (or negative) and eventually turns negative (or positive)?</p></li>
<li><p>Does the slope start out positive or negative and eventually <em>dies out</em> (i.e., goes to zero)?</p></li>
</ul>
<p>This chapter details three types of non-linear transformations designed to go after one of these three scenarios. Non-linear transformations are not <em>one size fits all</em>, so having a good idea of how to handle each type of relationship is essential.</p>
</div>
<div id="the-log-transformation" class="section level3" number="9.1.4">
<h3><span class="header-section-number">9.1.4</span> The Log transformation</h3>
<p>The natural log transformation is used when the relationship between a dependent and independent variable is not constant in units but constant in percentage changes (or growth rates). Imagine putting $100 in a bank at 5 percent interest. If you kept the entire balance in the account, then after one year you will have $105 (a $5 increase), after two years you will have $110.25 (a $5.25 increase), after three years you will have $115.76 (a $5.76 increase), and so on. What is happening is that the account balance is not growing in constant dollar units, but it is growing in constant percentage units. In fact the balance is said to be growing <em>exponentially</em>. Things like a country’s output, aggregate prices, population all grow exponentially because they build on each other just like the compound interest story.</p>
<p>If we kept our $100 dollars in the bank for a very long time, the balance would evolve according to the figure below on the left. The figure illustrates a non-linear relationship between account balance and time - and the slope is getting steeper as time goes on. While we know that the account balance is increasing by larger and larger dollar increments, we also know that it is growing at a constant five percent. We can uncover this constant percentage change by applying the natural log to the balance - as we did to the right figure. You can see that the natural log function <em>straightens</em> the exponential relationship - so the transformed relationship is linear and ready for our regression model.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-155-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="the-derivative-of-the-log-function" class="section level4" number="9.1.4.1">
<h4><span class="header-section-number">9.1.4.1</span> The derivative of the log function</h4>
<p>The natural log function has a very specific and meaningful derivative:</p>
<p><span class="math display">\[\frac{dln(Y)}{dY} = \frac{\Delta Y}{Y}\]</span></p>
<p>This formula is actually a generalization of the percentage change formula. Suppose you wanted to know the difference between <span class="math inline">\(Y_2\)</span> and <span class="math inline">\(Y_1\)</span> in percentage terms relative to <span class="math inline">\(Y_1\)</span>. The answer is</p>
<p><span class="math display">\[\frac{Y_2 - Y_1}{Y_1} * 100\%\]</span></p>
<p>Therefore, the only thing missing from the log transformation is the multiplication of <span class="math inline">\(\%100\)</span>, which we can do after estimation.</p>
<p>For example, suppose that you didn’t know the <em>average</em> percentage change (or average growth rate) of your account. If Y was your account balance and X was number of years in the account, then you could estimate what it was. Notice that the <em>slope</em> is 0.05. If you multiply that by <span class="math inline">\(100\%\)</span> then you have your 5% interest rate back.</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="Advanced.html#cb293-1" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(Y)<span class="sc">~</span>X)</span>
<span id="cb293-2"><a href="Advanced.html#cb293-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pander</span>(<span class="fu">coef</span>(R))</span></code></pre></div>
<table style="width:29%;">
<colgroup>
<col width="19%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">(Intercept)</th>
<th align="center">X</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">4.61</td>
<td align="center">0.05</td>
</tr>
</tbody>
</table>
</div>
<div id="log-log-and-semi-log-models" class="section level4" number="9.1.4.2">
<h4><span class="header-section-number">9.1.4.2</span> Log-log and Semi-log models</h4>
<p>Recall that a standard slope is the <em>change</em> in Y over a <em>change</em> in X. Combine this fact with the log of a variable delivers a percentage change in the derivative (provided you multiply by <span class="math inline">\(100%\)</span>), and you have several options for which variables you want to consider the logs of. The question you ask yourself is if you want to consider the change of a variable in units of the percentage change of a variable.</p>
<div id="log-log-model" class="section level5" number="9.1.4.2.1">
<h5><span class="header-section-number">9.1.4.2.1</span> Log-log model</h5>
<p>A Log-log model is one where both the dependent and the independent variable are logged.</p>
<p><span class="math display">\[ln(Y_i)=\beta_0 + \beta_1 ln(X_i) + \varepsilon_i\]</span></p>
<p>The slope coefficient <span class="math inline">\((\beta_1)\)</span> details the percentage change in the dependent variable given a one <em>percent</em> change in the independent variable. To see this, apply the derivative formula above to the entire formula.</p>
<p><span class="math display">\[\frac{dln(Y_i)}{dY} = \beta_1 \frac{dln(X_i)}{dX}\]</span>
<span class="math display">\[\frac{dln(Y_i)}{dY} * 100\% = \beta_1 \frac{dln(X_i)}{dX} * 100\%\]</span>
<span class="math display">\[\%\Delta Y_i = \beta_1 \%\Delta X_i\]</span>
<span class="math display">\[ \beta_1 =\frac{\%\Delta Y_i}{\%\Delta X_i}\]</span></p>
</div>
<div id="semi-log-models" class="section level5" number="9.1.4.2.2">
<h5><span class="header-section-number">9.1.4.2.2</span> Semi-log models</h5>
<p>Sometimes it makes no sense to take the log of a variable because the percentage change makes no sense. For example, it wouldn’t make sense to take the log of the year in the bank account example above because time is not relative. In other words, a percentage change in time doesn’t make sense. In addition, variables the reach values of zero or lower <em>cannot</em> be logged because the natural log is only defined on positive values. In either case, it would only make sense to log only one of the variables.</p>
<p>A Log-lin model is a semi-log model where only the dependent variable is logged. This is like the case with the bank account example above.</p>
<p><span class="math display">\[ln(Y_i)=\beta_0 + \beta_1 X_i + \varepsilon_i\]</span></p>
<p><span class="math display">\[\frac{dln(Y_i)}{dY} = \beta_1 \Delta X\]</span>
<span class="math display">\[\frac{dln(Y_i)}{dY} * 100\% = (\beta_1 * 100\%)\Delta X\]</span>
<span class="math display">\[\frac{\% \Delta Y}{\Delta X}= \beta_1 * 100\%\]</span>
Note that the 100% we baked into the interpretation is explicitly accounted for in order to turn the derivative of the log function into a percentage change.</p>
<p>A Lin-Log model is a semi-log model where only the independent variable is logged. This might come in handy when you want to determine the average change in the dependent variable in response to a percentage-change in the independent variable.</p>
<p><span class="math display">\[Y_i=\beta_0 + \beta_1 ln(X_i) + \varepsilon_i\]</span></p>
<p><span class="math display">\[\Delta Y=\beta_1 \frac{dln(X_i)}{X_i}\]</span>
<span class="math display">\[\Delta Y=\beta_1 \frac{dln(X_i)}{X_i}*\frac{100}{100}\]</span>
<span class="math display">\[\Delta Y=\frac{\beta_1}{100} \%\Delta X\]</span></p>
<p><span class="math display">\[\frac{\Delta Y}{\%\Delta X}=\frac{\beta_1}{100} \]</span>
Note that the derivation for the lin-log model suggests that you must divide the estimated coefficient by 100 in order to state the expected change in the dependent variable due to a <em>percentage</em> change in the independent variable.</p>
</div>
<div id="it-isnt-all-or-nothing" class="section level5" number="9.1.4.2.3">
<h5><span class="header-section-number">9.1.4.2.3</span> It isn’t ALL OR NOTHING!!!</h5>
<p>To be clear, if you have a multiple regression model with several independent variables, you get to treat the variables however you wish. In other words, if you log one independent variable, you do not need to automatically log the others. This is especially the case when some can while others cannot be logged. The bottom line is that if you have one of the relationships detailed above with the dependent variable and a single independent variable, then you use the correct derivative form and provide the correct interpretation.</p>
<p>In particular, suppose you had the following model</p>
<p><span class="math display">\[ln(Y_i)=\beta_0 + \beta_1 X_{1i} + \beta_2 ln(X_{2i}) + \varepsilon_i\]</span></p>
<p>This model is a combination between a log-lin model (with respect to <span class="math inline">\(X_{1i}\)</span>) and a log-log model (with respect to <span class="math inline">\(X_{2i}\)</span>). The derivatives are therefore
<span class="math display">\[\beta_1 * 100\% = \frac{\% \Delta Y}{\Delta X_1}\]</span>
<span class="math display">\[\beta_2= \frac{\% \Delta Y}{\%\Delta X_2}\]</span></p>
</div>
</div>
<div id="application-5" class="section level4" number="9.1.4.3">
<h4><span class="header-section-number">9.1.4.3</span> Application</h4>
<p>If we ran a regression with hourly wage as the dependent variable and tenure (i.e., years on the job) as the independent variable, then we are estimating the average change in dollars for an additional year of tenure. However, it might be more worthwhile to consider an annual average percentage change in wage as opposed to a dollar change. That is what happens to most people, anyway.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="Advanced.html#cb294-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(wage1,<span class="at">package=</span><span class="st">&quot;wooldridge&quot;</span>)</span>
<span id="cb294-2"><a href="Advanced.html#cb294-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb294-3"><a href="Advanced.html#cb294-3" aria-hidden="true" tabindex="-1"></a>REG <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(wage) <span class="sc">~</span> tenure, <span class="at">data =</span> wage1)</span>
<span id="cb294-4"><a href="Advanced.html#cb294-4" aria-hidden="true" tabindex="-1"></a><span class="fu">pander</span>(<span class="fu">summary</span>(REG))</span></code></pre></div>
<table style="width:88%;">
<colgroup>
<col width="25%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">1.5</td>
<td align="center">0.03</td>
<td align="center">55.87</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>tenure</strong></td>
<td align="center">0.02</td>
<td align="center">0</td>
<td align="center">7.88</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<table style="width:86%;">
<caption>Fitting linear model: log(wage) ~ tenure</caption>
<colgroup>
<col width="20%" />
<col width="30%" />
<col width="11%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">526</td>
<td align="center">0.5</td>
<td align="center">0.11</td>
<td align="center">0.1</td>
</tr>
</tbody>
</table>
<p>The slope estimate gets multiplied by <span class="math inline">\(100\%\)</span> so we can state that wages increase by <span class="math inline">\(2\%\)</span> on average for every additional year of tenure.</p>
</div>
</div>
<div id="the-quadratic-transformation" class="section level3" number="9.1.5">
<h3><span class="header-section-number">9.1.5</span> The Quadratic transformation</h3>
<p>A quadratic transformation is used when the relationship between a dependent and independent variable <em>changes direction</em>. The slope can start off positive and become negative (the blue line in the figure), or begin negative and become positive (the red line).</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-158-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Quadratic models are designed to handle functions featuring slopes that change qualitatively.</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \varepsilon_i\]</span></p>
<p>Notice that this regression has one variable showing up twice: once linearly and once squared. The regression is still linear in the coefficients, so we can estimate this regression as if the regression contained any two independent variables of interest. In particular, if you defined a new variable <span class="math inline">\(X_{2i} = X_i^2\)</span>, then the model would look like a standard multiple regression. Once the estimated coefficients are obtained, they are combined to calculate one non-linear slope equation.</p>
<p><span class="math display">\[\frac{\Delta Y}{\Delta X} = \beta_1 + 2\beta_2  X\]</span></p>
<p>A few things to note about this slope equation.</p>
<ol style="list-style-type: decimal">
<li><p>It is a function of X - meaning that you need to choose a value of the independent variable to calculate a numerical slope. This means you are calculating the expected change in X from a unit increase in X <em>from a particular value</em>.</p></li>
<li><p>The slope is increasing and or decreasing depending on the values of the coefficients and the independent value of X. The coefficients <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are usually of opposite sign. Therefore, the slope is negative if <span class="math inline">\(\beta_1&lt;0\)</span> and X is small so <span class="math inline">\(\beta_1 + 2\beta_2 X&lt;0\)</span>, or positive is <span class="math inline">\(\beta_1&gt;0\)</span> and X is small so <span class="math inline">\(\beta_1 + 2\beta_2 X&gt;0\)</span>.</p></li>
</ol>
<p>Note: just as in the log transformation, this quadratic transformation does not need to be done to every independent variable in the regression. Only those that are suspected to have a relationship with the dependent variable that changes direction.</p>
<div id="application-6" class="section level4" number="9.1.5.1">
<h4><span class="header-section-number">9.1.5.1</span> Application</h4>
<p>Consider some simulated data illustrating the relationship between tax rates and the amount of tax revenue collected by the government. One can intuitively imagine that the government will collect zero revenue if they tax income at zero percent. However, they will also collect zero revenue if they tax at 100 percent because nobody will work if their entire income is taxed away. Therefore, there should be a relationship between tax rate and tax revenue that looks something like the figure below.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-159-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This figure illustrates the infamous Laffer Curve of supply-side economics. We are using this as an example of a quadratic relationship only - I will spare you my tirade on the detrimental use of this very intuitive idea by supply-siders.</p>
<p>Suppose you had the data illustrated in the figure. If you assumed a linear relationship between tax revenue and rate, then you will get a very misleading result.</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="Advanced.html#cb295-1" aria-hidden="true" tabindex="-1"></a>REG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Revenue <span class="sc">~</span> Rate)</span>
<span id="cb295-2"><a href="Advanced.html#cb295-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pander</span>(<span class="fu">summary</span>(REG))</span></code></pre></div>
<table style="width:88%;">
<colgroup>
<col width="25%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">1650</td>
<td align="center">151.7</td>
<td align="center">10.88</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>Rate</strong></td>
<td align="center">0</td>
<td align="center">2.62</td>
<td align="center">0</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<table style="width:86%;">
<caption>Fitting linear model: Revenue ~ Rate</caption>
<colgroup>
<col width="20%" />
<col width="30%" />
<col width="11%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">101</td>
<td align="center">767.8</td>
<td align="center">0</td>
<td align="center">-0.01</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="Advanced.html#cb296-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X,Y,</span>
<span id="cb296-2"><a href="Advanced.html#cb296-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;The Laffer Curve&quot;</span>,</span>
<span id="cb296-3"><a href="Advanced.html#cb296-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Income Tax Rate&quot;</span>,</span>
<span id="cb296-4"><a href="Advanced.html#cb296-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Tax Revenue&quot;</span>)</span>
<span id="cb296-5"><a href="Advanced.html#cb296-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(X,<span class="fu">fitted</span>(REG),<span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-160-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The figure shows that the best fitting straight line is <em>horizontal</em> - meaning that the slope is zero. This linear model would suggest that there is no relationship between tax revenue and tax rate. It is partly true - there is no <em>linear relationship</em>.</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="Advanced.html#cb297-1" aria-hidden="true" tabindex="-1"></a>REG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Revenue <span class="sc">~</span> Rate <span class="sc">+</span> <span class="fu">I</span>(Rate<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb297-2"><a href="Advanced.html#cb297-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pander</span>(<span class="fu">summary</span>(REG))</span></code></pre></div>
<table style="width:94%;">
<colgroup>
<col width="25%" />
<col width="15%" />
<col width="18%" />
<col width="18%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">-4.99</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>Rate</strong></td>
<td align="center">100</td>
<td align="center">0</td>
<td align="center">3.729e+15</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="center"><strong>I(Rate^2)</strong></td>
<td align="center">-1</td>
<td align="center">0</td>
<td align="center">-3.853e+15</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<table style="width:86%;">
<caption>Fitting linear model: Revenue ~ Rate + I(Rate^2)</caption>
<colgroup>
<col width="20%" />
<col width="30%" />
<col width="11%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">101</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="Advanced.html#cb298-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X,Y,</span>
<span id="cb298-2"><a href="Advanced.html#cb298-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;The Laffer Curve&quot;</span>,</span>
<span id="cb298-3"><a href="Advanced.html#cb298-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Income Tax Rate&quot;</span>,</span>
<span id="cb298-4"><a href="Advanced.html#cb298-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Tax Revenue&quot;</span>)</span>
<span id="cb298-5"><a href="Advanced.html#cb298-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(X,<span class="fu">fitted</span>(REG),<span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-161-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="the-reciprocal-transformation" class="section level3" number="9.1.6">
<h3><span class="header-section-number">9.1.6</span> The Reciprocal transformation</h3>
<p>Suppose a relationship doesn’t change directions as much as it <em>dies out</em>.</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 \frac{1}{X_i} + \varepsilon_i\]</span></p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-162-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As in the quadratic transformation, this model is easily estimated by redefining variables (i.e., <span class="math inline">\(X_{2i}=1/X_i\)</span>). The slope of this function can be obtained using our standard derivative function and noting that <span class="math inline">\(1/X_i = X_i^{-1}\)</span></p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 \frac{1}{X_i} + \varepsilon_i\]</span>
<span class="math display">\[Y_i = \beta_0 + \beta_1 X_i^{-1} + \varepsilon_i\]</span>
<span class="math display">\[\frac{\Delta Y}{\Delta X} = -\beta_1 X_i^{-2}=\frac{-\beta_1} {X_i^{2}}\]</span></p>
<p>Notice again that a value of the independent variable in needed to calculate the slope of the function at a specific point. However, as X gets larger, the slope approaches zero. A slope of zero is a horizontal line - and that is when the relationship between Y and X dies out. If <span class="math inline">\(\beta_1&gt;0\)</span> then the slope begins negative and approaches zero from above (the blue line). If <span class="math inline">\(\beta_1&lt;0\)</span> then the slope begins positive and approaches zero from below (the red line).</p>
<div id="application-7" class="section level4" number="9.1.6.1">
<h4><span class="header-section-number">9.1.6.1</span> Application</h4>
<p>The richer a nation is, the better a nation’s health services are. However, a nation can eventually get to a point that health outcomes cannot improve no matter how rich it gets. This application considers child mortality rates of developing and developed countries and uses the wealth of a country measured by per-capita gross national product (PGNP) to help explain why the child mortality rate is different across countries.</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="Advanced.html#cb299-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readxl)</span>
<span id="cb299-2"><a href="Advanced.html#cb299-2" aria-hidden="true" tabindex="-1"></a>CM <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;C:/Data/ECO3137/CM.xlsx&quot;</span>)</span>
<span id="cb299-3"><a href="Advanced.html#cb299-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb299-4"><a href="Advanced.html#cb299-4" aria-hidden="true" tabindex="-1"></a>REG <span class="ot">&lt;-</span> <span class="fu">lm</span>(CM<span class="sc">$</span>CM <span class="sc">~</span> <span class="fu">I</span>(<span class="dv">1</span><span class="sc">/</span>CM<span class="sc">$</span>PGNP))</span>
<span id="cb299-5"><a href="Advanced.html#cb299-5" aria-hidden="true" tabindex="-1"></a><span class="fu">pander</span>(<span class="fu">summary</span>(REG))</span></code></pre></div>
<table style="width:89%;">
<colgroup>
<col width="26%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">81.79</td>
<td align="center">10.83</td>
<td align="center">7.55</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>I(1/CM$PGNP)</strong></td>
<td align="center">27273</td>
<td align="center">3760</td>
<td align="center">7.25</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<table style="width:86%;">
<caption>Fitting linear model: CM<span class="math inline">\(CM ~ I(1/CM\)</span>PGNP)</caption>
<colgroup>
<col width="20%" />
<col width="30%" />
<col width="11%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">64</td>
<td align="center">56.33</td>
<td align="center">0.46</td>
<td align="center">0.45</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="Advanced.html#cb300-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(CM<span class="sc">$</span>PGNP,CM<span class="sc">$</span>CM,</span>
<span id="cb300-2"><a href="Advanced.html#cb300-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Child Mortality and PGNP&quot;</span>,</span>
<span id="cb300-3"><a href="Advanced.html#cb300-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Child Mortality (per 1000 births)&quot;</span>,</span>
<span id="cb300-4"><a href="Advanced.html#cb300-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Gross National Product (Per Capita)&quot;</span>,</span>
<span id="cb300-5"><a href="Advanced.html#cb300-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb300-6"><a href="Advanced.html#cb300-6" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(CM<span class="sc">$</span>PGNP,<span class="fu">fitted</span>(REG),<span class="at">col =</span> <span class="st">&quot;cyan&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-163-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The estimated coefficient is large and positive. However, this isn’t the entire slope, because you need to consider the derivative formula above.</p>
<p><span class="math display">\[\frac{\Delta Y}{\Delta X} = \frac{-27,273} {X_i^{2}}\]</span></p>
<p>If you wanted to consider the impact on child mortality of making a relatively poor nation richer, you plug a low value for PGNP like 100. If you want to consider a richer nation, consider a larger value like 1000.</p>
<p><span class="math display">\[\frac{\Delta Y}{\Delta X} = \frac{-27,273} {100^{2}} = -2.73\]</span>
<span class="math display">\[\frac{\Delta Y}{\Delta X} = \frac{-27,273} {1000^{2}} = -0.0273\]</span>
These calculations show that increasing the wealth of richer countries has a smaller impact on child mortality rates. This should make sense, and it takes a reciprocal model to capture it.</p>
</div>
</div>
<div id="conclusion" class="section level3" number="9.1.7">
<h3><span class="header-section-number">9.1.7</span> Conclusion</h3>
<p>This chapter introduced you to three different ways of adding potential non-linear relationships into our model while still preserving linearity in the coefficients. This allows us to retain our OLS estimation procedure, and only requires some formulaic calculus steps after estimation to get at our answers.</p>
<p>One take away is that one can easily map out a functional form in theory, but it might not be entirely captured by the data sample. In other words, while we can always tell a story that a relationship might become non-linear <em>eventually</em>, if that extreme range is not in the data then a non-linear relationship isn’t necessary.</p>
<p>On the other hand, if there is a non-linear relationship in the data, then it might be the case that more than one functional form might fit. While it is true that the three transformations handle different <em>right-side</em> behaviors, notice that the <em>left-side</em> of the relationships look quite similar.</p>
<p>Take the figure below for example. We do not have enough data to see if the relationship stays increasing (requiring a log transformation), changes direction (requiring a quadratic transformation), or dies out (requiring a reciprocal transformation). If this is the case then trail and error combined with a lot of care is required.</p>
<ul>
<li><p>Which model has the highest <span class="math inline">\(R^2\)</span>? (Provided that the dependent variable is not transformed.)</p></li>
<li><p>Which model makes the most sense theoretically?</p></li>
<li><p>What is the differences in the out-of-sample forecasts between models? What is the <em>cost</em> of being wrong?</p></li>
</ul>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="Advanced.html#cb301-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">45</span>,<span class="dv">1</span>)</span>
<span id="cb301-2"><a href="Advanced.html#cb301-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="sc">*</span> X <span class="sc">-</span> X<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb301-3"><a href="Advanced.html#cb301-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-4"><a href="Advanced.html#cb301-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X,Y,</span>
<span id="cb301-5"><a href="Advanced.html#cb301-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Which Relationship is This?&quot;</span>,</span>
<span id="cb301-6"><a href="Advanced.html#cb301-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;X&quot;</span>,</span>
<span id="cb301-7"><a href="Advanced.html#cb301-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Y&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-164-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The bottom line is that choosing the wrong non-linear transformation will still lead to some amount of specification bias, but it might not be as much as a linear specification.</p>
</div>
</div>
<div id="collinearity" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Collinearity</h2>
<p>A significant problem in regression analysis arises when two more independent variables are significantly correlated with each other. This is known as collinearity or multicollinearity. A correlation means that two or more variables systematically move together. In regression analysis, movement is <em>information</em> that we use to explain differences or changes in the dependent variable. If independent variables have similar movements due to correlations, then they have similar (i.e., redundant) information.</p>
<p>Another issue with collinearity is that when two or more variables systematically move together, then it goes against the very interpretation of our estimates: <em>holding all else equal.</em> If the variables aren’t held equal in the data due to collinearity, then our estimates will reflect that by being unable to differentiate the changes in these variables along separate dimensions. Since the information from these independent variables are shared and redundant, then the dimensions from these collinear variables becomes blurred.</p>
<div id="an-application-1" class="section level3" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> An Application</h3>
<p>Consider an application that compares simulated data where two independent variables have different degrees of correlation. The simulated data was generated from the following model:</p>
<p><span class="math display">\[Y_i = 1 + 1 X_{1i} + 1 X_{2i} + \varepsilon_i\]</span></p>
<p>In other words, the simulated data <strong>should</strong> return the same coefficients above <strong>if</strong> there are no problems with the estimation. The exercise will show you how collinearity can become a problem.</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="Advanced.html#cb302-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Regression: correlation = 0.3289</span></span>
<span id="cb302-2"><a href="Advanced.html#cb302-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb302-3"><a href="Advanced.html#cb302-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(MDAT<span class="sc">$</span>X31,MDAT<span class="sc">$</span>X32)</span></code></pre></div>
<pre><code>## [1] 0.3289358</code></pre>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="Advanced.html#cb304-1" aria-hidden="true" tabindex="-1"></a>CREG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y3<span class="sc">~</span>X31<span class="sc">+</span>X32,<span class="at">data=</span>MDAT)</span>
<span id="cb304-2"><a href="Advanced.html#cb304-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(CREG)</span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 1.0015735  0.0214445  46.705 &lt; 2.2e-16 ***
## X31         1.0152385  0.0401048  25.315 &lt; 2.2e-16 ***
## X32         0.9905016  0.0099267  99.781 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="Advanced.html#cb306-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Regression: correlation = 0.938</span></span>
<span id="cb306-2"><a href="Advanced.html#cb306-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-3"><a href="Advanced.html#cb306-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(MDAT<span class="sc">$</span>X21,MDAT<span class="sc">$</span>X22)</span></code></pre></div>
<pre><code>## [1] 0.9380521</code></pre>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="Advanced.html#cb308-1" aria-hidden="true" tabindex="-1"></a>CREG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y2<span class="sc">~</span>X21<span class="sc">+</span>X22,<span class="at">data=</span>MDAT)</span>
<span id="cb308-2"><a href="Advanced.html#cb308-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(CREG)</span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 1.001574   0.021445  46.705 &lt; 2.2e-16 ***
## X21         1.100724   0.109304  10.070 &lt; 2.2e-16 ***
## X22         0.905016   0.099267   9.117 1.082e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="Advanced.html#cb310-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Regression: correlation = 0.999</span></span>
<span id="cb310-2"><a href="Advanced.html#cb310-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb310-3"><a href="Advanced.html#cb310-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(MDAT<span class="sc">$</span>X11,MDAT<span class="sc">$</span>X12)</span></code></pre></div>
<pre><code>## [1] 0.9992777</code></pre>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="Advanced.html#cb312-1" aria-hidden="true" tabindex="-1"></a>CREG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y1<span class="sc">~</span>X11<span class="sc">+</span>X12,<span class="at">data=</span>MDAT)</span>
<span id="cb312-2"><a href="Advanced.html#cb312-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(CREG)</span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.001574   0.021445 46.7053  &lt; 2e-16 ***
## X11         1.955579   0.996657  1.9621  0.05261 .  
## X12         0.050161   0.992672  0.0505  0.95980    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="Advanced.html#cb314-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Regression: Highest correlation = 1</span></span>
<span id="cb314-2"><a href="Advanced.html#cb314-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb314-3"><a href="Advanced.html#cb314-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(MDAT<span class="sc">$</span>X41,MDAT<span class="sc">$</span>X42)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="Advanced.html#cb316-1" aria-hidden="true" tabindex="-1"></a>CREG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y4<span class="sc">~</span>X41<span class="sc">+</span>X42,<span class="at">data=</span>MDAT)</span>
<span id="cb316-2"><a href="Advanced.html#cb316-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(CREG)</span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 1.003483   0.021342  47.019 &lt; 2.2e-16 ***
## X41         2.002616   0.037857  52.900 &lt; 2.2e-16 ***
## X42               NA         NA      NA        NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The above application considers four sets of data where the only difference is the degree of collinearity between the two independent variables. The first regression has a degree of correlation between <span class="math inline">\(X_{1i}\)</span> and <span class="math inline">\(X_{2i}\)</span> equal to 0.33, and you will see that the regression does a fairly good job at recovering the regression coefficients. The second regression has a degree of correlation between <span class="math inline">\(X_{1i}\)</span> and <span class="math inline">\(X_{2i}\)</span> equal to 0.94, and you will see that the regression is beginning to suffer a bit where both slope estimates are now off by about 10 percent. The Third regression has a degree of correlation between <span class="math inline">\(X_{1i}\)</span> and <span class="math inline">\(X_{2i}\)</span> equal to just shy of perfect (1), and you will see that the regression is now <em>way</em> off from the expected estimates. Finally, the fourth regression has <strong>perfect collinearity</strong> between <span class="math inline">\(X_{1i}\)</span> and <span class="math inline">\(X_{2i}\)</span>, and the regression actually chokes by providing an <em>NA</em> (meaning, not a number) as an answer for the second coefficient. Mathematically, perfect collinearity asks for a computer to divide a number by zero (which computers don’t like to do).</p>
</div>
<div id="what-does-collinearity-do-to-our-regression" class="section level3" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> What does Collinearity do to our regression?</h3>
<p>The takeaway from our application is that collinearity can become a significant problem if the degree of correlation among the independent variables is large enough. What the application does not show is that collinearity also results in excessively large standard errors of the coefficient estimates. Intuitively, if the regression doesn’t know which variable is providing the (redundant) information, then it shows this by placing little precision on the estimate - meaning a large standard deviation. This large standard deviation will impact the significance of estimates via confidence intervals and hypothesis tests.</p>
</div>
<div id="how-to-test-for-collinearity" class="section level3" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> How to test for Collinearity?</h3>
<p>Note that some collinearity exists in every equation. All variables are correlated to some degree (even if completely at
random). Therefore, the question is really how much multicollinearity exists in an equation? Is it enough to cause the types of problems we saw in the application above?</p>
<p>There are two characteristics that help detect the degree of collinearity in a regression:</p>
<ul>
<li><p>High simple correlation coefficients</p></li>
<li><p>High Variance Inflation Factors (VIFs)</p></li>
</ul>
<div id="correlation-coefficients" class="section level4" number="9.2.3.1">
<h4><span class="header-section-number">9.2.3.1</span> Correlation Coefficients</h4>
<p><span class="math display">\[Cov(X_1,X_2)=\frac{1}{n-1} \sum_{i=1}^n (X_{1i}-\bar{X}_1)(X_{2i}-\bar{X}_2)\]</span>
<span class="math display">\[S_{X_1} = \frac{1}{n-1} \sum_{i=1}^n (X_{1i}-\bar{X}_1)^2\]</span>
<span class="math display">\[S_{X_2} = \frac{1}{n-1} \sum_{i=1}^n (X_{2i}-\bar{X}_2)^2\]</span></p>
<p><span class="math display">\[\rho(X_1,X_2) = \frac{Cov(X_1,X_2)}{S_{X_1}S_{X_2}}\]</span></p>
<p>If a simple correlation coefficient between any two explanatory variables, <span class="math inline">\(\rho(X_1,X_2)\)</span>, is high in absolute value, then multicollinearity is a potential problem. Like we saw in the application, high is rather arbitrary. Therefore, researchers settle on a threshold of 0.80. In other words, if you have a correlation of 0.80 or higher, then you are running the risk of having your estimates biased by the existence of collinearity.</p>
<p>The problem with looking at simple correlations is that they are <em>pairwise</em> calculations. In other words, you can only look at two variables at a time. What if a collinearity problem is bigger than just two variables?</p>
</div>
<div id="variance-inflation-factors-vifs" class="section level4" number="9.2.3.2">
<h4><span class="header-section-number">9.2.3.2</span> Variance Inflation Factors (VIFs)</h4>
<p>Suppose you want to estimate a regression with three independent variables, but you want to test for collinearity first.</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \varepsilon_i\]</span></p>
<p>Correlation coefficients, being pairwise, will not be able to uncover a correlation structure that might exist across <em>all three</em> independent variables.</p>
<p>Take for example three independent variables: a pitcher’s ERA, the number of earned runs, and the number of innings pitched. For those of you (like me) who are unfamiliar with baseball, a pitcher’s ERA is essentially, their earned runs divided by the number of innings pitched. This means that ERA might be positively correlated with earned runs and negatively correlated with innings pitched, but you wouldn’t realize that the correlation is <em>perfect</em> (meaning, equal to 1) unless you consider both variables simultaneously. A Variance Inflation Factor (or VIF) is a method for examining a complete correlation structure on a list of three or more independent variables.</p>
<p>A Variance Inflation Factor (VIF) is calculated in two steps:</p>
<p>First, run an OLS regression where an independent variable (say, X1) takes a turn at being a dependent variable.</p>
<p><span class="math display">\[X_{1i} = a_0 + a_1 X_{2i} + a_2 X_{3i} + u_i\]</span></p>
<p>Note that the original dependent variable <span class="math inline">\((Y_i)\)</span> is NOT in this equation!</p>
<p>The purpose of this auxiliary regression is to see if there is a sophisticated correlation structure between <span class="math inline">\(X_{1i}\)</span> and the right-hand side variables. Conveniently, we already have an <span class="math inline">\(R^2\)</span> which will indicate exactly how much the variation in the left-hand variable is <em>explained</em> by the right-hand variables. The second step takes the <span class="math inline">\(R^2\)</span> from this regression and calculates the VIF for independent variable <span class="math inline">\(X_{1i}\)</span>. Since the VIF impacts the estimated coefficient of <span class="math inline">\(\beta_1\)</span> in the original regression, it is sometimes referred to as <span class="math inline">\(VIF(\hat{\beta}_1)\)</span>:</p>
<p><span class="math display">\[VIF(\hat{\beta}_1) = \frac{1}{1-R^2}\]</span></p>
<p>If we did this for every independent variable in the original regression, we would arrive at three VIF values.</p>
<p><span class="math display">\[X_{1i} = a_0 + a_1 X_{2i} + a_2 X_{3i} + u_i \rightarrow VIF(\hat{\beta}_1) = \frac{1}{1-R^2}\]</span></p>
<p><span class="math display">\[X_{2i} = a_0 + a_1 X_{1i} + a_2 X_{3i} + u_i \rightarrow VIF(\hat{\beta}_2) = \frac{1}{1-R^2}\]</span></p>
<p><span class="math display">\[X_{3i} = a_0 + a_1 X_{1i} + a_2 X_{2i} + u_i \rightarrow VIF(\hat{\beta}_3) = \frac{1}{1-R^2}\]</span></p>
<p>These VIF values will deliver the amount of bias the standard errors each of the estimated coefficients will receive due to the presence of collinearity. In order to determine if there is a problem, we again resort to an arbitrary threshold of <span class="math inline">\(VIF \geq 5\)</span>. Note that since an <span class="math inline">\(R^2\)</span> value is comparable to a correlation coefficient, this VIF measure corresponds to a correlation above 0.8.</p>
</div>
</div>
<div id="an-application-2" class="section level3" number="9.2.4">
<h3><span class="header-section-number">9.2.4</span> An Application:</h3>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="Advanced.html#cb318-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readxl)</span>
<span id="cb318-2"><a href="Advanced.html#cb318-2" aria-hidden="true" tabindex="-1"></a>MULTI2 <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;C:/Data/MBA8350/MULTI2.xlsx&quot;</span>)</span>
<span id="cb318-3"><a href="Advanced.html#cb318-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(MULTI2)</span></code></pre></div>
<pre><code>## [1] &quot;Team&quot;          &quot;League&quot;        &quot;Wins&quot;          &quot;ERA&quot;           &quot;Runs&quot;          &quot;Hits_Allowed&quot; 
## [7] &quot;Walks_Allowed&quot; &quot;Saves&quot;         &quot;Errors&quot;</code></pre>
<p>Suppose that you want to see you could explain why some baseball teams recorded more wins than others by looking at the season statistics listed above. Before we run a full regression with <em>Wins</em> as the dependent variable and the other right variables as independent variables, we need to test for collinearity.</p>
<p>If we were to follow the steps above for each independent variable, we will need to calculate seven VIF values (Team isn’t a variable… it’s a name). This is a lot easier done than said in R:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="Advanced.html#cb320-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate the &#39;intended&#39; model:</span></span>
<span id="cb320-2"><a href="Advanced.html#cb320-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-3"><a href="Advanced.html#cb320-3" aria-hidden="true" tabindex="-1"></a>REG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Wins <span class="sc">~</span> League <span class="sc">+</span> ERA <span class="sc">+</span> Runs <span class="sc">+</span> Hits_Allowed <span class="sc">+</span> Walks_Allowed <span class="sc">+</span> Saves <span class="sc">+</span> Errors, <span class="at">data =</span> MULTI2)</span>
<span id="cb320-4"><a href="Advanced.html#cb320-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-5"><a href="Advanced.html#cb320-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Use REG object to call vif command:</span></span>
<span id="cb320-6"><a href="Advanced.html#cb320-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span></code></pre></div>
<pre><code>## Loading required package: carData</code></pre>
<pre><code>## 
## Attaching package: &#39;car&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:gtools&#39;:
## 
##     logit</code></pre>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="Advanced.html#cb324-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(REG)</span></code></pre></div>
<pre><code>##        League           ERA          Runs  Hits_Allowed Walks_Allowed         Saves        Errors 
##      1.221101     11.026091      1.279997      6.342662      3.342659      1.762577      1.548678</code></pre>
<p>The output above shows a VIF for each of the independent variables. The largest are for ERA and Hits Allowed, and these are problematic given that they are above our threshold of 5.
So now that we detected collinearity… what do we do about it?</p>
</div>
<div id="how-do-we-remove-collinearity" class="section level3" number="9.2.5">
<h3><span class="header-section-number">9.2.5</span> How do we remove Collinearity?</h3>
<p>There are several ways to remove or reduce the degree of collinearity that vary in degrees of feasibility and effectiveness.</p>
<p>First, is the collinearity problem due to the inherent nature of the variables themselves or is it a coincidence with your current sample? If it is coincidence, then the problem might go away if you collected more observations. Note that this might not always work, and sometimes more data isn’t even available. However, it is a easy first pass if feasible.</p>
<p>Second, one could always <strong>ignore</strong> collinearity and proceed with the analysis. The reason for this is that while collinearity might bias the standard errors of the estimates, the bias might not be that bad. Think of increasing the value of zero by 100 times.</p>
<p>For example, lets try the ignorance approach with the baseball application above:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="Advanced.html#cb326-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pander</span>(<span class="fu">summary</span>(REG))</span></code></pre></div>
<table style="width:90%;">
<colgroup>
<col width="27%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">69.28</td>
<td align="center">13.64</td>
<td align="center">5.08</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>League</strong></td>
<td align="center">1.85</td>
<td align="center">1.01</td>
<td align="center">1.82</td>
<td align="center">0.08</td>
</tr>
<tr class="odd">
<td align="center"><strong>ERA</strong></td>
<td align="center">-6.06</td>
<td align="center">3.44</td>
<td align="center">-1.76</td>
<td align="center">0.09</td>
</tr>
<tr class="even">
<td align="center"><strong>Runs</strong></td>
<td align="center">0.09</td>
<td align="center">0.01</td>
<td align="center">11.52</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="center"><strong>Hits_Allowed</strong></td>
<td align="center">-0.03</td>
<td align="center">0.01</td>
<td align="center">-1.79</td>
<td align="center">0.09</td>
</tr>
<tr class="even">
<td align="center"><strong>Walks_Allowed</strong></td>
<td align="center">-0.03</td>
<td align="center">0.01</td>
<td align="center">-2.26</td>
<td align="center">0.03</td>
</tr>
<tr class="odd">
<td align="center"><strong>Saves</strong></td>
<td align="center">0.54</td>
<td align="center">0.08</td>
<td align="center">7.07</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>Errors</strong></td>
<td align="center">0</td>
<td align="center">0.04</td>
<td align="center">0.1</td>
<td align="center">0.92</td>
</tr>
</tbody>
</table>
<table style="width:86%;">
<caption>Fitting linear model: Wins ~ League + ERA + Runs + Hits_Allowed + Walks_Allowed + Saves + Errors</caption>
<colgroup>
<col width="20%" />
<col width="30%" />
<col width="11%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">30</td>
<td align="center">2.5</td>
<td align="center">0.96</td>
<td align="center">0.95</td>
</tr>
</tbody>
</table>
<p>The results suggest that the population coefficients for the variables League, ERA, Hits Allowed, and Errors are all insignificantly different from zero with 95% confidence. Now if they we all significant, then we could possibly ignore any potential of collinearity because the bias would not be <em>enough</em> for us to see if there was a problem. However, since two of these variables are ones that we already identified a collinearity problem with, then we a unable to go this route.</p>
<p>The third option to remove collinearity is to remove independent variables until the correlation structure is removed. The way to proceed down this route is to remove the variables (one-at-a-time) with the highest VIF values until all remaining values have VIF values below 5. The good side of this analysis is that you can now proceed with main regression knowing that collinearity is not a problem. The bad side is that you might have had to remove variables that you really wanted to have in the regression.</p>
<p>The VIF values from the baseball analysis suggest that ERA and Hits Allowed are two variables that potentially need to be removed from the analysis do to collinearity. The way to proceed is that if we were to only remove one variable at a time, we will remove the variable with the <em>highest</em> VIF because it is the one that has the most redundant information.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="Advanced.html#cb327-1" aria-hidden="true" tabindex="-1"></a>REG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Wins <span class="sc">~</span> League <span class="sc">+</span> Runs <span class="sc">+</span> Hits_Allowed <span class="sc">+</span> Walks_Allowed <span class="sc">+</span> Saves <span class="sc">+</span> Errors, <span class="at">data =</span> MULTI2)</span>
<span id="cb327-2"><a href="Advanced.html#cb327-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb327-3"><a href="Advanced.html#cb327-3" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(REG)</span></code></pre></div>
<pre><code>##        League          Runs  Hits_Allowed Walks_Allowed         Saves        Errors 
##      1.149383      1.279914      1.365583      1.235945      1.665172      1.546465</code></pre>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="Advanced.html#cb329-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(REG)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Wins ~ League + Runs + Hits_Allowed + Walks_Allowed + 
##     Saves + Errors, data = MULTI2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.8127 -2.0776  0.0551  2.0168  4.9951 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   83.214595  11.607524   7.169 2.67e-07 ***
## League         2.278948   1.026010   2.221   0.0365 *  
## Runs           0.088445   0.008031  11.013 1.20e-10 ***
## Hits_Allowed  -0.047231   0.006840  -6.905 4.86e-07 ***
## Walks_Allowed -0.043122   0.007485  -5.761 7.22e-06 ***
## Saves          0.569301   0.077227   7.372 1.69e-07 ***
## Errors         0.001322   0.043722   0.030   0.9761    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.615 on 23 degrees of freedom
## Multiple R-squared:  0.9557, Adjusted R-squared:  0.9441 
## F-statistic: 82.65 on 6 and 23 DF,  p-value: 2.119e-14</code></pre>
<p>The regression with ERA removed now is free of collinearity. We can confirm this be the fact that all VIF values of the remaining independent variables are well below 5. The regression results now suggest that after removing ERA, ERA and Hits Allowed now have population coefficients that were significantly different than zero with 95% confidence. Errors is still an insignificant variable. This suggests that the insignificance wasn’t due to collinearity. It simply the fact that Errors do not significantly help us explain why some teams win more games than others.</p>
<div id="sometimes-removing-collinearity-might-involve-multiple-rounds" class="section level4" number="9.2.5.1">
<h4><span class="header-section-number">9.2.5.1</span> Sometimes removing collinearity might involve multiple rounds</h4>
<p>You will note from the application above that we only needed to remove one independent variable, so only one round of VIF calculations displayed values above 5. It might sometimes be the case that even after you remove an independent variable, the next round of VIF values reports a value of 5. If this happens, you simply repeat the process by removing the variable with the highest VIF and checking again. In general, a complete removal of multicollinearity involves the following:</p>
<ul>
<li><p>calculate VIFs for your data set</p></li>
<li><p>drop the variable with the highest VIF (greater than 5)</p></li>
<li><p>calculate VIFs on your data again (with the dropped variable no longer
in the data set)</p></li>
<li><p>drop the variable with the highest VIF (greater than 5)</p></li>
<li><p>this is repeated until all VIFs are less than 5</p></li>
</ul>
</div>
</div>
</div>
<div id="heteroskedasticity" class="section level2" number="9.3">
<h2><span class="header-section-number">9.3</span> Heteroskedasticity</h2>
<p>Another violation of our regression assumptions is that of heteroskedasticity. Pure heteroskedasticity occurs when the variance of the error term is constant:</p>
<p><span class="math display">\[VAR (\varepsilon_i ) = \sigma^2 (i = 1, 2, ...,N)\]</span></p>
<p>With heteroskedasticity, this error term variance is not constant. In other words, the distribution of the error term now depends on which values are being considered.</p>
<p><span class="math display">\[VAR (\varepsilon_i ) = \sigma_i^2 (i = 1, 2, ...,N)\]</span></p>
<p>An example of heteroskedasticity would be one where you could break the forecast errors into two discrete groups. Suppose you have the results of a statistics test and you considered the forecast errors for males and females separately.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-171-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>One gender has forecasts errors that comprise the green distribution while the other gender has forecast errors that comprise the blue distribution. Both distributions have zero mean, but the differences in distributions suggest that there is more volatile forecast errors for green distribution than the blue. This is a problem because the standard deviation of the <em>entire</em> garbage can is used to deliver statistical inference on the precision of the model. If we were to say that females and males comprised the blue and green distributions respectively, then this is saying that the model does a much better job at predicting female test scores than male test scores. Since our precision is an average of these two results, it actually indicates that we do worse on females and better on males than what is truly going on.</p>
<div id="pure-versus-impure-heteroskedasticity" class="section level3" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Pure versus Impure Heteroskedasticity</h3>
<p>We can categorize heteroskedasticity depending on whether it is due to a model misspecification (on the part of the researcher) or if it is simply a force of nature. Heteroskedasticity is said to be <strong>impure</strong> if it is due to a model misspecification. If this is the case, then an change in the model might very well remove the heteroskedasticity and that’s that. If heteroskedasticity is said to be <strong>pure</strong>, then it is the result of the true relationship in the data and no change in model specifications will correct it. If this is the case, then more sophisticated methods are called for.</p>
</div>
<div id="consequences-of-heteroskedasticity" class="section level3" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> Consequences of Heteroskedasticity</h3>
<p>Heteroskedasticity has two consequences:</p>
<ol style="list-style-type: decimal">
<li><p>OLS is no longer the minimum variance estimator (of all the linear
unbiased estimators). In other words, OLS is no longer BLUE.</p></li>
<li><p>OLS estimates of the standard errors are biased. The bias is typically negative, meaning that OLS underestimates the standard errors, overestimates the t-scores, and makes hypothesis tests <strong>more likely</strong> to be rejected.</p></li>
</ol>
<p>Note that while heteroskedasticity underestimates the standard errors, it <strong>does not</strong> deliver bias on the estimates themselves. So it’s not a problem of the estimates, its that the estimates appear more significant due to heteroskedasticity.</p>
</div>
<div id="detection" class="section level3" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> Detection</h3>
<p>Before using any test for heteroskedasticity, ask the following:</p>
<ul>
<li><p>Are there any obvious specification errors? If so, then we might be able to correct impure heteroskedasticity without even knowing it. Fix those before testing!</p></li>
<li><p>Is the subject of the research likely to be afflicted with heteroskedasticity? Not only are cross-sectional studies the most frequent source of heteroskedasticity, but cross-sectional studies with large variations in the size of the dependent variable are particularly susceptible</p></li>
<li><p>Does a graph of the residuals show any evidence of heteroskedasticity? Specifically, plot the residuals against the independent variables. If it appears that the dispersion of the error term is in any way proportional to the independent variables, then it could indicate a problem as well as a solution.</p></li>
</ul>
<p>A clear example of how the forecast error variance can be proportional to an independent variable is to return to a previous scenario where we tried to estimate an individuals consumption based on their income levels.</p>
<p>Your data set has 50 household observations from each of three annual income levels: $10,000, $75,000, and $150,000 as well as their annual consumption expenditures. As the figure illustrates, households earning around $10,000 a year in income all have roughly the same consumption level (because they all save very little). As income levels increase, you see more <em>dispersion</em> in consumption expenditures because more income is paired with more options. Households earning $150,000 annually could choose to save a majority of it or even go into debt (i.e. spend more than $150,000). This data could be used to estimate a regression line (illustrated in black), but you can see that the model looks like it does a poorer and poorer job of predicting consumption expenditures as the income levels increase.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-172-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>If we were to examine the forecast errors of this regression and plot them with income, you will see that the dispersion in indeed proportional to income.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-173-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>A formal test for heteroskedasticity that we will focus on is the Park test. The Park test is simple but less general than another test called the White test, but the concepts are similar.</p>
<p>A Park test is a formal way of testing if the variance of the residual term is in fact proportional to an independent variable.</p>
<ol style="list-style-type: decimal">
<li><p>Run the initial regression and obtain the residuals <span class="math inline">\((e_i)\)</span>.</p></li>
<li><p>Use the log of the squared residuals as the dependent variable for an auxiliary regression with the independent variable on the right hand side.</p></li>
</ol>
<p><span class="math display">\[ ln e_i^2 = a_0 + a_1 ln X_i + u_i\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Test the null hypothesis that <span class="math inline">\(a_1 = 0\)</span>. If you reject the null, then you have formal evidence that the variance of the residual term is related to the independent variable.</li>
</ol>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="Advanced.html#cb331-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A Park test:</span></span>
<span id="cb331-2"><a href="Advanced.html#cb331-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb331-3"><a href="Advanced.html#cb331-3" aria-hidden="true" tabindex="-1"></a>EPS <span class="ot">&lt;-</span> <span class="fu">log</span>(<span class="fu">residuals</span>(REG)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb331-4"><a href="Advanced.html#cb331-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb331-5"><a href="Advanced.html#cb331-5" aria-hidden="true" tabindex="-1"></a>PARK <span class="ot">&lt;-</span> <span class="fu">lm</span>(EPS <span class="sc">~</span> X)</span>
<span id="cb331-6"><a href="Advanced.html#cb331-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb331-7"><a href="Advanced.html#cb331-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span>
<span id="cb331-8"><a href="Advanced.html#cb331-8" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(PARK)</span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 1.7304048  0.2938071  5.8896 2.501e-08 ***
## X           0.0272750  0.0030414  8.9678 1.231e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The results from the Park test suggest that the variance in the forecast errors of consumption are in fact related to income.</p>
</div>
<div id="remedies" class="section level3" number="9.3.4">
<h3><span class="header-section-number">9.3.4</span> Remedies</h3>
<p>There are two remedies for heteroskedasticity, and they primarily depend upon whether or not the heteroskedasticity is pure or not.</p>
<p>First, if we are dealing with impure heteroskedasticity then a <strong>model re-specification</strong> might resolve the issue. If we consider the consumption - income application above, it is fairly well known that including another value such as wealth might help explain dimensions of consumption that income cannot. It might be better to cast the model in a log-log form so we are looking at percentage changes in income as opposed to absolute changes. Either one of these proposed changes to the model could bring the model closer to a correct specification which could have the added benefit of removing heteroskedasticity.</p>
<p>Second, if we are dealing with pure heteroskedasticity then the issue will not go away no matter how we specify the model. In this case, there are Heteroskedasticity-corrected standard errors. This is a procedure (beyond the scope of our class) that <em>corrects</em> the standard errors of a regression in order to compensate for the negative bias caused by heteroskedasticity. These standard errors can be used for all inference of the model.</p>

</div>
</div>
</div>
















            </section>

          </div>
        </div>
      </div>
<a href="MLR.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sjdres/MBA8350_Companion/edit/master/09-Advanced.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/sjdres/MBA8350_Companion/blob/master/09-Advanced.Rmd",
"text": null
},
"download": ["bookdownproj.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
