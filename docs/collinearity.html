<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.2 Collinearity | MBA 8350: Course Companion for Analyzing and Leveraging Data</title>
  <meta name="description" content="Course companion for MBA 8350." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="9.2 Collinearity | MBA 8350: Course Companion for Analyzing and Leveraging Data" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course companion for MBA 8350." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.2 Collinearity | MBA 8350: Course Companion for Analyzing and Leveraging Data" />
  
  <meta name="twitter:description" content="Course companion for MBA 8350." />
  

<meta name="author" content="Scott Dressler" />


<meta name="date" content="2021-12-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="nonlinear-models.html"/>
<link rel="next" href="heteroskedasticity.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MBA 8350 Course Companion</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html"><i class="fa fa-check"></i>About this book…</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-big-picture-of-statistics.html"><a href="the-big-picture-of-statistics.html"><i class="fa fa-check"></i><b>1.1</b> The “Big Picture” of Statistics</a></li>
<li class="chapter" data-level="1.2" data-path="the-vocabulary-of-statistics.html"><a href="the-vocabulary-of-statistics.html"><i class="fa fa-check"></i><b>1.2</b> The Vocabulary of Statistics</a></li>
<li class="chapter" data-level="1.3" data-path="descriptive-measures.html"><a href="descriptive-measures.html"><i class="fa fa-check"></i><b>1.3</b> Descriptive Measures</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="descriptive-measures.html"><a href="descriptive-measures.html#central-tendency"><i class="fa fa-check"></i><b>1.3.1</b> Central Tendency</a></li>
<li class="chapter" data-level="1.3.2" data-path="descriptive-measures.html"><a href="descriptive-measures.html#variation"><i class="fa fa-check"></i><b>1.3.2</b> Variation</a></li>
<li class="chapter" data-level="1.3.3" data-path="descriptive-measures.html"><a href="descriptive-measures.html#measures-of-shape"><i class="fa fa-check"></i><b>1.3.3</b> Measures of shape</a></li>
<li class="chapter" data-level="1.3.4" data-path="descriptive-measures.html"><a href="descriptive-measures.html#covariance-and-correlation"><i class="fa fa-check"></i><b>1.3.4</b> Covariance and Correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>2</b> Data Collection and Sampling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html"><i class="fa fa-check"></i><b>2.1</b> Sampling Distributions</a></li>
<li class="chapter" data-level="2.2" data-path="sampling-bias---two-examples.html"><a href="sampling-bias---two-examples.html"><i class="fa fa-check"></i><b>2.2</b> Sampling Bias - two examples</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="sampling-bias---two-examples.html"><a href="sampling-bias---two-examples.html#dewey-defeats-truman"><i class="fa fa-check"></i><b>2.2.1</b> Dewey Defeats Truman?</a></li>
<li class="chapter" data-level="2.2.2" data-path="sampling-bias---two-examples.html"><a href="sampling-bias---two-examples.html#section-1"><i class="fa fa-check"></i><b>2.2.2</b> 98.6?</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="sampling-methods.html"><a href="sampling-methods.html"><i class="fa fa-check"></i><b>2.3</b> Sampling Methods</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="sampling-methods.html"><a href="sampling-methods.html#simple-random-sampling"><i class="fa fa-check"></i><b>2.3.1</b> Simple random sampling</a></li>
<li class="chapter" data-level="2.3.2" data-path="sampling-methods.html"><a href="sampling-methods.html#systematic-sampling"><i class="fa fa-check"></i><b>2.3.2</b> Systematic Sampling</a></li>
<li class="chapter" data-level="2.3.3" data-path="sampling-methods.html"><a href="sampling-methods.html#stratified-sampling"><i class="fa fa-check"></i><b>2.3.3</b> Stratified Sampling</a></li>
<li class="chapter" data-level="2.3.4" data-path="sampling-methods.html"><a href="sampling-methods.html#cluster-sampling"><i class="fa fa-check"></i><b>2.3.4</b> Cluster Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="sampling-in-practice.html"><a href="sampling-in-practice.html"><i class="fa fa-check"></i><b>2.4</b> Sampling in Practice</a></li>
<li class="chapter" data-level="2.5" data-path="sampling-and-sampling-distributions.html"><a href="sampling-and-sampling-distributions.html"><i class="fa fa-check"></i><b>2.5</b> Sampling and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="sampling-and-sampling-distributions.html"><a href="sampling-and-sampling-distributions.html#an-application"><i class="fa fa-check"></i><b>2.5.1</b> An Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="R.html"><a href="R.html"><i class="fa fa-check"></i><b>3</b> Getting Started with R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-r-project-for-statistical-computing.html"><a href="the-r-project-for-statistical-computing.html"><i class="fa fa-check"></i><b>3.1</b> The R Project for Statistical Computing</a></li>
<li class="chapter" data-level="3.2" data-path="downloading-and-installing-r.html"><a href="downloading-and-installing-r.html"><i class="fa fa-check"></i><b>3.2</b> Downloading and installing R</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="downloading-and-installing-r.html"><a href="downloading-and-installing-r.html#choosing-a-mirror"><i class="fa fa-check"></i><b>3.2.1</b> Choosing a <em>Mirror</em></a></li>
<li class="chapter" data-level="3.2.2" data-path="downloading-and-installing-r.html"><a href="downloading-and-installing-r.html#download-and-install-the-correct-version"><i class="fa fa-check"></i><b>3.2.2</b> Download and install the correct version</a></li>
<li class="chapter" data-level="3.2.3" data-path="downloading-and-installing-r.html"><a href="downloading-and-installing-r.html#downloading-and-installing-rstudio"><i class="fa fa-check"></i><b>3.2.3</b> Downloading and installing RStudio</a></li>
<li class="chapter" data-level="3.2.4" data-path="downloading-and-installing-r.html"><a href="downloading-and-installing-r.html#taking-stock"><i class="fa fa-check"></i><b>3.2.4</b> Taking Stock</a></li>
<li class="chapter" data-level="3.2.5" data-path="downloading-and-installing-r.html"><a href="downloading-and-installing-r.html#installing-packages"><i class="fa fa-check"></i><b>3.2.5</b> Installing <em>Packages</em></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="coding-basics.html"><a href="coding-basics.html"><i class="fa fa-check"></i><b>3.3</b> Coding Basics</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="coding-basics.html"><a href="coding-basics.html#assigning-objects"><i class="fa fa-check"></i><b>3.3.1</b> Assigning Objects</a></li>
<li class="chapter" data-level="3.3.2" data-path="coding-basics.html"><a href="coding-basics.html#listing-adding-and-removing"><i class="fa fa-check"></i><b>3.3.2</b> Listing, Adding, and Removing</a></li>
<li class="chapter" data-level="3.3.3" data-path="coding-basics.html"><a href="coding-basics.html#loading-data"><i class="fa fa-check"></i><b>3.3.3</b> Loading Data</a></li>
<li class="chapter" data-level="3.3.4" data-path="coding-basics.html"><a href="coding-basics.html#manipulating-data"><i class="fa fa-check"></i><b>3.3.4</b> Manipulating Data</a></li>
<li class="chapter" data-level="3.3.5" data-path="coding-basics.html"><a href="coding-basics.html#subsetting-data"><i class="fa fa-check"></i><b>3.3.5</b> Subsetting Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>3.4</b> Data Visualization</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="data-visualization.html"><a href="data-visualization.html#histograms"><i class="fa fa-check"></i><b>3.4.1</b> Histograms</a></li>
<li class="chapter" data-level="3.4.2" data-path="data-visualization.html"><a href="data-visualization.html#line-bar-and-scatter-plots"><i class="fa fa-check"></i><b>3.4.2</b> Line, bar, and Scatter Plots</a></li>
<li class="chapter" data-level="3.4.3" data-path="data-visualization.html"><a href="data-visualization.html#boxplots"><i class="fa fa-check"></i><b>3.4.3</b> Boxplots</a></li>
<li class="chapter" data-level="3.4.4" data-path="data-visualization.html"><a href="data-visualization.html#much-more-out-there"><i class="fa fa-check"></i><b>3.4.4</b> Much more out there</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CLT.html"><a href="CLT.html"><i class="fa fa-check"></i><b>4</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-clt-formally.html"><a href="the-clt-formally.html"><i class="fa fa-check"></i><b>4.1</b> The CLT (Formally)</a></li>
<li class="chapter" data-level="4.2" data-path="application-1-a-sampling-distribution-with-a-known-population.html"><a href="application-1-a-sampling-distribution-with-a-known-population.html"><i class="fa fa-check"></i><b>4.2</b> Application 1: A Sampling Distribution with a Known Population</a></li>
<li class="chapter" data-level="4.3" data-path="application-2-a-sampling-distribution-with-an-unknown-population.html"><a href="application-2-a-sampling-distribution-with-an-unknown-population.html"><i class="fa fa-check"></i><b>4.3</b> Application 2: A Sampling Distribution with an Unknown Population</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="application-2-a-sampling-distribution-with-an-unknown-population.html"><a href="application-2-a-sampling-distribution-with-an-unknown-population.html#the-sample"><i class="fa fa-check"></i><b>4.3.1</b> The Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-punchline-1.html"><a href="the-punchline-1.html"><i class="fa fa-check"></i><b>4.4</b> The Punchline</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.1" data-path="a-refresher-on-probability.html"><a href="a-refresher-on-probability.html"><i class="fa fa-check"></i><b>5.1</b> A Refresher on Probability</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="a-refresher-on-probability.html"><a href="a-refresher-on-probability.html#application-1"><i class="fa fa-check"></i><b>5.1.1</b> Application 1</a></li>
<li class="chapter" data-level="5.1.2" data-path="a-refresher-on-probability.html"><a href="a-refresher-on-probability.html#application-2"><i class="fa fa-check"></i><b>5.1.2</b> Application 2</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="deriving-a-confidence-interval.html"><a href="deriving-a-confidence-interval.html"><i class="fa fa-check"></i><b>5.2</b> Deriving a Confidence Interval</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="deriving-a-confidence-interval.html"><a href="deriving-a-confidence-interval.html#application-3"><i class="fa fa-check"></i><b>5.2.1</b> Application 3</a></li>
<li class="chapter" data-level="5.2.2" data-path="deriving-a-confidence-interval.html"><a href="deriving-a-confidence-interval.html#what-if-we-want-to-change-confidence"><i class="fa fa-check"></i><b>5.2.2</b> What if we want to change confidence?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="what-to-do-when-we-do-not-know-sigma.html"><a href="what-to-do-when-we-do-not-know-sigma.html"><i class="fa fa-check"></i><b>5.3</b> What to do when we do not know <span class="math inline">\(\sigma\)</span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="what-to-do-when-we-do-not-know-sigma.html"><a href="what-to-do-when-we-do-not-know-sigma.html#t-distribution-versus-z-distribution"><i class="fa fa-check"></i><b>5.3.1</b> t distribution versus Z distribution…</a></li>
<li class="chapter" data-level="5.3.2" data-path="what-to-do-when-we-do-not-know-sigma.html"><a href="what-to-do-when-we-do-not-know-sigma.html#application-4"><i class="fa fa-check"></i><b>5.3.2</b> Application 4</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="determining-sample-size.html"><a href="determining-sample-size.html"><i class="fa fa-check"></i><b>5.4</b> Determining Sample Size</a></li>
<li class="chapter" data-level="5.5" data-path="concluding-applications.html"><a href="concluding-applications.html"><i class="fa fa-check"></i><b>5.5</b> Concluding Applications</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="concluding-applications.html"><a href="concluding-applications.html#light-bulbs-last-time"><i class="fa fa-check"></i><b>5.5.1</b> Light Bulbs (Last Time)</a></li>
<li class="chapter" data-level="5.5.2" data-path="concluding-applications.html"><a href="concluding-applications.html#returning-to-the-philadelphia-school-policy-application"><i class="fa fa-check"></i><b>5.5.2</b> Returning to the Philadelphia School Policy Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="HT.html"><a href="HT.html"><i class="fa fa-check"></i><b>6</b> Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="6.1" data-path="anatomy-of-a-hypothesis-test.html"><a href="anatomy-of-a-hypothesis-test.html"><i class="fa fa-check"></i><b>6.1</b> Anatomy of a Hypothesis Test</a></li>
<li class="chapter" data-level="6.2" data-path="two-methods-for-conducting-a-hypothesis-test-when-sigma-is-known.html"><a href="two-methods-for-conducting-a-hypothesis-test-when-sigma-is-known.html"><i class="fa fa-check"></i><b>6.2</b> Two methods for conducting a hypothesis test (when <span class="math inline">\(\sigma\)</span> is known)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="two-methods-for-conducting-a-hypothesis-test-when-sigma-is-known.html"><a href="two-methods-for-conducting-a-hypothesis-test-when-sigma-is-known.html#rejection-region-method"><i class="fa fa-check"></i><b>6.2.1</b> Rejection Region Method</a></li>
<li class="chapter" data-level="6.2.2" data-path="two-methods-for-conducting-a-hypothesis-test-when-sigma-is-known.html"><a href="two-methods-for-conducting-a-hypothesis-test-when-sigma-is-known.html#p-value-approach"><i class="fa fa-check"></i><b>6.2.2</b> P-value Approach</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="two-sided-vs-one-sided-test.html"><a href="two-sided-vs-one-sided-test.html"><i class="fa fa-check"></i><b>6.3</b> Two-sided vs One-sided Test</a></li>
<li class="chapter" data-level="6.4" data-path="conducting-a-hypothesis-test-when-sigma-is-unknown.html"><a href="conducting-a-hypothesis-test-when-sigma-is-unknown.html"><i class="fa fa-check"></i><b>6.4</b> Conducting a hypothesis test (when <span class="math inline">\(\sigma\)</span> is unknown)</a></li>
<li class="chapter" data-level="6.5" data-path="appendix-a-note-on-calculating-p-values.html"><a href="appendix-a-note-on-calculating-p-values.html"><i class="fa fa-check"></i><b>6.5</b> Appendix: A note on calculating P-values</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="appendix-a-note-on-calculating-p-values.html"><a href="appendix-a-note-on-calculating-p-values.html#the-problem"><i class="fa fa-check"></i><b>6.5.1</b> The Problem</a></li>
<li class="chapter" data-level="6.5.2" data-path="appendix-a-note-on-calculating-p-values.html"><a href="appendix-a-note-on-calculating-p-values.html#how-to-calculate-p-values"><i class="fa fa-check"></i><b>6.5.2</b> How to calculate p-values</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="SLR.html"><a href="SLR.html"><i class="fa fa-check"></i><b>7</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="a-simple-linear-regression-model.html"><a href="a-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>7.1</b> A Simple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="a-simple-linear-regression-model.html"><a href="a-simple-linear-regression-model.html#what-does-a-regression-model-imply"><i class="fa fa-check"></i><b>7.1.1</b> What does a regression model imply?</a></li>
<li class="chapter" data-level="7.1.2" data-path="a-simple-linear-regression-model.html"><a href="a-simple-linear-regression-model.html#the-real-simple-linear-regression-model"><i class="fa fa-check"></i><b>7.1.2</b> The <em>REAL</em> Simple Linear Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="application-predicting-house-price-based-on-house-size.html"><a href="application-predicting-house-price-based-on-house-size.html"><i class="fa fa-check"></i><b>7.2</b> Application: Predicting House Price Based on House Size</a></li>
<li class="chapter" data-level="7.3" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html"><i class="fa fa-check"></i><b>7.3</b> Ordinary Least Squares (OLS)</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="ordinary-least-squares-ols.html"><a href="ordinary-least-squares-ols.html#b.l.u.e."><i class="fa fa-check"></i><b>7.3.1</b> B.L.U.E.</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="decomposition-of-variance.html"><a href="decomposition-of-variance.html"><i class="fa fa-check"></i><b>7.4</b> Decomposition of Variance</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="decomposition-of-variance.html"><a href="decomposition-of-variance.html#the-r2"><i class="fa fa-check"></i><b>7.4.1</b> The <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="7.4.2" data-path="decomposition-of-variance.html"><a href="decomposition-of-variance.html#what-is-a-good-r2"><i class="fa fa-check"></i><b>7.4.2</b> What is a <em>good</em> <span class="math inline">\(R^2\)</span>?</a></li>
<li class="chapter" data-level="7.4.3" data-path="decomposition-of-variance.html"><a href="decomposition-of-variance.html#standard-error-of-the-estimate"><i class="fa fa-check"></i><b>7.4.3</b> Standard Error of the Estimate</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="assumptions-of-the-linear-regression-model.html"><a href="assumptions-of-the-linear-regression-model.html"><i class="fa fa-check"></i><b>7.5</b> Assumptions of the Linear Regression Model</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="assumptions-of-the-linear-regression-model.html"><a href="assumptions-of-the-linear-regression-model.html#linearity"><i class="fa fa-check"></i><b>7.5.1</b> Linearity</a></li>
<li class="chapter" data-level="7.5.2" data-path="assumptions-of-the-linear-regression-model.html"><a href="assumptions-of-the-linear-regression-model.html#independence-of-errors"><i class="fa fa-check"></i><b>7.5.2</b> Independence of Errors</a></li>
<li class="chapter" data-level="7.5.3" data-path="assumptions-of-the-linear-regression-model.html"><a href="assumptions-of-the-linear-regression-model.html#equal-variance"><i class="fa fa-check"></i><b>7.5.3</b> Equal Variance</a></li>
<li class="chapter" data-level="7.5.4" data-path="assumptions-of-the-linear-regression-model.html"><a href="assumptions-of-the-linear-regression-model.html#normality-of-errors"><i class="fa fa-check"></i><b>7.5.4</b> Normality of Errors</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>7.6</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="statistical-inference.html"><a href="statistical-inference.html#confidence-intervals-around-population-parameters"><i class="fa fa-check"></i><b>7.6.1</b> Confidence Intervals (around population parameters)</a></li>
<li class="chapter" data-level="7.6.2" data-path="statistical-inference.html"><a href="statistical-inference.html#hypothesis-tests"><i class="fa fa-check"></i><b>7.6.2</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="7.6.3" data-path="statistical-inference.html"><a href="statistical-inference.html#confidence-intervals-around-forecasts"><i class="fa fa-check"></i><b>7.6.3</b> Confidence Intervals (around forecasts)</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="up-next.html"><a href="up-next.html"><i class="fa fa-check"></i><b>7.7</b> Up Next…</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="MLR.html"><a href="MLR.html"><i class="fa fa-check"></i><b>8</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="application-explaining-house-price-in-a-multiple-regression.html"><a href="application-explaining-house-price-in-a-multiple-regression.html"><i class="fa fa-check"></i><b>8.1</b> Application: Explaining house price in a multiple regression</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="application-explaining-house-price-in-a-multiple-regression.html"><a href="application-explaining-house-price-in-a-multiple-regression.html#the-importance-of-controls"><i class="fa fa-check"></i><b>8.1.1</b> The Importance of “Controls”</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="adjusted-r2.html"><a href="adjusted-r2.html"><i class="fa fa-check"></i><b>8.2</b> Adjusted <span class="math inline">\(R^2\)</span></a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="adjusted-r2.html"><a href="adjusted-r2.html#abusing-an-r2"><i class="fa fa-check"></i><b>8.2.1</b> Abusing an <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="8.2.2" data-path="adjusted-r2.html"><a href="adjusted-r2.html#an-adjusted-r2"><i class="fa fa-check"></i><b>8.2.2</b> An <em>Adjusted</em> <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="statistical-inference-1.html"><a href="statistical-inference-1.html"><i class="fa fa-check"></i><b>8.3</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="statistical-inference-1.html"><a href="statistical-inference-1.html#hypothesis-tests-1"><i class="fa fa-check"></i><b>8.3.1</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="8.3.2" data-path="statistical-inference-1.html"><a href="statistical-inference-1.html#confidence-intervals-around-population-parameters-1"><i class="fa fa-check"></i><b>8.3.2</b> Confidence Intervals (around population parameters)</a></li>
<li class="chapter" data-level="8.3.3" data-path="statistical-inference-1.html"><a href="statistical-inference-1.html#confidence-intervals-around-forecasts-1"><i class="fa fa-check"></i><b>8.3.3</b> Confidence Intervals (around forecasts)</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="qualitative-dummy-variables.html"><a href="qualitative-dummy-variables.html"><i class="fa fa-check"></i><b>8.4</b> Qualitative (Dummy) Variables</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="qualitative-dummy-variables.html"><a href="qualitative-dummy-variables.html#intercept-dummy-variable"><i class="fa fa-check"></i><b>8.4.1</b> Intercept dummy variable</a></li>
<li class="chapter" data-level="8.4.2" data-path="qualitative-dummy-variables.html"><a href="qualitative-dummy-variables.html#slope-dummy-variable"><i class="fa fa-check"></i><b>8.4.2</b> Slope dummy variable</a></li>
<li class="chapter" data-level="8.4.3" data-path="qualitative-dummy-variables.html"><a href="qualitative-dummy-variables.html#what-if-there-are-more-than-two-categories"><i class="fa fa-check"></i><b>8.4.3</b> What if there are more than two categories?</a></li>
<li class="chapter" data-level="8.4.4" data-path="qualitative-dummy-variables.html"><a href="qualitative-dummy-variables.html#a-final-application"><i class="fa fa-check"></i><b>8.4.4</b> A Final Application</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="joint-hypothesis-tests.html"><a href="joint-hypothesis-tests.html"><i class="fa fa-check"></i><b>8.5</b> Joint Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="joint-hypothesis-tests.html"><a href="joint-hypothesis-tests.html#simple-versus-joint-tests"><i class="fa fa-check"></i><b>8.5.1</b> Simple versus Joint Tests</a></li>
<li class="chapter" data-level="8.5.2" data-path="joint-hypothesis-tests.html"><a href="joint-hypothesis-tests.html#applications"><i class="fa fa-check"></i><b>8.5.2</b> Applications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Advanced.html"><a href="Advanced.html"><i class="fa fa-check"></i><b>9</b> Advanced Regression Topics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="nonlinear-models.html"><a href="nonlinear-models.html"><i class="fa fa-check"></i><b>9.1</b> Nonlinear Models</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="nonlinear-models.html"><a href="nonlinear-models.html#derivatives"><i class="fa fa-check"></i><b>9.1.1</b> Derivatives</a></li>
<li class="chapter" data-level="9.1.2" data-path="nonlinear-models.html"><a href="nonlinear-models.html#why-consider-non-linear-relationships"><i class="fa fa-check"></i><b>9.1.2</b> Why consider non-linear relationships?</a></li>
<li class="chapter" data-level="9.1.3" data-path="nonlinear-models.html"><a href="nonlinear-models.html#functional-forms"><i class="fa fa-check"></i><b>9.1.3</b> Functional Forms</a></li>
<li class="chapter" data-level="9.1.4" data-path="nonlinear-models.html"><a href="nonlinear-models.html#the-log-transformation"><i class="fa fa-check"></i><b>9.1.4</b> The Log transformation</a></li>
<li class="chapter" data-level="9.1.5" data-path="nonlinear-models.html"><a href="nonlinear-models.html#the-quadratic-transformation"><i class="fa fa-check"></i><b>9.1.5</b> The Quadratic transformation</a></li>
<li class="chapter" data-level="9.1.6" data-path="nonlinear-models.html"><a href="nonlinear-models.html#the-reciprocal-transformation"><i class="fa fa-check"></i><b>9.1.6</b> The Reciprocal transformation</a></li>
<li class="chapter" data-level="9.1.7" data-path="nonlinear-models.html"><a href="nonlinear-models.html#conclusion"><i class="fa fa-check"></i><b>9.1.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>9.2</b> Collinearity</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="collinearity.html"><a href="collinearity.html#an-application-1"><i class="fa fa-check"></i><b>9.2.1</b> An Application</a></li>
<li class="chapter" data-level="9.2.2" data-path="collinearity.html"><a href="collinearity.html#what-does-collinearity-do-to-our-regression"><i class="fa fa-check"></i><b>9.2.2</b> What does Collinearity do to our regression?</a></li>
<li class="chapter" data-level="9.2.3" data-path="collinearity.html"><a href="collinearity.html#how-to-test-for-collinearity"><i class="fa fa-check"></i><b>9.2.3</b> How to test for Collinearity?</a></li>
<li class="chapter" data-level="9.2.4" data-path="collinearity.html"><a href="collinearity.html#an-application-2"><i class="fa fa-check"></i><b>9.2.4</b> An Application:</a></li>
<li class="chapter" data-level="9.2.5" data-path="collinearity.html"><a href="collinearity.html#how-do-we-remove-collinearity"><i class="fa fa-check"></i><b>9.2.5</b> How do we remove Collinearity?</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html"><i class="fa fa-check"></i><b>9.3</b> Heteroskedasticity</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#pure-versus-impure-heteroskedasticity"><i class="fa fa-check"></i><b>9.3.1</b> Pure versus Impure Heteroskedasticity</a></li>
<li class="chapter" data-level="9.3.2" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#consequences-of-heteroskedasticity"><i class="fa fa-check"></i><b>9.3.2</b> Consequences of Heteroskedasticity</a></li>
<li class="chapter" data-level="9.3.3" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#detection"><i class="fa fa-check"></i><b>9.3.3</b> Detection</a></li>
<li class="chapter" data-level="9.3.4" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#remedies"><i class="fa fa-check"></i><b>9.3.4</b> Remedies</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MBA 8350: Course Companion for Analyzing and Leveraging Data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="collinearity" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Collinearity</h2>
<p>A significant problem in regression analysis arises when two more independent variables are significantly correlated with each other. This is known as collinearity or multicollinearity. A correlation means that two or more variables systematically move together. In regression analysis, movement is <em>information</em> that we use to explain differences or changes in the dependent variable. If independent variables have similar movements due to correlations, then they contain similar (i.e., redundant) information.</p>
<p>Another issue with collinearity is that when two or more variables systematically move together, then it goes against the very interpretation of our estimates: <em>holding all else equal.</em> If the variables aren’t held equal in the data due to collinearity, then our estimates will reflect that by being unable to differentiate the changes in these variables along separate dimensions. Since the information from these independent variables are shared and redundant, then the dimensions from these collinear variables becomes blurred.</p>
<div id="an-application-1" class="section level3" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> An Application</h3>
<p>Consider an application that compares simulated data where two independent variables have different degrees of correlation. The simulated data was generated from the following model:</p>
<p><span class="math display">\[Y_i = 1 + 1 X_{1i} + 1 X_{2i} + \varepsilon_i\]</span></p>
<p>In other words, the simulated data <strong>should</strong> return the same coefficients above <strong>if</strong> there are no problems with the estimation. The exercise will show you how collinearity can become a problem.</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="collinearity.html#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Regression: correlation = 0.3289</span></span>
<span id="cb322-2"><a href="collinearity.html#cb322-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb322-3"><a href="collinearity.html#cb322-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(MDAT<span class="sc">$</span>X31,MDAT<span class="sc">$</span>X32)</span></code></pre></div>
<pre><code>## [1] 0.3289358</code></pre>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="collinearity.html#cb324-1" aria-hidden="true" tabindex="-1"></a>CREG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y3<span class="sc">~</span>X31<span class="sc">+</span>X32,<span class="at">data=</span>MDAT)</span>
<span id="cb324-2"><a href="collinearity.html#cb324-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(CREG)</span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 1.0015735  0.0214445  46.705 &lt; 2.2e-16 ***
## X31         1.0152385  0.0401048  25.315 &lt; 2.2e-16 ***
## X32         0.9905016  0.0099267  99.781 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="collinearity.html#cb326-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Regression: correlation = 0.938</span></span>
<span id="cb326-2"><a href="collinearity.html#cb326-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb326-3"><a href="collinearity.html#cb326-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(MDAT<span class="sc">$</span>X21,MDAT<span class="sc">$</span>X22)</span></code></pre></div>
<pre><code>## [1] 0.9380521</code></pre>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="collinearity.html#cb328-1" aria-hidden="true" tabindex="-1"></a>CREG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y2<span class="sc">~</span>X21<span class="sc">+</span>X22,<span class="at">data=</span>MDAT)</span>
<span id="cb328-2"><a href="collinearity.html#cb328-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(CREG)</span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 1.001574   0.021445  46.705 &lt; 2.2e-16 ***
## X21         1.100724   0.109304  10.070 &lt; 2.2e-16 ***
## X22         0.905016   0.099267   9.117 1.082e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="collinearity.html#cb330-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Regression: correlation = 0.999</span></span>
<span id="cb330-2"><a href="collinearity.html#cb330-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb330-3"><a href="collinearity.html#cb330-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(MDAT<span class="sc">$</span>X11,MDAT<span class="sc">$</span>X12)</span></code></pre></div>
<pre><code>## [1] 0.9992777</code></pre>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="collinearity.html#cb332-1" aria-hidden="true" tabindex="-1"></a>CREG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y1<span class="sc">~</span>X11<span class="sc">+</span>X12,<span class="at">data=</span>MDAT)</span>
<span id="cb332-2"><a href="collinearity.html#cb332-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(CREG)</span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.001574   0.021445 46.7053  &lt; 2e-16 ***
## X11         1.955579   0.996657  1.9621  0.05261 .  
## X12         0.050161   0.992672  0.0505  0.95980    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="collinearity.html#cb334-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Regression: Highest correlation = 1</span></span>
<span id="cb334-2"><a href="collinearity.html#cb334-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb334-3"><a href="collinearity.html#cb334-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(MDAT<span class="sc">$</span>X41,MDAT<span class="sc">$</span>X42)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="collinearity.html#cb336-1" aria-hidden="true" tabindex="-1"></a>CREG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y4<span class="sc">~</span>X41<span class="sc">+</span>X42,<span class="at">data=</span>MDAT)</span>
<span id="cb336-2"><a href="collinearity.html#cb336-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(CREG)</span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 1.003483   0.021342  47.019 &lt; 2.2e-16 ***
## X41         2.002616   0.037857  52.900 &lt; 2.2e-16 ***
## X42               NA         NA      NA        NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The above application considers four sets of data where the only difference is the degree of collinearity between the two independent variables. The first regression has a degree of correlation between <span class="math inline">\(X_{1i}\)</span> and <span class="math inline">\(X_{2i}\)</span> equal to 0.33, and you will see that the regression does a fairly good job at recovering the regression coefficients. The second regression has a degree of correlation between <span class="math inline">\(X_{1i}\)</span> and <span class="math inline">\(X_{2i}\)</span> equal to 0.94, and you will see that the regression is beginning to suffer a bit where both slope estimates are now off by about 10 percent. The Third regression has a degree of correlation between <span class="math inline">\(X_{1i}\)</span> and <span class="math inline">\(X_{2i}\)</span> equal to just shy of perfect (1), and you will see that the regression is now <em>way</em> off from the expected estimates. Finally, the fourth regression has <strong>perfect collinearity</strong> between <span class="math inline">\(X_{1i}\)</span> and <span class="math inline">\(X_{2i}\)</span>, and the regression actually chokes by providing an <em>NA</em> (meaning, not a number) as an answer for the second coefficient. Mathematically, perfect collinearity asks for a computer to divide a number by zero (which computers don’t like to do).</p>
</div>
<div id="what-does-collinearity-do-to-our-regression" class="section level3" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> What does Collinearity do to our regression?</h3>
<p>The takeaway from our application is that collinearity can become a significant problem if the degree of correlation among the independent variables is large enough. What the application does not show is that collinearity also results in excessively large standard errors of the coefficient estimates. Intuitively, if the regression doesn’t know which variable is providing the (redundant) information, then it shows this by placing little precision on the estimate - meaning a large standard deviation. This large standard deviation will impact the significance of estimates via confidence intervals and hypothesis tests.</p>
</div>
<div id="how-to-test-for-collinearity" class="section level3" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> How to test for Collinearity?</h3>
<p>Note that some collinearity exists in every equation. All variables are correlated to some degree (even if completely at random). Therefore, the question is really how much multicollinearity exists in an equation? Is it enough to cause the types of problems we saw in the application above?</p>
<p>There are two characteristics that help detect the degree of collinearity in a regression:</p>
<ul>
<li><p>High simple correlation coefficients</p></li>
<li><p>High Variance Inflation Factors (VIFs)</p></li>
</ul>
<div id="correlation-coefficients" class="section level4 unnumbered">
<h4>Correlation Coefficients</h4>
<p><span class="math display">\[Cov(X_1,X_2)=\frac{1}{n-1} \sum_{i=1}^n (X_{1i}-\bar{X}_1)(X_{2i}-\bar{X}_2)\]</span>
<span class="math display">\[S_{X_1} = \frac{1}{n-1} \sum_{i=1}^n (X_{1i}-\bar{X}_1)^2\]</span>
<span class="math display">\[S_{X_2} = \frac{1}{n-1} \sum_{i=1}^n (X_{2i}-\bar{X}_2)^2\]</span></p>
<p><span class="math display">\[\rho(X_1,X_2) = \frac{Cov(X_1,X_2)}{S_{X_1}S_{X_2}}\]</span></p>
<p>If a simple correlation coefficient between any two explanatory variables, <span class="math inline">\(\rho(X_1,X_2)\)</span>, is high in absolute value, then multicollinearity is a potential problem. Like we saw in the application, high is rather arbitrary. Therefore, researchers settle on a threshold of 0.80. In other words, if you have a correlation of 0.80 or higher, then you are running the risk of having your estimates biased by the existence of collinearity.</p>
<p>The problem with looking at simple correlations is that they are <em>pairwise</em> calculations. In other words, you can only look at two variables at a time. What if a collinearity problem is bigger than just two variables?</p>
</div>
<div id="variance-inflation-factors-vifs" class="section level4 unnumbered">
<h4>Variance Inflation Factors (VIFs)</h4>
<p>Suppose you want to estimate a regression with three independent variables, but you want to test for collinearity first.</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \varepsilon_i\]</span></p>
<p>Correlation coefficients, being pairwise, will not be able to uncover a correlation structure that might exist across <em>all three</em> independent variables.</p>
<p>Take for example three independent variables: a pitcher’s ERA, the number of earned runs, and the number of innings pitched. For those of you (like me) who are unfamiliar with baseball, a pitcher’s ERA is essentially, their earned runs divided by the number of innings pitched. This means that ERA might be positively correlated with earned runs and negatively correlated with innings pitched, but you wouldn’t realize that the correlation is <em>perfect</em> (meaning, equal to 1) unless you consider both variables simultaneously. A Variance Inflation Factor (or VIF) is a method for examining a complete correlation structure on a list of three or more independent variables.</p>
<p>A Variance Inflation Factor (VIF) is calculated in two steps:</p>
<p>First, run an OLS regression where an independent variable (say, X1) takes a turn at being a dependent variable.</p>
<p><span class="math display">\[X_{1i} = a_0 + a_1 X_{2i} + a_2 X_{3i} + u_i\]</span></p>
<p>Note that the original dependent variable <span class="math inline">\((Y_i)\)</span> is NOT in this equation!</p>
<p>The purpose of this auxiliary regression is to see if there is a sophisticated correlation structure between <span class="math inline">\(X_{1i}\)</span> and the right-hand side variables. Conveniently, we already have an <span class="math inline">\(R^2\)</span> which will indicate exactly how much the variation in the left-hand variable is <em>explained</em> by the right-hand variables. The second step takes the <span class="math inline">\(R^2\)</span> from this regression and calculates the VIF for independent variable <span class="math inline">\(X_{1i}\)</span>. Since the VIF impacts the estimated coefficient of <span class="math inline">\(\beta_1\)</span> in the original regression, it is sometimes referred to as <span class="math inline">\(VIF(\hat{\beta}_1)\)</span>:</p>
<p><span class="math display">\[VIF(\hat{\beta}_1) = \frac{1}{1-R^2}\]</span></p>
<p>If we did this for every independent variable in the original regression, we would arrive at three VIF values.</p>
<p><span class="math display">\[X_{1i} = a_0 + a_1 X_{2i} + a_2 X_{3i} + u_i \rightarrow VIF(\hat{\beta}_1) = \frac{1}{1-R^2}\]</span></p>
<p><span class="math display">\[X_{2i} = a_0 + a_1 X_{1i} + a_2 X_{3i} + u_i \rightarrow VIF(\hat{\beta}_2) = \frac{1}{1-R^2}\]</span></p>
<p><span class="math display">\[X_{3i} = a_0 + a_1 X_{1i} + a_2 X_{2i} + u_i \rightarrow VIF(\hat{\beta}_3) = \frac{1}{1-R^2}\]</span></p>
<p>These VIF values will deliver the amount of bias the standard errors each of the estimated coefficients will receive due to the presence of collinearity. In order to determine if there is a problem, we again resort to an arbitrary threshold of <span class="math inline">\(VIF \geq 5\)</span>. Note that since an <span class="math inline">\(R^2\)</span> value is comparable to a correlation coefficient, this VIF measure corresponds to a correlation above 0.8.</p>
</div>
</div>
<div id="an-application-2" class="section level3" number="9.2.4">
<h3><span class="header-section-number">9.2.4</span> An Application:</h3>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="collinearity.html#cb338-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readxl)</span>
<span id="cb338-2"><a href="collinearity.html#cb338-2" aria-hidden="true" tabindex="-1"></a>MULTI2 <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;data/MULTI2.xlsx&quot;</span>)</span>
<span id="cb338-3"><a href="collinearity.html#cb338-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(MULTI2)</span></code></pre></div>
<pre><code>## [1] &quot;Team&quot;          &quot;League&quot;        &quot;Wins&quot;          &quot;ERA&quot;           &quot;Runs&quot;          &quot;Hits_Allowed&quot; 
## [7] &quot;Walks_Allowed&quot; &quot;Saves&quot;         &quot;Errors&quot;</code></pre>
<p>Suppose that you want to explain why some baseball teams recorded more wins than others by looking at the season statistics listed above. Before we run a full regression with <em>Wins</em> as the dependent variable and the other right variables as independent variables, we need to test for collinearity.</p>
<p>If we were to follow the steps above for each independent variable, we will need to calculate seven VIF values (Team isn’t a variable… it’s a name). This is a lot easier done than said in R:</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="collinearity.html#cb340-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate the &#39;intended&#39; model:</span></span>
<span id="cb340-2"><a href="collinearity.html#cb340-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb340-3"><a href="collinearity.html#cb340-3" aria-hidden="true" tabindex="-1"></a>REG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Wins <span class="sc">~</span> League <span class="sc">+</span> ERA <span class="sc">+</span> Runs <span class="sc">+</span> Hits_Allowed <span class="sc">+</span> Walks_Allowed <span class="sc">+</span> Saves <span class="sc">+</span> Errors, <span class="at">data =</span> MULTI2)</span>
<span id="cb340-4"><a href="collinearity.html#cb340-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb340-5"><a href="collinearity.html#cb340-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Use REG object to call vif command:</span></span>
<span id="cb340-6"><a href="collinearity.html#cb340-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb340-7"><a href="collinearity.html#cb340-7" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(REG)</span></code></pre></div>
<pre><code>##        League           ERA          Runs  Hits_Allowed Walks_Allowed         Saves        Errors 
##      1.221101     11.026091      1.279997      6.342662      3.342659      1.762577      1.548678</code></pre>
<p>The output above shows a VIF for each of the independent variables. The largest are for ERA and Hits Allowed, and these are problematic given that they are above our threshold of 5.
So now that we detected collinearity… what do we do about it?</p>
</div>
<div id="how-do-we-remove-collinearity" class="section level3" number="9.2.5">
<h3><span class="header-section-number">9.2.5</span> How do we remove Collinearity?</h3>
<p>There are several ways to remove or reduce the degree of collinearity that vary in degrees of feasibility and effectiveness.</p>
<p>First, is the collinearity problem due to the inherent nature of the variables themselves or is it a coincidence with your current sample? If it is coincidence, then the problem might go away if you collected more observations. Note that this might not always work, and sometimes more data isn’t even available. However, it is a easy first pass if feasible.</p>
<p>Second, one could always <strong>ignore</strong> collinearity and proceed with the analysis. The reason for this is that while collinearity might bias the standard errors of the estimates, the bias might not be that bad. Think of increasing the value of zero by 100 times.</p>
<p>For example, lets try the ignorance approach with the baseball application above:</p>
<table style="width:90%;">
<colgroup>
<col width="27%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">69.28</td>
<td align="center">13.64</td>
<td align="center">5.08</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>League</strong></td>
<td align="center">1.85</td>
<td align="center">1.01</td>
<td align="center">1.82</td>
<td align="center">0.08</td>
</tr>
<tr class="odd">
<td align="center"><strong>ERA</strong></td>
<td align="center">-6.06</td>
<td align="center">3.44</td>
<td align="center">-1.76</td>
<td align="center">0.09</td>
</tr>
<tr class="even">
<td align="center"><strong>Runs</strong></td>
<td align="center">0.09</td>
<td align="center">0.01</td>
<td align="center">11.52</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="center"><strong>Hits_Allowed</strong></td>
<td align="center">-0.03</td>
<td align="center">0.01</td>
<td align="center">-1.79</td>
<td align="center">0.09</td>
</tr>
<tr class="even">
<td align="center"><strong>Walks_Allowed</strong></td>
<td align="center">-0.03</td>
<td align="center">0.01</td>
<td align="center">-2.26</td>
<td align="center">0.03</td>
</tr>
<tr class="odd">
<td align="center"><strong>Saves</strong></td>
<td align="center">0.54</td>
<td align="center">0.08</td>
<td align="center">7.07</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"><strong>Errors</strong></td>
<td align="center">0</td>
<td align="center">0.04</td>
<td align="center">0.1</td>
<td align="center">0.92</td>
</tr>
</tbody>
</table>
<table style="width:86%;">
<caption>Fitting linear model: Wins ~ League + ERA + Runs + Hits_Allowed + Walks_Allowed + Saves + Errors</caption>
<colgroup>
<col width="20%" />
<col width="30%" />
<col width="11%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">30</td>
<td align="center">2.5</td>
<td align="center">0.96</td>
<td align="center">0.95</td>
</tr>
</tbody>
</table>
<p>The results suggest that the population coefficients for the variables League, ERA, Hits Allowed, and Errors are all insignificantly different from zero with 95% confidence. Now if they were all significant, then we could possibly ignore any potential collinearity issues because the bias would not be <em>enough</em> for us to see if there was a problem. However, since two of these insignificant variables are ones we already identified as having a collinearity problem, then we are unable to go this route.</p>
<p>The third option to remove collinearity is to remove independent variables until the correlation structure is removed. The way to proceed down this route is to remove the variables (one-at-a-time) with the highest VIF values until all remaining values have VIF values below 5. The good side of this analysis is that you can now proceed with the main regression knowing that collinearity is not a problem. The bad side is that you might have had to remove variables that you really wanted to have in the regression.</p>
<p>The VIF values from the baseball analysis suggest that ERA and Hits Allowed are two variables that potentially need to be removed from the analysis due to collinearity. The way to proceed is that if we were to only remove one variable at a time, we will remove the variable with the <em>highest</em> VIF because it is the one that has the most redundant information.</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="collinearity.html#cb342-1" aria-hidden="true" tabindex="-1"></a>REG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Wins <span class="sc">~</span> League <span class="sc">+</span> Runs <span class="sc">+</span> Hits_Allowed <span class="sc">+</span> Walks_Allowed <span class="sc">+</span> Saves <span class="sc">+</span> Errors, <span class="at">data =</span> MULTI2)</span>
<span id="cb342-2"><a href="collinearity.html#cb342-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb342-3"><a href="collinearity.html#cb342-3" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(REG)</span></code></pre></div>
<pre><code>##        League          Runs  Hits_Allowed Walks_Allowed         Saves        Errors 
##      1.149383      1.279914      1.365583      1.235945      1.665172      1.546465</code></pre>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="collinearity.html#cb344-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(REG)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Wins ~ League + Runs + Hits_Allowed + Walks_Allowed + 
##     Saves + Errors, data = MULTI2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.8127 -2.0776  0.0551  2.0168  4.9951 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   83.214595  11.607524   7.169 2.67e-07 ***
## League         2.278948   1.026010   2.221   0.0365 *  
## Runs           0.088445   0.008031  11.013 1.20e-10 ***
## Hits_Allowed  -0.047231   0.006840  -6.905 4.86e-07 ***
## Walks_Allowed -0.043122   0.007485  -5.761 7.22e-06 ***
## Saves          0.569301   0.077227   7.372 1.69e-07 ***
## Errors         0.001322   0.043722   0.030   0.9761    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.615 on 23 degrees of freedom
## Multiple R-squared:  0.9557, Adjusted R-squared:  0.9441 
## F-statistic: 82.65 on 6 and 23 DF,  p-value: 2.119e-14</code></pre>
<p>The regression with ERA removed now is free of collinearity. We can confirm this by the fact that all VIF values of the remaining independent variables are well below 5. The regression results suggest that after removing ERA, ERA and Hits Allowed now have population coefficients that were significantly different than zero with 95% confidence. Errors is still an insignificant variable. This suggests that the insignificance wasn’t due to collinearity. It’s simply the fact that Errors do not significantly help us explain why some teams win more games than others.</p>
<div id="sometimes-removing-collinearity-might-involve-multiple-rounds" class="section level4 unnumbered">
<h4>Sometimes removing collinearity might involve multiple rounds</h4>
<p>You will note from the application above that we only needed to remove one independent variable, so only one round of VIF calculations displayed values above 5. It might sometimes be the case that even after you remove an independent variable, the next round of VIF values reports a value of 5. If this happens, you simply repeat the process by removing the variable with the highest VIF and check again. In general, a complete removal of multicollinearity involves the following:</p>
<ol style="list-style-type: decimal">
<li><p>calculate VIFs for your data set</p></li>
<li><p>drop the variable with the highest VIF (greater than 5)</p></li>
<li><p>calculate VIFs on your data again (with the dropped variable no longer in the data set)</p></li>
<li><p>drop the variable with the highest VIF (greater than 5)</p></li>
<li><p>this is repeated until all VIFs are less than 5</p></li>
</ol>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="nonlinear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="heteroskedasticity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sjdres/MBA8350_Companion/edit/master/09-Advanced.Rmd",
"text": "Suggest an edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/sjdres/MBA8350_Companion/blob/master/09-Advanced.Rmd",
"text": null
},
"download": ["MBA8350book.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
