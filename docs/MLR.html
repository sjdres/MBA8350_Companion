<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Multiple Linear Regression | MBA 8350: Analyzing and Leveraging Data   The Course Companion</title>
  <meta name="description" content="This is a course companion for MBA 8350." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Multiple Linear Regression | MBA 8350: Analyzing and Leveraging Data   The Course Companion" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a course companion for MBA 8350." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Multiple Linear Regression | MBA 8350: Analyzing and Leveraging Data   The Course Companion" />
  
  <meta name="twitter:description" content="This is a course companion for MBA 8350." />
  

<meta name="author" content="Scott Dressler" />


<meta name="date" content="2021-06-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="SLR.html"/>
<link rel="next" href="Advanced.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MBA 8350 Course Companion</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#about-this-book"><i class="fa fa-check"></i>About this book…</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#the-big-picture-of-statistics"><i class="fa fa-check"></i><b>1.1</b> The “Big Picture” of Statistics</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#the-vocabulary-of-statistics"><i class="fa fa-check"></i><b>1.2</b> The Vocabulary of Statistics</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#descriptive-measures"><i class="fa fa-check"></i><b>1.3</b> Descriptive Measures</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#central-tendency"><i class="fa fa-check"></i><b>1.3.1</b> Central Tendency</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#variation"><i class="fa fa-check"></i><b>1.3.2</b> Variation</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#measures-of-shape"><i class="fa fa-check"></i><b>1.3.3</b> Measures of shape</a></li>
<li class="chapter" data-level="1.3.4" data-path="intro.html"><a href="intro.html#covariance-and-correlation"><i class="fa fa-check"></i><b>1.3.4</b> Covariance and Correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>2</b> Data Collection and Sampling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="Data.html"><a href="Data.html#sampling-distributions"><i class="fa fa-check"></i><b>2.1</b> Sampling Distributions</a></li>
<li class="chapter" data-level="2.2" data-path="Data.html"><a href="Data.html#sampling-bias---two-examples"><i class="fa fa-check"></i><b>2.2</b> Sampling Bias - two examples</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="Data.html"><a href="Data.html#dewey-defeats-truman"><i class="fa fa-check"></i><b>2.2.1</b> Dewey Defeats Truman?</a></li>
<li class="chapter" data-level="2.2.2" data-path="Data.html"><a href="Data.html#section-1"><i class="fa fa-check"></i><b>2.2.2</b> 98.6?</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="Data.html"><a href="Data.html#sampling-methods"><i class="fa fa-check"></i><b>2.3</b> Sampling Methods</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="Data.html"><a href="Data.html#simple-random-sampling"><i class="fa fa-check"></i><b>2.3.1</b> Simple random sampling</a></li>
<li class="chapter" data-level="2.3.2" data-path="Data.html"><a href="Data.html#systematic-sampling"><i class="fa fa-check"></i><b>2.3.2</b> Systematic Sampling</a></li>
<li class="chapter" data-level="2.3.3" data-path="Data.html"><a href="Data.html#stratified-sampling"><i class="fa fa-check"></i><b>2.3.3</b> Stratified Sampling</a></li>
<li class="chapter" data-level="2.3.4" data-path="Data.html"><a href="Data.html#cluster-sampling"><i class="fa fa-check"></i><b>2.3.4</b> Cluster Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="Data.html"><a href="Data.html#sampling-in-practice"><i class="fa fa-check"></i><b>2.4</b> Sampling in Practice</a></li>
<li class="chapter" data-level="2.5" data-path="Data.html"><a href="Data.html#sampling-and-sampling-distributions"><i class="fa fa-check"></i><b>2.5</b> Sampling and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="Data.html"><a href="Data.html#an-application"><i class="fa fa-check"></i><b>2.5.1</b> An Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="R.html"><a href="R.html"><i class="fa fa-check"></i><b>3</b> Getting Started with R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="R.html"><a href="R.html#the-r-project-for-statistical-computing"><i class="fa fa-check"></i><b>3.1</b> The R Project for Statistical Computing</a></li>
<li class="chapter" data-level="3.2" data-path="R.html"><a href="R.html#downloading-and-installing-r"><i class="fa fa-check"></i><b>3.2</b> Downloading and installing R</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="R.html"><a href="R.html#choosing-a-mirror"><i class="fa fa-check"></i><b>3.2.1</b> Choosing a <em>Mirror</em></a></li>
<li class="chapter" data-level="3.2.2" data-path="R.html"><a href="R.html#download-and-install-the-correct-version"><i class="fa fa-check"></i><b>3.2.2</b> Download and install the correct version</a></li>
<li class="chapter" data-level="3.2.3" data-path="R.html"><a href="R.html#downloading-and-installing-rstudio"><i class="fa fa-check"></i><b>3.2.3</b> Downloading and installing RStudio</a></li>
<li class="chapter" data-level="3.2.4" data-path="R.html"><a href="R.html#taking-stock"><i class="fa fa-check"></i><b>3.2.4</b> Taking Stock</a></li>
<li class="chapter" data-level="3.2.5" data-path="R.html"><a href="R.html#installing-packages"><i class="fa fa-check"></i><b>3.2.5</b> Installing <em>Packages</em></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="R.html"><a href="R.html#coding-basics"><i class="fa fa-check"></i><b>3.3</b> Coding Basics</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="R.html"><a href="R.html#assigning-objects"><i class="fa fa-check"></i><b>3.3.1</b> Assigning Objects</a></li>
<li class="chapter" data-level="3.3.2" data-path="R.html"><a href="R.html#listing-adding-and-removing"><i class="fa fa-check"></i><b>3.3.2</b> Listing, Adding, and Removing</a></li>
<li class="chapter" data-level="3.3.3" data-path="R.html"><a href="R.html#loading-data"><i class="fa fa-check"></i><b>3.3.3</b> Loading Data</a></li>
<li class="chapter" data-level="3.3.4" data-path="R.html"><a href="R.html#manipulating-data"><i class="fa fa-check"></i><b>3.3.4</b> Manipulating Data</a></li>
<li class="chapter" data-level="3.3.5" data-path="R.html"><a href="R.html#subsetting-data"><i class="fa fa-check"></i><b>3.3.5</b> Subsetting Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="R.html"><a href="R.html#data-visualization"><i class="fa fa-check"></i><b>3.4</b> Data Visualization</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="R.html"><a href="R.html#histograms"><i class="fa fa-check"></i><b>3.4.1</b> Histograms</a></li>
<li class="chapter" data-level="3.4.2" data-path="R.html"><a href="R.html#line-bar-and-scatter-plots"><i class="fa fa-check"></i><b>3.4.2</b> Line, bar, and Scatter Plots</a></li>
<li class="chapter" data-level="3.4.3" data-path="R.html"><a href="R.html#boxplots"><i class="fa fa-check"></i><b>3.4.3</b> Boxplots</a></li>
<li class="chapter" data-level="3.4.4" data-path="R.html"><a href="R.html#much-more-out-there"><i class="fa fa-check"></i><b>3.4.4</b> Much more out there</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CLT.html"><a href="CLT.html"><i class="fa fa-check"></i><b>4</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="4.1" data-path="CLT.html"><a href="CLT.html#the-clt-formally"><i class="fa fa-check"></i><b>4.1</b> The CLT (Formally)</a></li>
<li class="chapter" data-level="4.2" data-path="CLT.html"><a href="CLT.html#application-1-a-sampling-distribution-with-a-known-population"><i class="fa fa-check"></i><b>4.2</b> Application 1: A Sampling Distribution with a Known Population</a></li>
<li class="chapter" data-level="4.3" data-path="CLT.html"><a href="CLT.html#application-2-a-sampling-distribution-with-an-unknown-population"><i class="fa fa-check"></i><b>4.3</b> Application 2: A Sampling Distribution with an Unknown Population</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="CLT.html"><a href="CLT.html#the-sample"><i class="fa fa-check"></i><b>4.3.1</b> The Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="CLT.html"><a href="CLT.html#the-punchline-1"><i class="fa fa-check"></i><b>4.4</b> The Punchline</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.1" data-path="CI.html"><a href="CI.html#a-refresher-on-probability"><i class="fa fa-check"></i><b>5.1</b> A Refresher on Probability</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="CI.html"><a href="CI.html#application-1"><i class="fa fa-check"></i><b>5.1.1</b> Application 1</a></li>
<li class="chapter" data-level="5.1.2" data-path="CI.html"><a href="CI.html#application-2"><i class="fa fa-check"></i><b>5.1.2</b> Application 2</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="CI.html"><a href="CI.html#deriving-a-confidence-interval"><i class="fa fa-check"></i><b>5.2</b> Deriving a Confidence Interval</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="CI.html"><a href="CI.html#application-3"><i class="fa fa-check"></i><b>5.2.1</b> Application 3</a></li>
<li class="chapter" data-level="5.2.2" data-path="CI.html"><a href="CI.html#what-if-we-want-to-change-confidence"><i class="fa fa-check"></i><b>5.2.2</b> What if we want to change confidence?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="CI.html"><a href="CI.html#what-to-do-when-we-do-not-know-sigma"><i class="fa fa-check"></i><b>5.3</b> What to do when we do not know <span class="math inline">\(\sigma\)</span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="CI.html"><a href="CI.html#t-distribution-versus-z-distribution"><i class="fa fa-check"></i><b>5.3.1</b> t distribution versus Z distribution…</a></li>
<li class="chapter" data-level="5.3.2" data-path="CI.html"><a href="CI.html#application-4"><i class="fa fa-check"></i><b>5.3.2</b> Application 4</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="CI.html"><a href="CI.html#determining-sample-size"><i class="fa fa-check"></i><b>5.4</b> Determining Sample Size</a></li>
<li class="chapter" data-level="5.5" data-path="CI.html"><a href="CI.html#concluding-applications"><i class="fa fa-check"></i><b>5.5</b> Concluding Applications</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="CI.html"><a href="CI.html#light-bulbs-last-time"><i class="fa fa-check"></i><b>5.5.1</b> Light Bulbs (Last Time)</a></li>
<li class="chapter" data-level="5.5.2" data-path="CI.html"><a href="CI.html#returning-to-the-philadelphia-school-policy-application"><i class="fa fa-check"></i><b>5.5.2</b> Returning to the Philadelphia School Policy Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="HT.html"><a href="HT.html"><i class="fa fa-check"></i><b>6</b> Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="6.1" data-path="HT.html"><a href="HT.html#anatomy-of-a-hypothesis-test"><i class="fa fa-check"></i><b>6.1</b> Anatomy of a Hypothesis Test</a></li>
<li class="chapter" data-level="6.2" data-path="HT.html"><a href="HT.html#two-methods-for-conducting-a-hypothesis-test-when-sigma-is-known"><i class="fa fa-check"></i><b>6.2</b> Two methods for conducting a hypothesis test (when <span class="math inline">\(\sigma\)</span> is known)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="HT.html"><a href="HT.html#rejection-region-method"><i class="fa fa-check"></i><b>6.2.1</b> Rejection Region Method</a></li>
<li class="chapter" data-level="6.2.2" data-path="HT.html"><a href="HT.html#p-value-approach"><i class="fa fa-check"></i><b>6.2.2</b> P-value Approach</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="HT.html"><a href="HT.html#two-sided-vs-one-sided-test"><i class="fa fa-check"></i><b>6.3</b> Two-sided vs One-sided Test</a></li>
<li class="chapter" data-level="6.4" data-path="HT.html"><a href="HT.html#conducting-a-hypothesis-test-when-sigma-is-unknown"><i class="fa fa-check"></i><b>6.4</b> Conducting a hypothesis test (when <span class="math inline">\(\sigma\)</span> is unknown)</a></li>
<li class="chapter" data-level="6.5" data-path="HT.html"><a href="HT.html#appendix-a-note-on-calculating-p-values"><i class="fa fa-check"></i><b>6.5</b> Appendix: A note on calculating P-values</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="HT.html"><a href="HT.html#the-problem"><i class="fa fa-check"></i><b>6.5.1</b> The Problem</a></li>
<li class="chapter" data-level="6.5.2" data-path="HT.html"><a href="HT.html#how-to-calculate-p-values"><i class="fa fa-check"></i><b>6.5.2</b> How to calculate p-values</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="SLR.html"><a href="SLR.html"><i class="fa fa-check"></i><b>7</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="SLR.html"><a href="SLR.html#a-simple-linear-regression-model"><i class="fa fa-check"></i><b>7.1</b> A Simple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="SLR.html"><a href="SLR.html#what-does-a-regression-model-imply"><i class="fa fa-check"></i><b>7.1.1</b> What does a regression model imply?</a></li>
<li class="chapter" data-level="7.1.2" data-path="SLR.html"><a href="SLR.html#the-real-simple-linear-regression-model"><i class="fa fa-check"></i><b>7.1.2</b> The <em>REAL</em> Simple Linear Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="SLR.html"><a href="SLR.html#application-predicting-house-price-based-on-house-size"><i class="fa fa-check"></i><b>7.2</b> Application: Predicting House Price Based on House Size</a></li>
<li class="chapter" data-level="7.3" data-path="SLR.html"><a href="SLR.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>7.3</b> Ordinary Least Squares (OLS)</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="SLR.html"><a href="SLR.html#b.l.u.e."><i class="fa fa-check"></i><b>7.3.1</b> B.L.U.E.</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="SLR.html"><a href="SLR.html#decomposition-of-variance"><i class="fa fa-check"></i><b>7.4</b> Decomposition of Variance</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="SLR.html"><a href="SLR.html#the-r2"><i class="fa fa-check"></i><b>7.4.1</b> The <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="7.4.2" data-path="SLR.html"><a href="SLR.html#what-is-a-good-r2"><i class="fa fa-check"></i><b>7.4.2</b> What is a <em>good</em> <span class="math inline">\(R^2\)</span>?</a></li>
<li class="chapter" data-level="7.4.3" data-path="SLR.html"><a href="SLR.html#standard-error-of-the-estimate"><i class="fa fa-check"></i><b>7.4.3</b> Standard Error of the Estimate</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="SLR.html"><a href="SLR.html#assumptions-of-the-linear-regression-model"><i class="fa fa-check"></i><b>7.5</b> Assumptions of the Linear Regression Model</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="SLR.html"><a href="SLR.html#linearity"><i class="fa fa-check"></i><b>7.5.1</b> Linearity</a></li>
<li class="chapter" data-level="7.5.2" data-path="SLR.html"><a href="SLR.html#independence-of-errors"><i class="fa fa-check"></i><b>7.5.2</b> Independence of Errors</a></li>
<li class="chapter" data-level="7.5.3" data-path="SLR.html"><a href="SLR.html#equal-variance"><i class="fa fa-check"></i><b>7.5.3</b> Equal Variance</a></li>
<li class="chapter" data-level="7.5.4" data-path="SLR.html"><a href="SLR.html#normality-of-errors"><i class="fa fa-check"></i><b>7.5.4</b> Normality of Errors</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="SLR.html"><a href="SLR.html#statistical-inference"><i class="fa fa-check"></i><b>7.6</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="SLR.html"><a href="SLR.html#confidence-intervals-around-population-parameters"><i class="fa fa-check"></i><b>7.6.1</b> Confidence Intervals (around population parameters)</a></li>
<li class="chapter" data-level="7.6.2" data-path="SLR.html"><a href="SLR.html#hypothesis-tests"><i class="fa fa-check"></i><b>7.6.2</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="7.6.3" data-path="SLR.html"><a href="SLR.html#confidence-intervals-around-forecasts"><i class="fa fa-check"></i><b>7.6.3</b> Confidence Intervals (around forecasts)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="MLR.html"><a href="MLR.html"><i class="fa fa-check"></i><b>8</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="MLR.html"><a href="MLR.html#application-explaining-house-price-in-a-multiple-regression"><i class="fa fa-check"></i><b>8.1</b> Application: Explaining house price in a multiple regression</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="MLR.html"><a href="MLR.html#the-importance-of-controls"><i class="fa fa-check"></i><b>8.1.1</b> The Importance of “Controls”</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="MLR.html"><a href="MLR.html#adjusted-r2"><i class="fa fa-check"></i><b>8.2</b> Adjusted <span class="math inline">\(R^2\)</span></a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="MLR.html"><a href="MLR.html#abusing-an-r2"><i class="fa fa-check"></i><b>8.2.1</b> Abusing an <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="8.2.2" data-path="MLR.html"><a href="MLR.html#an-adjusted-r2"><i class="fa fa-check"></i><b>8.2.2</b> An <em>Adjusted</em> <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="MLR.html"><a href="MLR.html#qualitative-dummy-variables"><i class="fa fa-check"></i><b>8.3</b> Qualitative (Dummy) Variables</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="MLR.html"><a href="MLR.html#intercept-dummy-variable"><i class="fa fa-check"></i><b>8.3.1</b> Intercept dummy variable</a></li>
<li class="chapter" data-level="8.3.2" data-path="MLR.html"><a href="MLR.html#slope-dummy-variable"><i class="fa fa-check"></i><b>8.3.2</b> Slope dummy variable</a></li>
<li class="chapter" data-level="8.3.3" data-path="MLR.html"><a href="MLR.html#what-if-there-are-more-than-two-categories"><i class="fa fa-check"></i><b>8.3.3</b> What if there are more than two categories?</a></li>
<li class="chapter" data-level="8.3.4" data-path="MLR.html"><a href="MLR.html#a-final-application"><i class="fa fa-check"></i><b>8.3.4</b> A Final Application</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="MLR.html"><a href="MLR.html#joint-hypothesis-tests"><i class="fa fa-check"></i><b>8.4</b> Joint Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="MLR.html"><a href="MLR.html#simple-hypothesis-tests"><i class="fa fa-check"></i><b>8.4.1</b> Simple Hypothesis Tests</a></li>
<li class="chapter" data-level="8.4.2" data-path="MLR.html"><a href="MLR.html#simple-versus-joint-tests"><i class="fa fa-check"></i><b>8.4.2</b> Simple versus Joint Tests</a></li>
<li class="chapter" data-level="8.4.3" data-path="MLR.html"><a href="MLR.html#applications"><i class="fa fa-check"></i><b>8.4.3</b> Applications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Advanced.html"><a href="Advanced.html"><i class="fa fa-check"></i><b>9</b> Advanced Regression Topics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="Advanced.html"><a href="Advanced.html#nonlinear-models"><i class="fa fa-check"></i><b>9.1</b> Nonlinear Models</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="Advanced.html"><a href="Advanced.html#derivatives"><i class="fa fa-check"></i><b>9.1.1</b> Derivatives</a></li>
<li class="chapter" data-level="9.1.2" data-path="Advanced.html"><a href="Advanced.html#why-consider-non-linear-relationships"><i class="fa fa-check"></i><b>9.1.2</b> Why consider non-linear relationships?</a></li>
<li class="chapter" data-level="9.1.3" data-path="Advanced.html"><a href="Advanced.html#functional-forms"><i class="fa fa-check"></i><b>9.1.3</b> Functional Forms</a></li>
<li class="chapter" data-level="9.1.4" data-path="Advanced.html"><a href="Advanced.html#the-log-transformation"><i class="fa fa-check"></i><b>9.1.4</b> The Log transformation</a></li>
<li class="chapter" data-level="9.1.5" data-path="Advanced.html"><a href="Advanced.html#the-quadratic-transformation"><i class="fa fa-check"></i><b>9.1.5</b> The Quadratic transformation</a></li>
<li class="chapter" data-level="9.1.6" data-path="Advanced.html"><a href="Advanced.html#the-reciprocal-transformation"><i class="fa fa-check"></i><b>9.1.6</b> The Reciprocal transformation</a></li>
<li class="chapter" data-level="9.1.7" data-path="Advanced.html"><a href="Advanced.html#conclusion"><i class="fa fa-check"></i><b>9.1.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="Advanced.html"><a href="Advanced.html#collinearity"><i class="fa fa-check"></i><b>9.2</b> Collinearity</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="Advanced.html"><a href="Advanced.html#an-application-1"><i class="fa fa-check"></i><b>9.2.1</b> An Application</a></li>
<li class="chapter" data-level="9.2.2" data-path="Advanced.html"><a href="Advanced.html#what-does-collinearity-do-to-our-regression"><i class="fa fa-check"></i><b>9.2.2</b> What does Collinearity do to our regression?</a></li>
<li class="chapter" data-level="9.2.3" data-path="Advanced.html"><a href="Advanced.html#how-to-test-for-collinearity"><i class="fa fa-check"></i><b>9.2.3</b> How to test for Collinearity?</a></li>
<li class="chapter" data-level="9.2.4" data-path="Advanced.html"><a href="Advanced.html#an-application-2"><i class="fa fa-check"></i><b>9.2.4</b> An Application:</a></li>
<li class="chapter" data-level="9.2.5" data-path="Advanced.html"><a href="Advanced.html#how-do-we-remove-collinearity"><i class="fa fa-check"></i><b>9.2.5</b> How do we remove Collinearity?</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="Advanced.html"><a href="Advanced.html#heteroskedasticity"><i class="fa fa-check"></i><b>9.3</b> Heteroskedasticity</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="Advanced.html"><a href="Advanced.html#pure-versus-impure-heteroskedasticity"><i class="fa fa-check"></i><b>9.3.1</b> Pure versus Impure Heteroskedasticity</a></li>
<li class="chapter" data-level="9.3.2" data-path="Advanced.html"><a href="Advanced.html#consequences-of-heteroskedasticity"><i class="fa fa-check"></i><b>9.3.2</b> Consequences of Heteroskedasticity</a></li>
<li class="chapter" data-level="9.3.3" data-path="Advanced.html"><a href="Advanced.html#detection"><i class="fa fa-check"></i><b>9.3.3</b> Detection</a></li>
<li class="chapter" data-level="9.3.4" data-path="Advanced.html"><a href="Advanced.html#remedies"><i class="fa fa-check"></i><b>9.3.4</b> Remedies</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MBA 8350: Analyzing and Leveraging Data <br> The Course Companion</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="MLR" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Multiple Linear Regression</h1>
<p><em>Sometimes one independent variable just doesn’t cut it.</em></p>
<p><span class="math display">\[PRF:\;Y_i=\beta_0+\beta_1X_{1i}+\beta_2X_{2i}+...+\beta_kX_{ki}+\varepsilon_i\]</span></p>
<p><span class="math display">\[SRF:\;Y_i=\hat{\beta}_0+\hat{\beta}_1X_{1i}+\hat{\beta}_2X_{2i}+...+\hat{\beta}_kX_{ki}+e_i\]</span></p>
<p>A <strong>Multiple Regression Model</strong> is a direct extension of the <strong>Simple Regression Model</strong> by adding additional independent variables. Adding additional independent variables allows the regression to use more information when trying to explain movements in the single dependent variable. In other words, multiple independent variables can explain changes in the dependent variable along different <em>dimensions</em>.</p>
<p>The multiple regression model has a lot in common with the simple regression model.</p>
<ol style="list-style-type: decimal">
<li><p>It is still solved via OLS. The first-order conditions are a bit more complicated than those stemming from a simple regression, but they are conceptually the same.</p></li>
<li><p>It still contains an intercept term and a residual term.</p></li>
<li><p>This is still a <em>line</em> equation - only it is a multi-dimensional line equation (i.e., a plane in the case of two dimensions).</p></li>
</ol>
<p>The only significant change we need to make is with respect to interpretation of the slope coefficients. These slope coefficients still deliver the <em>expected or average change in the dependent variable</em> given a unit change in an independent variable. However, since we are looking at multiple independent variables simultaneously, we need to be <strong>explicit</strong> that we are examining the relationships <em>one independent variable at a time</em>. In other words, when we examine the relationship between the dependent variable and a particular independent variable, we need to explicitly state that we are holding all other independent variables <em>constant</em>.</p>
<p><span class="math display">\[\beta_k=\frac{\Delta Y_i}{\Delta X_{ki}}\]</span></p>
<blockquote>
<p>In the population: a PRF slope coefficient indicates the EXPECTED or AVERAGE change in the dependent variable associated with a one-unit increase in the explanatory variable holding the other explanatory variables constant.</p>
</blockquote>
<p><span class="math display">\[\hat{\beta}_k=\frac{\Delta Y_i}{\Delta X_{ki}}\]</span></p>
<blockquote>
<p>In the sample: a SRF slope coefficient indicates the expected or average change in the dependent variable associated with a one-unit increase in the explanatory variable holding the other explanatory variables constant.</p>
</blockquote>
<div id="application-explaining-house-price-in-a-multiple-regression" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Application: Explaining house price in a multiple regression</h2>
<p>Let us revisit the relationship between house price and house size, but extend the regression model to include the number of bedrooms as a second independent variable.</p>
<p>Our PRF becomes:</p>
<p><span class="math display">\[Price_i=\beta_0+\beta_1Size_i+\beta_2Rooms_i+\varepsilon_i\]</span></p>
<p>Our SRF becomes:</p>
<p><span class="math display">\[Price_i=\hat{\beta}_0+\hat{\beta}_1Size_i+\hat{\beta}_2Rooms_i+e_i\]</span></p>
<p>To visualize what we are about to do, lets start with scatter plots looking at the relationships between the dependent variable and each independent variable.</p>
<p>The figure on the left is the scatter plot between the House Price and House Size. This positive relationship is exactly what we have looked at previously. The figure on the right is the scatter plot between the same House Price but the number of bedrooms each house has. This figure illustrates that the houses in our sample have between 2 and 7 bedrooms (with no half-rooms), and homes with more bedrooms generally have higher prices (as expected). Note that we are looking at the same dependent variable along different <em>dimensions</em>. We can combine these dimensions into a singe (3-Dimensional) figure to see how the relationships between the dependent variable and each independent variable appear simultaneously.</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="MLR.html#cb227-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(hprice1,<span class="at">package=</span><span class="st">&#39;wooldridge&#39;</span>)</span>
<span id="cb227-2"><a href="MLR.html#cb227-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> hprice1<span class="sc">$</span>price</span>
<span id="cb227-3"><a href="MLR.html#cb227-3" aria-hidden="true" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> hprice1<span class="sc">$</span>sqrft</span>
<span id="cb227-4"><a href="MLR.html#cb227-4" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> hprice1<span class="sc">$</span>bdrms</span>
<span id="cb227-5"><a href="MLR.html#cb227-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb227-6"><a href="MLR.html#cb227-6" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb227-7"><a href="MLR.html#cb227-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X1,Y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb227-8"><a href="MLR.html#cb227-8" aria-hidden="true" tabindex="-1"></a>          <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">1</span>,</span>
<span id="cb227-9"><a href="MLR.html#cb227-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">xlab =</span> <span class="st">&quot;House Size&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;House Price&quot;</span>)</span>
<span id="cb227-10"><a href="MLR.html#cb227-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X2,Y, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb227-11"><a href="MLR.html#cb227-11" aria-hidden="true" tabindex="-1"></a>          <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">1</span>,</span>
<span id="cb227-12"><a href="MLR.html#cb227-12" aria-hidden="true" tabindex="-1"></a>          <span class="at">xlab =</span> <span class="st">&quot;Bedrooms&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;House Price&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-121-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="MLR.html#cb228-1" aria-hidden="true" tabindex="-1"></a><span class="fu">scatter3D</span>(X1, X2, Y, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">1</span>, <span class="at">phi =</span> <span class="dv">0</span>,</span>
<span id="cb228-2"><a href="MLR.html#cb228-2" aria-hidden="true" tabindex="-1"></a>          <span class="at">colkey=</span><span class="cn">FALSE</span>, <span class="at">ticktype =</span> <span class="st">&quot;detailed&quot;</span>,</span>
<span id="cb228-3"><a href="MLR.html#cb228-3" aria-hidden="true" tabindex="-1"></a>          <span class="at">xlab =</span> <span class="st">&quot;House Size&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Bedrooms&quot;</span>,</span>
<span id="cb228-4"><a href="MLR.html#cb228-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">zlab =</span> <span class="st">&quot;House Price&quot;</span>, <span class="at">main =</span> <span class="st">&quot;3D Scatterplot&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-122-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>For comparison, suppose that we consider these independent variables one at a time in a simple regression. In particular, we can examine one simple regression model where House Size is the only independent variable, and another simple regression model where Bedrooms is the only independent variable.</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="MLR.html#cb229-1" aria-hidden="true" tabindex="-1"></a>REG1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(hprice1<span class="sc">$</span>price <span class="sc">~</span> hprice1<span class="sc">$</span>sqrft)</span>
<span id="cb229-2"><a href="MLR.html#cb229-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(REG1)</span></code></pre></div>
<pre><code>##   (Intercept) hprice1$sqrft 
##     11.204145      0.140211</code></pre>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="MLR.html#cb231-1" aria-hidden="true" tabindex="-1"></a>REG2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(hprice1<span class="sc">$</span>price <span class="sc">~</span> hprice1<span class="sc">$</span>bdrms)</span>
<span id="cb231-2"><a href="MLR.html#cb231-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(REG2)</span></code></pre></div>
<pre><code>##   (Intercept) hprice1$bdrms 
##      72.23111      62.02456</code></pre>
<p>The regression that only considers house size has a slope coefficient of 0.14. Remember that since house price was denoted in thousands of dollars and house size was denoted in square feet, this slope coefficient states that an additional square foot of house size will increase the average house price by $140.</p>
<p>The regression that only considers number of bedrooms has a slope coefficient of 62. This slope coefficient states that an additional bedroom will increase the average house price by $62,000.</p>
<p>While the results from these two simple regressions make sense, we need to realize that a simple regression model only considers a single independent variable to be important (and throws all of the other information into the garbage can). This means that the first regression takes no notice of the number of rooms a house has, while the second regression takes no notice of the size of the home. Since it is reasonable to assume that bigger homes have more bedrooms, then a regression model that is only given one of these pieces of information might be overstating the quantitative impact of the single independent variable.</p>
<p>To illustrate this, let us run a multiple regression model where both house size and number of bedrooms are considered.</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb233-1"><a href="MLR.html#cb233-1" aria-hidden="true" tabindex="-1"></a>REG3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(hprice1<span class="sc">$</span>price <span class="sc">~</span> </span>
<span id="cb233-2"><a href="MLR.html#cb233-2" aria-hidden="true" tabindex="-1"></a>             hprice1<span class="sc">$</span>sqrft <span class="sc">+</span> hprice1<span class="sc">$</span>bdrms)</span>
<span id="cb233-3"><a href="MLR.html#cb233-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(REG3)</span></code></pre></div>
<pre><code>##   (Intercept) hprice1$sqrft hprice1$bdrms 
##   -19.3149958     0.1284362    15.1981910</code></pre>
<p>The slope with respect to house size is now 0.128 (down from 0.14) while the slope with respect to number of bedrooms is now 15.2 (down from 62). In order to make sense of these changes, let us explicitly interpret these slope coefficients within the context of a multiple regression model (where we can hold all other independent variables constant).</p>
<blockquote>
<p>Holding number of bedrooms constant, an additional square foot of house size will increase a house price by $128, on average.</p>
</blockquote>
<blockquote>
<p>Holding house size constant, an additional bedroom will increase a house price by $15,200, on average.</p>
</blockquote>
<p>The power of a multiple regression comes through when you look at the second slope interpretation. Multiple regression allows us to consider two houses that have the same house size but one house has an additional bedroom. In other words, imagine building a wall that turns one bedroom into two smaller bedrooms. This will increase the expected house price by $15,200, but this is much smaller than the simple regression relationship of $62,000. This is because the simple regression could not differentiate the impact of a bedroom from the impact of an increase in house size. The multiple regression model can.</p>
<p>The final figure shows the 3-dimensional regression line that best fits the sample. You can see that considering multiple dimensions increases the performance of the deterministic component of the model and therefore reduces the amount of information that goes into the garbage can as <em>unpredictable</em>. This can be shown in the last picture that only looks at the relationship from the “Size” dimension. In the figure on the left, the blue dots are the observations in the data, the black line is the regression line from the simple regression model (without Bedrooms), while the red dots are the model predictions from the multiple regression model with Bedrooms included as an additional independent variable. Notice how this allows the regression predictions to veer off of a straight line. This results in slightly less prediction errors showing up in your garbage can - as illustrated in the figure on the right.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-125-1.png" width="624" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb235-1"><a href="MLR.html#cb235-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb235-2"><a href="MLR.html#cb235-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hprice1<span class="sc">$</span>sqrft,hprice1<span class="sc">$</span>price, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb235-3"><a href="MLR.html#cb235-3" aria-hidden="true" tabindex="-1"></a>          <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">1</span>,</span>
<span id="cb235-4"><a href="MLR.html#cb235-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">xlab =</span> <span class="st">&quot;House Size&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;House Price&quot;</span>)</span>
<span id="cb235-5"><a href="MLR.html#cb235-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(hprice1<span class="sc">$</span>sqrft,<span class="fu">fitted</span>(REG1))</span>
<span id="cb235-6"><a href="MLR.html#cb235-6" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(hprice1<span class="sc">$</span>sqrft,<span class="fu">fitted</span>(REG3),<span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb235-7"><a href="MLR.html#cb235-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-8"><a href="MLR.html#cb235-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hprice1<span class="sc">$</span>sqrft,<span class="fu">residuals</span>(REG1), <span class="at">col =</span> <span class="st">&quot;black&quot;</span>,</span>
<span id="cb235-9"><a href="MLR.html#cb235-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">1</span>,</span>
<span id="cb235-10"><a href="MLR.html#cb235-10" aria-hidden="true" tabindex="-1"></a>          <span class="at">xlab =</span> <span class="st">&quot;House Size&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb235-11"><a href="MLR.html#cb235-11" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(hprice1<span class="sc">$</span>sqrft,<span class="fu">residuals</span>(REG3),<span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb235-12"><a href="MLR.html#cb235-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-126-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="the-importance-of-controls" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> The Importance of “Controls”</h3>
<p>One very important item to point out in the last application is exactly why the coefficient on number of bedrooms dropped from $62,000 to $15,200 when the size of the house was added to the regression. The reason can be broken up into two categories.</p>
<p><strong>1. The independent variables are correlated</strong></p>
<p>It seems reasonable to believe that bigger houses have more bedrooms. This means that the size of a house and the number of bedrooms are correlated with each other.</p>
<p><strong>2. “All Else Equal” in a Multiple Regression is more than just words</strong></p>
<p>A multiple regression can separately identify the impact of each independent variable on the dependent variable.</p>
<p>Put together, these two items suggest that when two independent variables are correlated, then they should both appear in the regression model. If not, then the correlation between an included independent variable and an omitted independent variable might lead to <em>omitted variable bias</em>. This is what we saw above in the regression with only number of bedrooms as an independent variable. The coefficient of $62,000 is giving you the combined impact of an additional room <em>and</em> a bigger house. When you add house size as another independent variable, you are now able to determine the expected increase in house price for an additional bedroom <em>holding house size constant</em>.</p>
<p>Bottom line is that even though you are concerned with the results from a particular independent variable, it is important to try and include all independent variables that might be correlated with the independent variable of interest. This attempts to alleviate omitted variable bias.</p>
</div>
</div>
<div id="adjusted-r2" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Adjusted <span class="math inline">\(R^2\)</span></h2>
<p>Regardless of the number of independent variables, the variance of a regression model can be decomposed and a <span class="math inline">\(R^2\)</span> can be calculated.</p>
<p><span class="math display">\[TSS = \sum^{N}_{i=1}(Y_i - \bar{Y})^2\]</span></p>
<p><span class="math display">\[ESS = \sum^{N}_{i=1}(\hat{Y}_i - \bar{Y})^2\]</span></p>
<p><span class="math display">\[RSS = \sum^{N}_{i=1}(Y_i - \hat{Y}_i)^2 = \sum^{N}_{i=1}e_i^2\]</span></p>
<p><span class="math display">\[R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}\]</span></p>
<p>The <span class="math inline">\(R^2\)</span> still delivers the proportion of the variation in the dependent variable explained by the model, only now the model is comprised of multiple independent variables.</p>
<p>An <span class="math inline">\(R^2\)</span> is a very intuitive calculation, but it sometimes might be misleading.</p>
<div id="abusing-an-r2" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Abusing an <span class="math inline">\(R^2\)</span></h3>
<p>No matter how hard I try to downplay the importance of an <span class="math inline">\(R^2\)</span>, students always have the tendency to shoot for that measure to be as close to 1 as possible. The problem with this goal is that an <span class="math inline">\(R^2\)</span> equal to 1 in not necessarily a good thing.</p>
<p>Consider a previous regression where we explained house prices with only the number of bedrooms.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="MLR.html#cb236-1" aria-hidden="true" tabindex="-1"></a>REG1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> bdrms, <span class="at">data =</span> hprice1)</span>
<span id="cb236-2"><a href="MLR.html#cb236-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(REG1)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.2581489</code></pre>
<p>The coefficient of determination states that the number of bedrooms explains slightly over 25 percent of the variation in house prices. If we include the size of the house in the regression,</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="MLR.html#cb238-1" aria-hidden="true" tabindex="-1"></a>REG2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> bdrms <span class="sc">+</span> sqrft, <span class="at">data =</span> hprice1)</span>
<span id="cb238-2"><a href="MLR.html#cb238-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(REG2)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.6319184</code></pre>
<p>we see that the <span class="math inline">\(R^2\)</span> increases to 0.63 as before. If we include yet another variable such as the size of the property,</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="MLR.html#cb240-1" aria-hidden="true" tabindex="-1"></a>REG3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> bdrms <span class="sc">+</span> sqrft <span class="sc">+</span> lotsize, <span class="at">data =</span> hprice1)</span>
<span id="cb240-2"><a href="MLR.html#cb240-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(REG3)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.6723622</code></pre>
<p>we see that the regression now explains over 67 percent of the variation in house prices.</p>
<p>What we are seeing is that the more variables you add the higher the <span class="math inline">\(R^2\)</span> is getting. While this might lead you to believe that we are adding <em>important</em> independent variables to the regression, the problem is that the <span class="math inline">\(R^2\)</span> will go up <strong>no matter what variable you add</strong>. The increase might be slight, but the <span class="math inline">\(R^2\)</span> will never go down.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="MLR.html#cb242-1" aria-hidden="true" tabindex="-1"></a>Xcrap <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">88</span>)</span></code></pre></div>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="MLR.html#cb243-1" aria-hidden="true" tabindex="-1"></a>REG4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> bdrms <span class="sc">+</span> sqrft <span class="sc">+</span> lotsize <span class="sc">+</span> Xcrap, <span class="at">data =</span> hprice1)</span>
<span id="cb243-2"><a href="MLR.html#cb243-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(REG3)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.6723622</code></pre>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="MLR.html#cb245-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(REG4)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.6731707</code></pre>
<p>The exercise above adds a completely random variable as a fourth independent variable. It should have nothing to do with explaining house prices. However, if you generate <em>the correct</em> random variables, then you might get an increase in the <span class="math inline">\(R^2\)</span> by as much as an entire percentage point. Does this say that the random variable actually helps explain variations in house prices? Of course not. What it does show is that sometimes we can abuse the <span class="math inline">\(R^2\)</span>, so we need an additional measure of goodness of fit.</p>
</div>
<div id="an-adjusted-r2" class="section level3" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> An <em>Adjusted</em> <span class="math inline">\(R^2\)</span></h3>
<p>The problem with an <span class="math inline">\(R^2\)</span> is that is will increase no matter what independent variable you throw into the regression. If you think about it, if a regression with two independent variables explains 63 percent of the variation in the dependent variable, then adding a third variable (no matter how silly) will deliver a regression that will explain <em>no less</em> than 63 percent of the variation. We therefore cannot use the <span class="math inline">\(R^2\)</span> as a measure for whether or not we should include an independent variable because we don’t know how <em>big</em> an increase in <span class="math inline">\(R^2\)</span> needs to be. We therefore need a goodness of fit measure that not only has the potential to increase when the added variable is deemed important, but has the potential to decrease when the variable is unimportant. This is called an <em>adjusted</em> <span class="math inline">\(R^2\)</span>.</p>
<p><span class="math display">\[\bar{R}^2 = 1 - \frac{RSS/(N-k-1)}{TSS/(N-1)}\]</span></p>
<p>The main difference between the adjusted <span class="math inline">\(R^2\)</span> and it’s unadjusted measure are the degrees of freedom in the numerator. When you add an additional independent variable, <span class="math inline">\(k\)</span> goes up by one but <span class="math inline">\(N\)</span> stays constant. Also, when adding an additional independent variable, the RSS goes down (which is what delivers an increase in the standard <span class="math inline">\(R^2\)</span>). What you have in the numerator is a cost / benefit analysis. In other words, if the decrease in RSS is greater - then the <span class="math inline">\(\bar{R}^2\)</span> increases and the independent variable of question <em>might be somewhat important</em>. However, if the decrease in <span class="math inline">\(N-k-1\)</span> is greater, then the <span class="math inline">\(\bar{R}^2\)</span> decreases and the independent variable of question is <em>not important</em>.</p>
<div id="conclusion-for-informal-use-only" class="section level4" number="8.2.2.1">
<h4><span class="header-section-number">8.2.2.1</span> Conclusion: for informal use only!</h4>
<p>While the <span class="math inline">\(R^2\)</span> and adjusted <span class="math inline">\(R^2\)</span> are two common measures of goodness of fit, they are informal at best. One can interpret them along the lines of how we did above, but there will more formal measures of whether or not an independent variable improves the forecasts of the regression model. Bottom line: these measures can give some insight to the results of a regression model, but they aren’t anything worth hanging you final conclusions on.</p>
</div>
</div>
</div>
<div id="qualitative-dummy-variables" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Qualitative (Dummy) Variables</h2>
<p>Quantitative variables are easy to model and interpret because they take on numerical values and are readily dealt with by computers. Qualitative variables, however, are variables that do not naturally deliver numerical values. Examples of qualitative variables are:</p>
<ul>
<li><p>Gender (male, female)</p></li>
<li><p>Marital status (yes, no)</p></li>
<li><p>Ethnicity (white, Hispanic, Asian, etc.)</p></li>
</ul>
<p>Qualitative variables are made operational for regression analysis by creating <strong>dummy variables</strong>. A dummy variable can only take on two values (i.e., 0 or 1) and should be thought of as a <em>switch</em>.</p>
<ul>
<li><p>1 implies the switch is <em>on</em>, meaning that the designated trait is present for an individual observation.</p></li>
<li><p>0 implies the switch is <em>off</em>, meaning that the trait is absent for an individual observation.</p></li>
</ul>
<p>We can consider two different types of dummy variables depending on if we model the presence or absence of a trait to impact the intercept of the model or the relationship (or slope) between the dependent variable and other independent variables. We will cover these in turn.</p>
<div id="intercept-dummy-variable" class="section level3" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Intercept dummy variable</h3>
<p>An intercept dummy variable is a qualitative variable that <em>stands alone</em> in a regression just like other quantitative variables we have encountered. Let us illustrate this by adding an intercept dummy variable to a wage analysis.</p>
<p>Suppose you are a consultant hired by a firm to help determine the underlying features of the current wage structure for their employees. You want to understand why some wage rates are different from others. Let our dependent variable be <em>wage</em> (the hourly wage of an individual employee) and the independent variables be given by…</p>
<ul>
<li><p><em>educ</em> be the total years of education of an individual employee</p></li>
<li><p><em>exper</em> be the total years of experience an individual employee had prior to starting with the company</p></li>
<li><p><em>tenure</em> is the number of years an employee has been working with the firm.</p></li>
</ul>
<p>These independent variables are all <em>quantitative</em> because they directly translate to numbers, and the model considered previously allowed us to analyze if these independent variables helped us explain why some people earn a higher wage than others.</p>
<p>We can also add a qualitative variable to this list of independent variables to see if gender can help explain why some people earn a higher wage than others. In particular, consider the qualitative variable <em>female</em> which equals 1 if the individual is female and 0 if the individual is not (i.e., male).</p>
<p>The Specified model (the PRF) now becomes</p>
<p><span class="math display">\[wage_i=\beta_0+\beta_1educ_i+\beta_2exper_i+\beta_3tenure_i+\beta_4female_i+\varepsilon_i\]</span></p>
<p>Note that the slope of the three quantitative variables considered earlier are completely unchanged. The slope with respect to the dummy variable is similar, but needs to be interpreted in a specific manner. In particular, since we normally interpret slopes with respect to a <em>unit increase</em> in the independent variable, and the fact that a dummy variable can only go up one unit (i.e., from a 0 to a 1), we therefore interpret a dummy variable accordingly.</p>
<p><span class="math display">\[\beta_4 = \frac{\Delta wage}{\Delta female}\]</span></p>
<blockquote>
<p>Holding education, tenure, and experience constant, a female earns a <span class="math inline">\(\beta_4\)</span> difference in wage relative to a male, on average</p>
</blockquote>
<p>Note that the dummy variable is constructed such that males receive a 0 while females receive a 1. This implies that <span class="math inline">\(\beta_4\)</span> will denote the average change in a female’s wage <em>relative</em> to a male’s wage. If <span class="math inline">\(\beta_4 &lt; 0\)</span>, then this would imply that a female’s average wage is less than a male’s.</p>
<p>The four independent variables are illustrated in the scatter plots below. Notice that even though the dummy variable takes on only two numbers by design, we can still see how it effectively <em>splits</em> the observations into the two groups.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-132-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>There is no difference between estimating quantitative and qualitative variables as far as R in concerned.</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="MLR.html#cb247-1" aria-hidden="true" tabindex="-1"></a>REG <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage<span class="sc">~</span>educ<span class="sc">+</span>exper<span class="sc">+</span>tenure,<span class="at">data=</span>wage1)</span>
<span id="cb247-2"><a href="MLR.html#cb247-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(REG)</span></code></pre></div>
<pre><code>## (Intercept)        educ       exper      tenure 
## -2.87273482  0.59896507  0.02233952  0.16926865</code></pre>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="MLR.html#cb249-1" aria-hidden="true" tabindex="-1"></a>REG <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage<span class="sc">~</span>educ<span class="sc">+</span>exper<span class="sc">+</span>tenure<span class="sc">+</span>female,<span class="at">data=</span>wage1)</span>
<span id="cb249-2"><a href="MLR.html#cb249-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(REG)</span></code></pre></div>
<pre><code>## (Intercept)        educ       exper      tenure      female 
## -1.56793870  0.57150477  0.02539587  0.14100506 -1.81085218</code></pre>
<p>Interpretations of the other independent variables are unchanged. However, <span class="math inline">\(\hat{\beta}_4 = -1.81\)</span> suggests the following:</p>
<blockquote>
<p>Holding education, tenure, and experience constant, a female earns $1.81 less in wages relative to a male, on average</p>
</blockquote>
<p>This states that we can compare two individuals with the same education, experience, and tenure levels but differ in gender and conclude that the male earns more.</p>
<p>Let us examine this further to show exactly why this type of qualitative variable is called an <em>intercept dummy variable</em>. Since the dummy variable can only take on the values 1 or 0, we can write down the PRF for both cases. In particular, the PRF for a male has female = 0 while the PRF for a female has female = 1.</p>
<p><span class="math display">\[Male: wage_i=\beta_0+\beta_1educ_i+\beta_2exper_i+\beta_3tenure_i+\varepsilon_i\]</span>
<span class="math display">\[Female: wage_i=(\beta_0+\beta_4)+\beta_1educ_i+\beta_2exper_i+\beta_3tenure_i+\varepsilon_i\]</span></p>
<p>Notice that <span class="math inline">\(\beta_4\)</span> does not appear in the PRF for males because the female variable equals 0, while it appears <em>alone</em> in the PRF for females because the female variable equals 1. After rearranging a bit, you can now see that the intercept term of the PRF for males is <span class="math inline">\(\beta_0\)</span> while the intercept term of the PRF for females is <span class="math inline">\((\beta_0+\beta_4)\)</span>. This illustrates that if you hold the other three independent variables constant, the difference between the wage rates of a male and female is <span class="math inline">\(\beta_4\)</span> on average. In other words, if you plug in the same numbers for education, experience, and tenure in the two PRFs above, then the difference in wages between men and women who share these traits will be <span class="math inline">\(\beta_4\)</span>.</p>
</div>
<div id="slope-dummy-variable" class="section level3" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Slope dummy variable</h3>
<p>While an intercept dummy variable is a very powerful modeling tool, it makes one glaring assumption. Consider the regression results above, namely the estimated slope coefficient with respect to tenure</p>
<p><span class="math display">\[\hat{\beta}_3 = 0.14\]</span></p>
<p>The interpretation of this slope coefficient is as follows:</p>
<blockquote>
<p>Holding education, experience, and gender constant, an individual will receive $0.14 more in wages for every additional year of tenure, on average.</p>
</blockquote>
<p>In particular, this states that a female receives the same annual raise than a male. This is an <em>assumption</em> of the model, because the model is incapable of differentiating the annual wage with respect to gender. We can extend the model to explicitly test this assumption with the use of a <em>slope dummy variable</em>.</p>
<p>A slope dummy variable is an example of an <em>interaction term</em>. In other words, it is a new variable that arises from taking the product of two variables. In this case, in order for us to examine the gender difference of tenure, we consider the product between female and tenure.</p>
<p><span class="math display">\[\begin{aligned}
wage_i = \;&amp;\beta_0+\beta_1educ_i+\beta_2exper_i+ \beta_3tenure_i+\beta_4female_i+... \\ &amp;\beta_5(tenure_i*female_i)+\varepsilon_i 
\end{aligned}\]</span></p>
<p>Like with our illustration of an intercept dummy, we can see what this PRF looks like for males and females.</p>
<p><span class="math display">\[Male: wage_i=\beta_0+\beta_1educ_i+\beta_2exper_i+\beta_3tenure_i+\varepsilon_i\]</span>
<span class="math display">\[Female: wage_i=(\beta_0+\beta_4)+\beta_1educ_i+\beta_2exper_i+(\beta_3+\beta_5)tenure_i+\varepsilon_i\]</span></p>
<p>For males, the PRF looks <em>exactly</em> as it does when we only considered an intercept dummy because both <span class="math inline">\(\beta_4\)</span> and <span class="math inline">\(\beta_5\)</span> drop out when <span class="math inline">\(female_i = 0\)</span>. For females, we can see the potential change in the intercept (as before), but we can now see a potential change in the slope with respect to tenure.</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="MLR.html#cb251-1" aria-hidden="true" tabindex="-1"></a>REG <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage<span class="sc">~</span>educ<span class="sc">+</span>exper<span class="sc">+</span>tenure<span class="sc">+</span></span>
<span id="cb251-2"><a href="MLR.html#cb251-2" aria-hidden="true" tabindex="-1"></a>            female<span class="sc">+</span>female<span class="sc">*</span>tenure,<span class="at">data=</span>wage1)</span>
<span id="cb251-3"><a href="MLR.html#cb251-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(REG)</span></code></pre></div>
<pre><code>##   (Intercept)          educ         exper        tenure        female tenure:female 
##   -2.00229568    0.58279061    0.02834532    0.17780235   -1.17787884   -0.14359567</code></pre>
<p>Our extended model now gives a better picture of the gender impact on wages.
<span class="math display">\[\hat{\beta}_4 = -1.18\]</span></p>
<blockquote>
<p>Holding all else constant, a female earns $1.18 less than a male on average.</p>
</blockquote>
<p>When considering the impact of tenure on wages, we could show the difference explicitly:</p>
<p><span class="math display">\[Males: \frac{\Delta wage}{\Delta tenure} = \hat{\beta}_3=0.18\]</span>
<span class="math display">\[Females: \frac{\Delta wage}{\Delta tenure} = \hat{\beta}_3+\hat{\beta}_5=0.18-0.14=0.04\]</span></p>
<p>The regression states that males receive an $0.18 increase in wages on average for every additional year in tenure (holding all else constant), while females receive only a $0.04 increase in wages on average.</p>
<p>Note that we could also consider slope dummy variables with respect to education as well as experience. You should do those on your own.</p>
</div>
<div id="what-if-there-are-more-than-two-categories" class="section level3" number="8.3.3">
<h3><span class="header-section-number">8.3.3</span> What if there are more than two categories?</h3>
<p>Since a dummy variable can take on either a zero or a one, it is perfectly designed to identify two categories. This might be fine for some variables like yes / no or win / lose, but what if a variable has more than two categories? Examples would be direct extensions of the above variables: yes / no / maybe or win / lose / draw.</p>
<p>The rule of thumb (to be explained in detail soon) is:</p>
<p><strong>A variable containing <span class="math inline">\(N\)</span> categories requires <span class="math inline">\(N-1\)</span> dummy variables.</strong></p>
<p>This rule actually applies to our standard case, because we can model <span class="math inline">\(N=2\)</span> categories with <span class="math inline">\(N-1=1\)</span> dummy variables. In our example above, we wanted to identify 2 categories of gender (male or female) so we needed 1 dummy variable. However, we need to take a little more care and follow additional steps when dealing with more than one category. Suppose we extended our gender characteristics to identify a third gender category (<em>other</em>) in order to account for individuals who do not subscribe to one of the two traditional categories. We will use this scenario to illustrate how our model gets extended.</p>
<ol style="list-style-type: decimal">
<li>Identify a <em>benchmark</em> category</li>
</ol>
<p>A benchmark category is one of the characteristics that the researcher identifies as the category that all other categories get compared against. In our gender example, suppose we choose <em>male</em> as our benchmark characteristic. You will find that this choice is arbitrary, but it may have implications.</p>
<ol start="2" style="list-style-type: decimal">
<li>Construct appropriate dummy variables</li>
</ol>
<p>Once the benchmark category has been established as male, we need two dummy variables: one that identifies individuals as <em>female</em> and one that identifies individuals as <em>other</em>.</p>
<p><span class="math display">\[female_i = 1 \mbox{ if female; } 0 \mbox{ if male or other }\]</span></p>
<p><span class="math display">\[other_i = 1 \mbox{ if other; } 0 \mbox{ if male or female }\]</span></p>
<p>Note that each dummy variable is still a switch that signals the presence or absence of a characteristic. However, when <strong>BOTH</strong> dummy variables are zero at the same time… you have your benchmark category. That is how you can identify three categories with only two dummy variables.</p>
<p>To illustrate, consider the original model restricting attention to intercept dummies.</p>
<p><span class="math display">\[wage_i=\beta_0+\beta_1educ_i+\beta_2exper_i+\beta_3tenure_i+\beta_4female_i +\beta_5other_i +\varepsilon_i\]</span></p>
<p>We can write down what the model looks like for each of our three categories:</p>
<p><span class="math display">\[Male: wage_i=\beta_0+\beta_1educ_i+\beta_2exper_i+\beta_3tenure_i+\varepsilon_i\]</span></p>
<p><span class="math display">\[Female: wage_i=(\beta_0+\beta_4)+\beta_1educ_i+\beta_2exper_i+\beta_3tenure_i+\varepsilon_i\]</span></p>
<p><span class="math display">\[Other: wage_i=(\beta_0+\beta_5)+\beta_1educ_i+\beta_2exper_i+\beta_3tenure_i+\varepsilon_i\]</span></p>
<p>When comparing these three equations, you can hopefully see how the benchmark category comes into play. The first equation is essentially the benchmark equation, indicating that <span class="math inline">\(\beta_0\)</span> is the intercept term for males. The second equation is for females, and shows how the intercept for females differs from males (given by <span class="math inline">\(\beta_4\)</span>). The third equation is for those identifying as other, and shows how the intercept for these individuals differs from males (given by <span class="math inline">\(\beta_5\)</span>). Note that all of the other slopes are <em>assumed</em> to be identical here (but we could consider slope dummies like above).</p>
<p>One detail about the application above worthy of mention is that the coefficients <span class="math inline">\(\beta_4\)</span> and <span class="math inline">\(\beta_5\)</span> show how each category compares to the benchmark category. We can test if these coefficients are significantly different from zero with standard hypothesis tests. For example:</p>
<p><span class="math display">\[H_0: \; \beta_4 = 0 \quad H_1: \; \beta_4 \neq 0\]</span></p>
<p>However, if we show that <span class="math inline">\(\beta_4\)</span> and <span class="math inline">\(\beta_5\)</span> were significantly different than zero, we can only conclude that females and individuals in the <em>other</em> category are treated differently than males. We cannot determine if <em>female</em> and <em>other</em> are significantly different from each other without a joint hypothesis test (examined below) or a choice of a new benchmark category.</p>
</div>
<div id="a-final-application" class="section level3" number="8.3.4">
<h3><span class="header-section-number">8.3.4</span> A Final Application</h3>
<p>Let us consider an in-depth application where dummy variables are essential for making time-series variables ready for analysis.</p>
<p>Consider the following time series data:</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="MLR.html#cb253-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readxl)</span>
<span id="cb253-2"><a href="MLR.html#cb253-2" aria-hidden="true" tabindex="-1"></a>AUTO <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;data/AUTO_SA.xlsx&quot;</span>)</span>
<span id="cb253-3"><a href="MLR.html#cb253-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb253-4"><a href="MLR.html#cb253-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(AUTO<span class="sc">$</span>INDEX,AUTO<span class="sc">$</span>AUTOSALE,</span>
<span id="cb253-5"><a href="MLR.html#cb253-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">main =</span> <span class="st">&quot;U.S. Retail Auto Sales ($bn)&quot;</span>,</span>
<span id="cb253-6"><a href="MLR.html#cb253-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Date&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Sales&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb253-7"><a href="MLR.html#cb253-7" aria-hidden="true" tabindex="-1"></a>xtick <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">length</span>(AUTO<span class="sc">$</span>INDEX), <span class="at">by =</span> <span class="dv">36</span>)</span>
<span id="cb253-8"><a href="MLR.html#cb253-8" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side=</span><span class="dv">1</span>, <span class="at">at=</span>xtick, <span class="at">labels =</span> <span class="cn">FALSE</span>)</span>
<span id="cb253-9"><a href="MLR.html#cb253-9" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span>xtick, <span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">3</span>],  </span>
<span id="cb253-10"><a href="MLR.html#cb253-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;1970&quot;</span>,<span class="st">&quot;1973&quot;</span>,<span class="st">&quot;1976&quot;</span>,<span class="st">&quot;1979&quot;</span>,<span class="st">&quot;1982&quot;</span>,<span class="st">&quot;1985&quot;</span>,</span>
<span id="cb253-11"><a href="MLR.html#cb253-11" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;1988&quot;</span>,<span class="st">&quot;1991&quot;</span>,<span class="st">&quot;1994&quot;</span>,<span class="st">&quot;1997&quot;</span>), </span>
<span id="cb253-12"><a href="MLR.html#cb253-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">pos =</span> <span class="dv">1</span>, <span class="at">xpd =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-135-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The figure illustrates retail automobile sales, denoted in billions of dollars, for the US between 1970 and 1998. As with most time series, this data is actually a combination of several components.</p>
<ul>
<li><p><strong>Trend</strong>: The long-term (i.e. average) increase or decrease in value over time.</p></li>
<li><p><strong>Seasonality</strong>: The repeating (i.e. predictable) short-term cycle in the series caused by the seasons or months of the year</p></li>
<li><p><strong>Random</strong>: The information in the series that is not due to a long-term trend or a short-term cyclical pattern is what we would actually like to explain.</p></li>
</ul>
<p>Lets us decompose this series in several steps to not only give us more exposure to dummy variables, but to also learn a bit more about time series data.</p>
<div id="make-a-nominal-series-a-real-series" class="section level4" number="8.3.4.1">
<h4><span class="header-section-number">8.3.4.1</span> Make a <em>Nominal</em> Series a <em>Real</em> Series</h4>
<p>The auto sales series above is known as a <em>nominal</em> series because the dollar values for each time period are expressed in the prices of that time period. For example, the data indicates that the US had $4.79 billion in auto sales in January 1970 and $47.4 billion in January 1998. We cannot say that auto sales increased by ten times during this time period, because the US also experience <em>inflation</em> during this time period. In particular, $4.79 billion is denoted in 1970 dollars while $47.4 billion is denoted in 1998 dollars. In order to remove any inflationary distortions from the data, we need to divide these numbers by some measure of how average prices have evolved. There are many ways of doing this, but a direct method is to use the <em>consumer price index</em> or CPI. The CPI tells us how average prices have evolved relative to a benchmark year that is set to 100 (or 1). If the CPI differs in a particular year, then we know how prices have changes relative to the benchmark year.</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="MLR.html#cb254-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(AUTO<span class="sc">$</span>INDEX,AUTO<span class="sc">$</span>CPI,</span>
<span id="cb254-2"><a href="MLR.html#cb254-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Consumer Price Index (1990 = 100)&quot;</span>,</span>
<span id="cb254-3"><a href="MLR.html#cb254-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Date&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Index&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb254-4"><a href="MLR.html#cb254-4" aria-hidden="true" tabindex="-1"></a>xtick <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">length</span>(AUTO<span class="sc">$</span>INDEX), <span class="at">by =</span> <span class="dv">36</span>)</span>
<span id="cb254-5"><a href="MLR.html#cb254-5" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side=</span><span class="dv">1</span>, <span class="at">at=</span>xtick, <span class="at">labels =</span> <span class="cn">FALSE</span>)</span>
<span id="cb254-6"><a href="MLR.html#cb254-6" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span>xtick, <span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">3</span>],  </span>
<span id="cb254-7"><a href="MLR.html#cb254-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;1970&quot;</span>,<span class="st">&quot;1973&quot;</span>,<span class="st">&quot;1976&quot;</span>,<span class="st">&quot;1979&quot;</span>,<span class="st">&quot;1982&quot;</span>,<span class="st">&quot;1985&quot;</span>,</span>
<span id="cb254-8"><a href="MLR.html#cb254-8" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;1988&quot;</span>,<span class="st">&quot;1991&quot;</span>,<span class="st">&quot;1994&quot;</span>,<span class="st">&quot;1997&quot;</span>), </span>
<span id="cb254-9"><a href="MLR.html#cb254-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">pos =</span> <span class="dv">1</span>, <span class="at">xpd =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-136-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The figure above illustrates the CPI where 1990 is denoted as the benchmark year (because it is set to 1). All other time periods now have prices calculated relative to the benchmark. For example, the CPI in January 1970 is <span class="math inline">\(0.30\)</span> which means that average prices were 70 percent lower than what they were in 1990.</p>
<p>We use the CPI to transform a nominal series into a real series. For example:</p>
<p><span class="math display">\[\mbox{Real Auto Sales} = \frac{\mbox{Nominal Auto Sales}}{\mbox{CPI}}\]</span></p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="MLR.html#cb255-1" aria-hidden="true" tabindex="-1"></a>AUTO<span class="sc">$</span>RAUTO <span class="ot">=</span> AUTO<span class="sc">$</span>AUTOSALE <span class="sc">/</span> AUTO<span class="sc">$</span>CPI</span>
<span id="cb255-2"><a href="MLR.html#cb255-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb255-3"><a href="MLR.html#cb255-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(AUTO<span class="sc">$</span>INDEX,AUTO<span class="sc">$</span>RAUTO,</span>
<span id="cb255-4"><a href="MLR.html#cb255-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">main =</span> <span class="st">&quot;U.S. Retail Auto Sales (1990 $bn)&quot;</span>,</span>
<span id="cb255-5"><a href="MLR.html#cb255-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Date&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Real Sales&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb255-6"><a href="MLR.html#cb255-6" aria-hidden="true" tabindex="-1"></a>xtick <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">length</span>(AUTO<span class="sc">$</span>INDEX), <span class="at">by =</span> <span class="dv">36</span>)</span>
<span id="cb255-7"><a href="MLR.html#cb255-7" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side=</span><span class="dv">1</span>, <span class="at">at=</span>xtick, <span class="at">labels =</span> <span class="cn">FALSE</span>)</span>
<span id="cb255-8"><a href="MLR.html#cb255-8" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span>xtick, <span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">3</span>],  </span>
<span id="cb255-9"><a href="MLR.html#cb255-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;1970&quot;</span>,<span class="st">&quot;1973&quot;</span>,<span class="st">&quot;1976&quot;</span>,<span class="st">&quot;1979&quot;</span>,<span class="st">&quot;1982&quot;</span>,<span class="st">&quot;1985&quot;</span>,</span>
<span id="cb255-10"><a href="MLR.html#cb255-10" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;1988&quot;</span>,<span class="st">&quot;1991&quot;</span>,<span class="st">&quot;1994&quot;</span>,<span class="st">&quot;1997&quot;</span>), </span>
<span id="cb255-11"><a href="MLR.html#cb255-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">pos =</span> <span class="dv">1</span>, <span class="at">xpd =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-137-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This figure now shows the <em>Real</em> US Auto Sales denoted in 1990 prices. For example, January 1970 experienced $16.15 billion in auto sales while January 1998 experienced $47.05. Now that these two numbers are both stated using the same price level, we can say that car sales increased by three times (not ten) over the time period.</p>
</div>
<div id="remove-a-trend" class="section level4" number="8.3.4.2">
<h4><span class="header-section-number">8.3.4.2</span> Remove a Trend</h4>
<p>Our real sales data still shows signs of both a trend and a seasonal cycle that need to be removed. Let us start by removing the trend.</p>
<p>Given that a trend is defined as the average change in a time series, we are technically attempting to identify (and remove) the average change in the series given a one-unit increase in time. Since this data is monthly, we are attempting to identify the average monthly change in the series. We can identify the trend with a regression equation.</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="MLR.html#cb256-1" aria-hidden="true" tabindex="-1"></a>AUTO<span class="sc">$</span>TREND <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">length</span>(AUTO<span class="sc">$</span>INDEX), <span class="at">by =</span> <span class="dv">1</span>)</span>
<span id="cb256-2"><a href="MLR.html#cb256-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb256-3"><a href="MLR.html#cb256-3" aria-hidden="true" tabindex="-1"></a>DTRND <span class="ot">&lt;-</span> <span class="fu">lm</span>(AUTO<span class="sc">$</span>RAUTO <span class="sc">~</span> AUTO<span class="sc">$</span>TREND)</span>
<span id="cb256-4"><a href="MLR.html#cb256-4" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(DTRND)</span></code></pre></div>
<pre><code>## (Intercept)  AUTO$TREND 
## 18.37195789  0.05942082</code></pre>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="MLR.html#cb258-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(AUTO<span class="sc">$</span>INDEX,AUTO<span class="sc">$</span>RAUTO,</span>
<span id="cb258-2"><a href="MLR.html#cb258-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">main =</span> <span class="st">&quot;U.S. Retail Auto Sales (1990 $bn)&quot;</span>,</span>
<span id="cb258-3"><a href="MLR.html#cb258-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Date&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Real Sales&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb258-4"><a href="MLR.html#cb258-4" aria-hidden="true" tabindex="-1"></a>xtick <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">length</span>(AUTO<span class="sc">$</span>INDEX), <span class="at">by =</span> <span class="dv">36</span>)</span>
<span id="cb258-5"><a href="MLR.html#cb258-5" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side=</span><span class="dv">1</span>, <span class="at">at=</span>xtick, <span class="at">labels =</span> <span class="cn">FALSE</span>)</span>
<span id="cb258-6"><a href="MLR.html#cb258-6" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span>xtick, <span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">3</span>],  </span>
<span id="cb258-7"><a href="MLR.html#cb258-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;1970&quot;</span>,<span class="st">&quot;1973&quot;</span>,<span class="st">&quot;1976&quot;</span>,<span class="st">&quot;1979&quot;</span>,<span class="st">&quot;1982&quot;</span>,<span class="st">&quot;1985&quot;</span>,</span>
<span id="cb258-8"><a href="MLR.html#cb258-8" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;1988&quot;</span>,<span class="st">&quot;1991&quot;</span>,<span class="st">&quot;1994&quot;</span>,<span class="st">&quot;1997&quot;</span>), </span>
<span id="cb258-9"><a href="MLR.html#cb258-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">pos =</span> <span class="dv">1</span>, <span class="at">xpd =</span> <span class="cn">TRUE</span>)</span>
<span id="cb258-10"><a href="MLR.html#cb258-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(AUTO<span class="sc">$</span>INDEX,<span class="fu">fitted</span>(DTRND), <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-138-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The code above does three things. First, it creates a variable called <em>TREND</em> which is simply an increasing list of numbers from 1 (the first observation) to 341 (the last observation). Each increase is an additional month. Second, it runs a regression where real auto sales is the dependent variable while trend is the only independent variable.</p>
<p><span class="math display">\[Real\;Sales_t = \beta_0 + \beta_1 \; Trend_t + \varepsilon_t\]</span></p>
<p>The slope coefficient with respect to the trend is <span class="math inline">\(0.059\)</span> which means that average auto sales increase by roughly <span class="math inline">\(0.06\)</span> billion 1990 dollars each month on average. Finally, it plots the real series as well as our calculated trend together. Notice how the <em>predicted</em> sales coming from the trend line is straight - indicating how this is only the expected sales for a particular month given information only on the evolution of time.</p>
<p>Comparing these two lines in the figure should give you an idea how the trend gets removed from a time series. If we want to remove the predictable change in a series over time, then we can subtract these numbers from the original series once we estimate the trend. Note however that this is already done for you, because the residual of the above regression is actually the information in auto sales that cannot be explained by the predictable evolution of time.</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="MLR.html#cb259-1" aria-hidden="true" tabindex="-1"></a>AUTO<span class="sc">$</span>RAUTO_DT <span class="ot">=</span> <span class="fu">residuals</span>(DTRND)</span>
<span id="cb259-2"><a href="MLR.html#cb259-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(AUTO<span class="sc">$</span>INDEX,AUTO<span class="sc">$</span>RAUTO_DT,</span>
<span id="cb259-3"><a href="MLR.html#cb259-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, </span>
<span id="cb259-4"><a href="MLR.html#cb259-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;U.S. Retail Auto Sales (1990 $bn, Dentrended)&quot;</span>,</span>
<span id="cb259-5"><a href="MLR.html#cb259-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;cyan&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Date&quot;</span>, </span>
<span id="cb259-6"><a href="MLR.html#cb259-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Real, Detrended Sales&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb259-7"><a href="MLR.html#cb259-7" aria-hidden="true" tabindex="-1"></a>xtick <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">length</span>(AUTO<span class="sc">$</span>INDEX), <span class="at">by =</span> <span class="dv">36</span>)</span>
<span id="cb259-8"><a href="MLR.html#cb259-8" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side=</span><span class="dv">1</span>, <span class="at">at=</span>xtick, <span class="at">labels =</span> <span class="cn">FALSE</span>)</span>
<span id="cb259-9"><a href="MLR.html#cb259-9" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span>xtick, <span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">3</span>],  </span>
<span id="cb259-10"><a href="MLR.html#cb259-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;1970&quot;</span>,<span class="st">&quot;1973&quot;</span>,<span class="st">&quot;1976&quot;</span>,<span class="st">&quot;1979&quot;</span>,<span class="st">&quot;1982&quot;</span>,<span class="st">&quot;1985&quot;</span>,</span>
<span id="cb259-11"><a href="MLR.html#cb259-11" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;1988&quot;</span>,<span class="st">&quot;1991&quot;</span>,<span class="st">&quot;1994&quot;</span>,<span class="st">&quot;1997&quot;</span>), </span>
<span id="cb259-12"><a href="MLR.html#cb259-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">pos =</span> <span class="dv">1</span>, <span class="at">xpd =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-139-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The above figure illustrates the detrended data, where negative numbers indicate that observations are <em>below trend</em> while positive numbers indicate that observations are <em>above trend</em>.</p>
</div>
<div id="remove-seasonality" class="section level4" number="8.3.4.3">
<h4><span class="header-section-number">8.3.4.3</span> Remove Seasonality</h4>
<p>The figure above still includes a seasonal component which needs to be removed. We will do this using dummy variables.</p>
<p>Identifying seasonality generally refers to the short-run average pattern observed in the series. Since this is monthly data, we would like to observe the average sales in each month. If this were quarterly series, we would like to observe the average sales in each <em>season</em> (summer, fall, winter, spring). We can identify these average amounts by using dummy variables to identify if each observation falls into a particular month.</p>
<p>The first step is to establish a benchmark month. This is essentially an arbitrary decision, so lets just go with December (i.e. the twelfth month of the year).</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="MLR.html#cb260-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(AUTO)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 9
##   INDEX  YEAR MONTH  DATE AUTOSALE   CPI RAUTO TREND RAUTO_DT
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1     1  1970     1 1970      4.79 0.297  16.2     1   -2.28 
## 2     2  1970     2 1970.     4.96 0.298  16.6     2   -1.88 
## 3     3  1970     3 1970.     5.64 0.300  18.8     3    0.256
## 4     4  1970     4 1970.     5.98 0.302  19.8     4    1.16 
## 5     5  1970     5 1970.     6.08 0.303  20.1     5    1.38 
## 6     6  1970     6 1970.     6.55 0.305  21.5     6    2.77</code></pre>
<p>Note that our dataset already has a variable called month which identifies 1 as January, 2 an February, etc. This will make the creation of dummy variables very easy.</p>
<p>Since we want to break this data into 12 categories, then we will need to construct 11 dummy variables. One dummy variable will deliver a 1 every time the observation is in January (0 elsewhere), one dummy variable will deliver a 1 every time the observation is in February (0 elsewhere), and so on. We can do this by hand (which is tedious), or we can use a new package called <em>fastDummies</em>.</p>
<p>If you are using <em>fastDummies</em> for the first time, you will want to install it:</p>
<pre><code>install.packages(&quot;fastDummies&quot;)</code></pre>
<p>This package is designed to accept a variable and construct dummy variables for however many categories it can identify. For example:</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="MLR.html#cb263-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fastDummies)</span>
<span id="cb263-2"><a href="MLR.html#cb263-2" aria-hidden="true" tabindex="-1"></a>AUTO <span class="ot">&lt;-</span> <span class="fu">dummy_cols</span>(AUTO, <span class="at">select_columns =</span> <span class="st">&#39;MONTH&#39;</span>)</span>
<span id="cb263-3"><a href="MLR.html#cb263-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(AUTO)</span></code></pre></div>
<pre><code>##  [1] &quot;INDEX&quot;    &quot;YEAR&quot;     &quot;MONTH&quot;    &quot;DATE&quot;     &quot;AUTOSALE&quot; &quot;CPI&quot;      &quot;RAUTO&quot;    &quot;TREND&quot;    &quot;RAUTO_DT&quot;
## [10] &quot;MONTH_1&quot;  &quot;MONTH_2&quot;  &quot;MONTH_3&quot;  &quot;MONTH_4&quot;  &quot;MONTH_5&quot;  &quot;MONTH_6&quot;  &quot;MONTH_7&quot;  &quot;MONTH_8&quot;  &quot;MONTH_9&quot; 
## [19] &quot;MONTH_10&quot; &quot;MONTH_11&quot; &quot;MONTH_12&quot;</code></pre>
<p>Note how the dataset <em>AUTO</em> now contains 21 variables when it previously contained 9. This is because the above lines of code created 12 new variables - a dummy variable for each month of the year (1-12). Since we are considering the 12th month as our benchmark, we simply do not include it in our regression.</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="MLR.html#cb265-1" aria-hidden="true" tabindex="-1"></a>DS <span class="ot">&lt;-</span> <span class="fu">lm</span>(RAUTO_DT <span class="sc">~</span> MONTH_1 <span class="sc">+</span> MONTH_2 <span class="sc">+</span> MONTH_3 <span class="sc">+</span> MONTH_4 <span class="sc">+</span></span>
<span id="cb265-2"><a href="MLR.html#cb265-2" aria-hidden="true" tabindex="-1"></a>           MONTH_5 <span class="sc">+</span> MONTH_6 <span class="sc">+</span> MONTH_7 <span class="sc">+</span> MONTH_8 <span class="sc">+</span> MONTH_9 <span class="sc">+</span></span>
<span id="cb265-3"><a href="MLR.html#cb265-3" aria-hidden="true" tabindex="-1"></a>           MONTH_10 <span class="sc">+</span> MONTH_11, <span class="at">data =</span> AUTO)</span>
<span id="cb265-4"><a href="MLR.html#cb265-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb265-5"><a href="MLR.html#cb265-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(DS)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RAUTO_DT ~ MONTH_1 + MONTH_2 + MONTH_3 + MONTH_4 + 
##     MONTH_5 + MONTH_6 + MONTH_7 + MONTH_8 + MONTH_9 + MONTH_10 + 
##     MONTH_11, data = AUTO)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.3877 -1.8444  0.4071  2.3868  9.4046 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -3.2222     0.6231  -5.172 4.04e-07 ***
## MONTH_1      -0.5227     0.8735  -0.598 0.550014    
## MONTH_2       0.5072     0.8735   0.581 0.561902    
## MONTH_3       5.2955     0.8735   6.062 3.68e-09 ***
## MONTH_4       4.9933     0.8735   5.716 2.44e-08 ***
## MONTH_5       6.1152     0.8735   7.001 1.44e-11 ***
## MONTH_6       6.1181     0.8811   6.943 2.05e-11 ***
## MONTH_7       4.8101     0.8811   5.459 9.45e-08 ***
## MONTH_8       4.6222     0.8811   5.246 2.79e-07 ***
## MONTH_9       2.8030     0.8811   3.181 0.001607 ** 
## MONTH_10      3.1100     0.8811   3.530 0.000476 ***
## MONTH_11      0.8048     0.8811   0.913 0.361724    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.297 on 329 degrees of freedom
## Multiple R-squared:  0.3468, Adjusted R-squared:  0.325 
## F-statistic: 15.88 on 11 and 329 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The dummy variable coefficients presented above show how the average auto sales for a particular month differ from the benchmark month of December. For example, June sales (<em>MONTH_6</em>) is the largest above trend on average with 6.12 billion, while January sales (<em>MONTH_1</em>) is actually lower than December average trend sales by 0.52 billion on average. However, note that the difference between June and December is significantly different from zero, while the difference between January and December is not.</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="MLR.html#cb267-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(AUTO<span class="sc">$</span>INDEX,AUTO<span class="sc">$</span>RAUTO_DT,</span>
<span id="cb267-2"><a href="MLR.html#cb267-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, </span>
<span id="cb267-3"><a href="MLR.html#cb267-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;U.S. Retail Auto Sales (1990 $bn, Dentrended)&quot;</span>,</span>
<span id="cb267-4"><a href="MLR.html#cb267-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;cyan&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Date&quot;</span>, </span>
<span id="cb267-5"><a href="MLR.html#cb267-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Real, Detrended Sales&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb267-6"><a href="MLR.html#cb267-6" aria-hidden="true" tabindex="-1"></a>xtick <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">length</span>(AUTO<span class="sc">$</span>INDEX), <span class="at">by =</span> <span class="dv">36</span>)</span>
<span id="cb267-7"><a href="MLR.html#cb267-7" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side=</span><span class="dv">1</span>, <span class="at">at=</span>xtick, <span class="at">labels =</span> <span class="cn">FALSE</span>)</span>
<span id="cb267-8"><a href="MLR.html#cb267-8" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span>xtick, <span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">3</span>],  </span>
<span id="cb267-9"><a href="MLR.html#cb267-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;1970&quot;</span>,<span class="st">&quot;1973&quot;</span>,<span class="st">&quot;1976&quot;</span>,<span class="st">&quot;1979&quot;</span>,<span class="st">&quot;1982&quot;</span>,<span class="st">&quot;1985&quot;</span>,</span>
<span id="cb267-10"><a href="MLR.html#cb267-10" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;1988&quot;</span>,<span class="st">&quot;1991&quot;</span>,<span class="st">&quot;1994&quot;</span>,<span class="st">&quot;1997&quot;</span>), </span>
<span id="cb267-11"><a href="MLR.html#cb267-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">pos =</span> <span class="dv">1</span>, <span class="at">xpd =</span> <span class="cn">TRUE</span>)</span>
<span id="cb267-12"><a href="MLR.html#cb267-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(AUTO<span class="sc">$</span>INDEX,<span class="fu">fitted</span>(DS),<span class="at">col =</span> <span class="st">&quot;gray&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-143-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The figure above compares the actual detrended series (composed of seasonal and random components) and the seasonal component estimated from our use of dummy variables. As with removing the trend, we can now remove the seasonal component by taking the difference between these two series or simply using the residuals of the regression (since this is the part of the series that cannot be explained by the repeating of months).</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="MLR.html#cb268-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(AUTO<span class="sc">$</span>INDEX,<span class="fu">residuals</span>(DS),</span>
<span id="cb268-2"><a href="MLR.html#cb268-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, </span>
<span id="cb268-3"><a href="MLR.html#cb268-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;U.S. Retail Auto Sales (1990 $bn, Detrended and SA)&quot;</span>, <span class="at">col =</span> <span class="st">&quot;purple&quot;</span>,</span>
<span id="cb268-4"><a href="MLR.html#cb268-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Date&quot;</span>, </span>
<span id="cb268-5"><a href="MLR.html#cb268-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Real, Detrended, and SA Sales&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb268-6"><a href="MLR.html#cb268-6" aria-hidden="true" tabindex="-1"></a>xtick <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">length</span>(AUTO<span class="sc">$</span>INDEX), <span class="at">by =</span> <span class="dv">36</span>)</span>
<span id="cb268-7"><a href="MLR.html#cb268-7" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side=</span><span class="dv">1</span>, <span class="at">at=</span>xtick, <span class="at">labels =</span> <span class="cn">FALSE</span>)</span>
<span id="cb268-8"><a href="MLR.html#cb268-8" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span>xtick, <span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">3</span>],  </span>
<span id="cb268-9"><a href="MLR.html#cb268-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;1970&quot;</span>,<span class="st">&quot;1973&quot;</span>,<span class="st">&quot;1976&quot;</span>,<span class="st">&quot;1979&quot;</span>,<span class="st">&quot;1982&quot;</span>,<span class="st">&quot;1985&quot;</span>,</span>
<span id="cb268-10"><a href="MLR.html#cb268-10" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;1988&quot;</span>,<span class="st">&quot;1991&quot;</span>,<span class="st">&quot;1994&quot;</span>,<span class="st">&quot;1997&quot;</span>), </span>
<span id="cb268-11"><a href="MLR.html#cb268-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">pos =</span> <span class="dv">1</span>, <span class="at">xpd =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-144-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This final figure illustrates the random component of US auto retail sales once we removed price distortions, a long-run trend, and a seasonal cycle. What remains is the component that cannot be explained by these predictable (and uninteresting) things - and this is exactly what analysts what to explain with other more interesting variables (e.g. interest rates, exchange rates, bond prices, etc.). Notice how you can make out the two recessions that occurred during the time frame quite easily.</p>
</div>
</div>
</div>
<div id="joint-hypothesis-tests" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Joint Hypothesis Tests</h2>
<div id="simple-hypothesis-tests" class="section level3" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> Simple Hypothesis Tests</h3>
<p>With respect to statistical inference, confidence intervals and <em>simple</em> hypothesis tests are performed in multiple regression models exactly the same way as in simple regression models. The <em>only</em> difference is that a simple regression model calculated probabilities using <span class="math inline">\(n-2\)</span> degrees of freedom, while a multiple regression calculates probabilities using <span class="math inline">\(n-k-1\)</span> degrees of freedom, where <span class="math inline">\(k\)</span> is the number of independent variables in the model. Note that the degrees of freedom are consistent across models - it’s just that a simple regression model has <span class="math inline">\(k=1\)</span> by default.</p>
</div>
<div id="simple-versus-joint-tests" class="section level3" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> Simple versus Joint Tests</h3>
<p>We have already considered all there is to know about <em>simple</em> hypothesis tests.</p>
<p><span class="math display">\[H_0: \beta = 0 \quad \text{versus} \quad H_A: \beta \neq 0\]</span></p>
<p>With the established (one-sided or two-sided) hypotheses, we were able to calculate a p-value and conclude. There is nothing more to it than that.</p>
<p>A simple hypothesis test follows the same constraints as how we interpret single coefficients: <em>all else equal</em>. In particular, when we conduct a simple hypothesis test, we must calculate a test statistic under the null while assuming that all other coefficients are unchanged. This might be fine under some circumstances, but what if we want to test the population values of multiple regression coefficients at the same time? Doing this requires going from simple hypothesis tests to <strong>joint</strong> hypothesis tests.</p>
<p>Joint hypothesis tests consider a stated null involving multiple PRF coefficients simultaneously. Consider the following general PRF:</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \varepsilon_i\]</span></p>
<p>A simple hypothesis test such as</p>
<p><span class="math display">\[H_0: \beta_1 = 0 \quad \text{versus} \quad H_A: \beta_1 \neq 0\]</span></p>
<p>is conducted under the assumption that <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\beta_3\)</span> are left to be whatever the data says they should be. In other words, a simple hypothesis test can only address a value for one coefficient at a time while being silent on all others.</p>
<p>A joint hypothesis states a null hypothesis that considers multiple PRF coefficients simultaneously. The statement in the null hypothesis can become quite sophisticated and test some very interesting statements.</p>
<p>For example, we can test if <em>all</em> population coefficients are equal to zero - which explicitly states that none of the independent variables are important.</p>
<p><span class="math display">\[H_0: \beta_1 = \beta_2 = \beta_3 = 0 \quad \text{versus} \quad H_A: \beta_1 \neq 0,\; \beta_2 \neq 0,\; \text{or} \; \beta_3 \neq 0\]</span></p>
<p>We don’t have to be so extreme and test that just two of the three coefficients are simultaneously zero.</p>
<p><span class="math display">\[H_0: \beta_1 = \beta_3 = 0 \quad \text{versus} \quad H_A: \beta_1 \neq 0\; \text{or} \; \beta_3 \neq 0\]</span></p>
<p>If we have a specific theory in mind, we could also test if PRF coefficients are simultaneously equal to specific (nonzero) numbers.</p>
<p><span class="math display">\[H_0: \beta_1 = 1 \; \text{or} \; \beta_3 = 4 \quad \text{versus} \quad H_A: \beta_1 \neq 1\; \text{or} \; \beta_3 \neq 4\]</span></p>
<p>Finally, we can test if PRF coefficients behave according to some relative measures. Instead of stating in the null that coefficients are equal to some specific number, we can state that they are equal (or opposite) to each other or they behave according to some mathematical condition.</p>
<p><span class="math display">\[H_0: \beta_1 = -\beta_3 \quad \text{versus} \quad H_A: \beta_1 \neq -\beta_3\]</span></p>
<p><span class="math display">\[H_0: \beta_1 + \beta_3 = 1 \quad \text{versus} \quad H_A: \beta_1 + \beta_3 \neq 1\]</span></p>
<p><span class="math display">\[H_0: \beta_1 + 5\beta_3 = 3 \quad \text{versus} \quad H_A: \beta_1 + 5\beta_3 \neq 3\]</span></p>
<p>As long as you can state a hypothesis involving multiple PRF coefficients in a linear expression, then we can test the hypothesis using a joint test. There are an infinite number of possibilities, so it is best to give you a couple of concrete examples to establish just how powerful these tests can be.</p>
<div id="application" class="section level4" number="8.4.2.1">
<h4><span class="header-section-number">8.4.2.1</span> Application</h4>
<p>One chapter of my PhD dissertation concluded with a single joint hypothesis test. The topic I was researching was the <em>Bank-Lending Channel of Monetary Policy Transmission</em>, which is a bunch of jargon dealing with how banks respond to changes in monetary policy established by the federal reserve. A paper from 1992 written by Ben Bernanke and Alan Blinder established that aggregate bank lending volume responded to changes in monetary policy (identified as movements in the Federal Funds Rate).<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> A simplified version of their model (below) considers the movement in bank lending as the dependent variable and the movement in the Fed Funds Rate as the dependent variable.</p>
<p><span class="math display">\[L_i = \beta_0 + \beta_1 FFR_i + \varepsilon_i\]</span></p>
<p>While this is a simplification of the model actually estimated, you can see that <span class="math inline">\(\beta_1\)</span> will concisely capture the change in bank lending given an increase in the Fed Funds Rate.</p>
<p><span class="math display">\[\beta_1 = \frac{\Delta L_i}{\Delta FFR_i}\]</span></p>
<p>Since an increase in the Federal Funds Rate indicates a tightening of monetary policy, the authors proposed a simple hypothesis test to show that an increase in the FFR delivers a decrease in bank lending.</p>
<p><span class="math display">\[H_0:\beta_1 \geq 0 \quad \text{versus} \quad H_A:\beta_1 &lt; 0\]</span></p>
<p>Their 1992 paper rejects the null hypothesis above, which gave them empirical evidence that bank lending responds to monetary policy changes. The bank lending channel was established!</p>
<p>My dissertation tested an implicit assumption of their model: <em>symmetry</em>.</p>
<p><span class="math display">\[\beta_1 = \frac{\Delta L_i}{\Delta FFR_i}\]</span></p>
<p>The interpretation of the slope of this regression works for both increases and decreases in the Fed Funds Rate. Assuming that <span class="math inline">\(\beta_1 &lt;0\)</span>, a one-unit increase in the FFR will deliver an expected decline of <span class="math inline">\(\beta_1\)</span> units of lending on average. However, it also states that a one-unit <em>decrease</em> in the FFR will deliver an expected <em>increase</em> of <span class="math inline">\(\beta_1\)</span> units of lending on average. This symmetry is baked into the model. The only way we can explicitly test this assumption is to extend the model and perform a joint hypothesis test.</p>
<p>Suppose we separated the FFR variable into increases in the interest rate and decreases in the interest rate.</p>
<p><span class="math display">\[FFR_i^+ = FFR_i &gt;0 \quad \text{(zero otherwise)}\]</span>
<span class="math display">\[FFR_i^- = FFR_i &lt;0 \quad \text{(zero otherwise)}\]</span></p>
<p>If we were to put both of these variables into a similar regression, then we could separate the change in lending from increases and decreases in the interest rate.</p>
<p><span class="math display">\[L_i = \beta_0 + \beta_1 FFR_i^+ + \beta_2 FFR_i^- + \varepsilon_i\]</span></p>
<p><span class="math display">\[\beta_1 = \frac{\Delta L_i}{\Delta FFR_i^+}, \quad \beta_2 = \frac{\Delta L_i}{\Delta FFR_i^-}\]</span></p>
<p>Notice that both <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are still hypothesized to be negative numbers. However, the first model imposed the assumption that they were the <em>same</em> negative number while this model allows them to be different. We can therefore test the hypothesis that they are the same number by performing the following joint hypothesis:</p>
<p><span class="math display">\[H_0: \beta_1=\beta_2 \quad \text{versus} \quad H_A: \beta_1 \neq \beta_2\]</span></p>
<p>In case you were curious, the null hypothesis get rejected and this provides evidence that the bank lending channel is indeed <em>asymmetric</em>. This implies that banks respond more to monetary tightenings than monetary expansions, which should make sense given all of the low amounts of bank lending in the post-global recession of 2008 despite interest rates being at all time lows.</p>
</div>
<div id="conducting-a-joint-hypothesis-test" class="section level4" number="8.4.2.2">
<h4><span class="header-section-number">8.4.2.2</span> Conducting a Joint Hypothesis Test</h4>
<p>A joint hypothesis test involves four steps:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate an <em>unrestricted</em> model</p></li>
<li><p>Impose the null hypothesis and estimate a <em>restricted</em> model</p></li>
<li><p>Construct a <em>test statistic under the null</em></p></li>
<li><p>Determine a p-value and conclude</p></li>
</ol>
<p><strong>1. Estimate an Unrestricted Model</strong></p>
<p>An analysis begins with a regression model that can adequately capture what you are setting out to uncover. In general terms, this is a model that doesn’t impose any serious assumptions on the way the world works so you can adequately test these assumptions. Suppose we have a hypothesis that two independent variables impact a dependent variable by the same quantitative degree. In that case, we need a model that does not impose this hypothesis.</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \varepsilon_i\]</span></p>
<p>The model above allows for the two independent variables to impact the dependent variable in whatever way the data sees fit. Since there is no imposition of the hypothesis on the model, or no restriction that the hypothesis be obeyed, then this model is called the <em>unrestricted</em> model.</p>
<p><strong>2. Estimate a Restricted Model</strong></p>
<p>A restricted model involves both the unrestricted model and the null hypothesis. If we wanted to test if the two slope hypotheses were the same, then our joint hypothesis is just like the one in the previous example:</p>
<p><span class="math display">\[H_0:\beta_1=\beta_2 \quad \text{versus} \quad H_1:\beta_1 \neq \beta_2\]</span></p>
<p>With the null hypothesis established, we now need to construct a <em>restricted</em> model which results from imposing the null hypothesis on the unrestricted model. In particular, staring with the unrestricted model and substituting the null, we get the following:</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \varepsilon_i\]</span></p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_2 X_{1i} + \beta_2 X_{2i} + \varepsilon_i\]</span></p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_2 (X_{1i} + X_{2i}) + \varepsilon_i\]</span></p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_2 \tilde{X}_{i} + \varepsilon_i \quad \text{where} \quad \tilde{X}_{i} = X_{1i} + X_{2i}\]</span></p>
<p>Imposing the null hypothesis restricts the two slope coefficients to be identical. If we construct the new variable <span class="math inline">\(\tilde{X}_i\)</span> according to how the model dictates, then we can use the new variable to estimate the <em>restricted</em> model.</p>
<p><strong>3. Construct a test statistic under the null</strong></p>
<p>Now that we have our unrestricted and restricted models estimated, the only two things we need from them are the <span class="math inline">\(R^2\)</span> values from each. We will denote the <span class="math inline">\(R^2\)</span> from the unrestricted model as the <em>unrestricted</em> <span class="math inline">\(R^2\)</span> or <span class="math inline">\(R^2_u\)</span>, and the <span class="math inline">\(R^2\)</span> from the restricted model as the <em>restricted</em> <span class="math inline">\(R^2\)</span> or <span class="math inline">\(R^2_r\)</span>.</p>
<p>These two pieces of information are used with <em>two</em> degrees of freedom measures to construct a test statistic under the null - which is conceptually similar to how we perform simple hypothesis tests. However, while simple hypothesis tests are performed assuming a Student’s t distribution, joint hypothesis tests are performed assuming an entirely new distribution: An F distribution.</p>
<p>Roughly speaking, an F distribution arises from taking the square of a t distribution. Since simple hypothesis tests deal with t distributions, and the joint hypothesis deals with <span class="math inline">\(R^2\)</span> values, you get the general idea. An F-statistic under the null is given by</p>
<p><span class="math display">\[F=\frac{(R^2_u - R^2_r)/m}{(1-R^2_u)/(n-k-1)} \sim F_{m,\;n-k-1}\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(R^2_u\)</span> is the unrestricted <span class="math inline">\(R^2\)</span> - the <span class="math inline">\(R^2\)</span> from the unrestricted model.</p></li>
<li><p><span class="math inline">\(R^2_r\)</span> is the restricted <span class="math inline">\(R^2\)</span> - the <span class="math inline">\(R^2\)</span> from the restricted model.</p></li>
<li><p><span class="math inline">\(m\)</span> is the numerator degrees of freedom - the number of restrictions imposed on the restricted model. In other words, count up the number of equal signs in the null hypothesis.</p></li>
<li><p><span class="math inline">\(n-k-1\)</span> is the denominator degrees of freedom - this is the degrees of freedom for a simple hypothesis test performed on the <em>unrestricted</em> model.</p></li>
</ul>
<p>In simple hypothesis tests, we constructed a t-statistic that is presumably drawn from a t-distribution. We are essentially doing the same thing here by constructing a F-statistic that is presumably drawn from a F-distribution.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-145-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The F-distribution has a few conceptual properties we should discuss.</p>
<p><strong>An F statistic is restricted to be non-negative.</strong></p>
<p>This should make sense because the expressions in both the numerator and denominator of our F-statistic calculation are both going to be non-negative. The numerator is always going to be non-negative because <span class="math inline">\(R^2_u \geq R^2_r\)</span>. In other words, the unrestricted model will always explain more or at least as much of the variation in the dependent variable as the restricted model does. When the two models explain the same amount of variation, then the <span class="math inline">\(R^2\)</span> values are the same and the numerator is zero. When the two models explain different amounts of variation, then this means that the restriction prevents the model from explaining as much of the variation in the dependent variable it otherwise would when not being restricted.</p>
<p><strong>The Rejection Region is Always in the Right Tail</strong></p>
<p>If we have <span class="math inline">\(R^2_u = R^2_r\)</span>, then this implies that the restricted model and the unrestricted model are explaining the same amount of variation in the dependent variable. Think hard about what this is saying. If both models have the same <span class="math inline">\(R^2\)</span>, then they are essentially <em>the same model</em>. One model is unrestricted meaning it can choose any values for coefficients it sees fit. The other model is restricted meaning we are forcing it to follow whatever is specified in the null. If these two models are the same, then the <em>restriction doesn’t matter</em>. In other words, the model is choosing the values under the null whether or not we are imposing the null. If that is the case, then the f-statistic will be equal to or close to zero.</p>
<p>If we have <span class="math inline">\(R^2_u &gt; R^2_r\)</span>, then this implies that the restriction imposed by the null hypothesis is hampering the model from explaining as much of the volatility in the dependent variable than it otherwise would have. The more <span class="math inline">\(R^2_u &gt; R^2_r\)</span>, the more <span class="math inline">\(F&gt;0\)</span>. Once this F-statistic under the null becomes large enough, we reject the null. This means that the difference between the unrestricted and restricted models is so large that we have evidence to state that the null hypothesis is simply not going on in the data. This implies that the rejection region in <em>always</em> in the right tail, and the p-value is always calculated from the right as well.</p>
<p><strong>4. Determine a P-value and Conclude</strong></p>
<p>Again, we establish a confidence level <span class="math inline">\(\alpha\)</span> as we would with any hypothesis test. This delivers an acceptable probability of a type I error and breaks the distribution into a rejection region and a non-rejection region.</p>
<p>For example, suppose you set <span class="math inline">\(\alpha = 0.05\)</span> and have <span class="math inline">\(m=2\)</span> and <span class="math inline">\(n-k-1 = 100\)</span>. This means that the non-rejection region will take up 95% of the area of the F-distribution with 2 and 100 degrees of freedom.</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="MLR.html#cb269-1" aria-hidden="true" tabindex="-1"></a>(Fcrit <span class="ot">&lt;-</span> <span class="fu">qf</span>(<span class="fl">0.95</span>,<span class="dv">2</span>,<span class="dv">100</span>))</span></code></pre></div>
<pre><code>## [1] 3.087296</code></pre>
<p>If an F-statistic is greater than 3.09 then we can reject the null of the joint hypothesis with at least 95% confidence.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-147-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As in any hypothesis test, we can also calculate a p-value. This will deliver the maximum confidence level at which we can reject the null.</p>
<pre><code>pf(q, df1, df2, lower.tail = TRUE)</code></pre>
<p>Notice that since the probability is calculated from the left by default (like the other commands), we can use the above code to automatically calculate <span class="math inline">\(1-p\)</span>.</p>
</div>
</div>
<div id="applications" class="section level3" number="8.4.3">
<h3><span class="header-section-number">8.4.3</span> Applications</h3>
<p>Lets consider two applications. The first application is not terribly interesting, but it will illustrate a joint hypothesis test that is <em>always</em> provided you free of charge with any set of regression results. The second application is more involved and delivers the true importance of joint tests.</p>
<div id="application-1-a-wage-application" class="section level4" number="8.4.3.1">
<h4><span class="header-section-number">8.4.3.1</span> Application 1: A wage application</h4>
<p>Suppose you are a consultant hired by a firm to help determine the underlying features of the current wage structure for their employees. You want to understand why some wage rates are different from others. Let our dependent variable be <em>wage</em> (the hourly wage of an individual employee) and the independent variables be given by…</p>
<ul>
<li><p><em>educ</em> be the total years of education of an individual employee</p></li>
<li><p><em>exper</em> be the total years of experience an individual employee had prior to starting with the company</p></li>
<li><p><em>tenure</em> is the number of years an employee has been working with the firm.</p></li>
</ul>
<p>The resulting PRF is given by…</p>
<p><span class="math display">\[wage_i=\beta_0+\beta_1educ_i+\beta_2exper_i+\beta_3tenure_i+\varepsilon_i\]</span></p>
<p>Suppose we wanted to test that none of these independent variables help explain movements in wages, so the resulting joint hypothesis would be</p>
<p><span class="math display">\[H_0: \beta_1 = \beta_2 = \beta_3 = 0 \quad \text{versus} \quad H_A: \beta_1 \neq 0, \; \beta_2 \neq 0, \; \text{or} \; \beta_3 \neq 0\]</span></p>
<p>The unrestricted model is one where each of the coefficients can be whatever number the data wants them to be.</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="MLR.html#cb272-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(wage1, <span class="at">package =</span> <span class="st">&quot;wooldridge&quot;</span>)</span>
<span id="cb272-2"><a href="MLR.html#cb272-2" aria-hidden="true" tabindex="-1"></a>UREG <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage<span class="sc">~</span>educ<span class="sc">+</span>exper<span class="sc">+</span>tenure,<span class="at">data=</span>wage1)</span>
<span id="cb272-3"><a href="MLR.html#cb272-3" aria-hidden="true" tabindex="-1"></a>(R2u <span class="ot">&lt;-</span> <span class="fu">summary</span>(UREG)<span class="sc">$</span>r.squared)</span></code></pre></div>
<pre><code>## [1] 0.3064224</code></pre>
<p>Our unrestricted model can explain roughly 30% of the variation in wages.</p>
<p>The next step is to estimate the restricted model - the model with the null hypothesis imposed. In this case you will notice that setting all slope coefficients to zero results in a rather strange looking model:</p>
<p><span class="math display">\[wage_i=\beta_0+\varepsilon_i\]</span></p>
<p>This model contains no independent variables. If you were to estimate this model, then the intercept term would return the average wage in the data set and the error term will simply be every deviation from the individual wage observations with it’s average value. Since it is impossible for the deterministic component of this model to explain <em>any</em> of the variation in wages, then this implies that the restricted <span class="math inline">\(R^2\)</span> is zero by definition. Note that this is only a special case because of what the restricted model looks like. There will be more interesting cases where the restricted <span class="math inline">\(R^2\)</span> will need to be calculated.</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="MLR.html#cb274-1" aria-hidden="true" tabindex="-1"></a>R2r <span class="ot">&lt;-</span> <span class="dv">0</span> <span class="co"># By definition</span></span></code></pre></div>
<p>Now that we have the restricted and unrestricted <span class="math inline">\(R^2\)</span>, we need the degrees of freedom to calculate an F-statistic under the null. The numerator degrees of freedom <span class="math inline">\((m)\)</span> denote how many restrictions we placed on the restricted model. Since the null hypothesis sets all three slope coefficients to zero, we consider this to be 3 restrictions. The denominator degrees of freedom <span class="math inline">\((n-k-1)\)</span> is taken directly from the unrestricted model. Since <span class="math inline">\(n=526\)</span> and we originally had 3 independent variables (<span class="math inline">\(k=3\)</span>), the denominator degrees of freedom is <span class="math inline">\(n-k-1=522\)</span>. We can now calculate our F statistic under the null as well as our p-value.</p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="MLR.html#cb275-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">=</span> <span class="dv">3</span>; n <span class="ot">=</span> <span class="dv">526</span>; k <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb275-2"><a href="MLR.html#cb275-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb275-3"><a href="MLR.html#cb275-3" aria-hidden="true" tabindex="-1"></a>(Fstat <span class="ot">&lt;-</span> ((R2u <span class="sc">-</span> R2r)<span class="sc">/</span>m)<span class="sc">/</span>((<span class="dv">1</span><span class="sc">-</span>R2u)<span class="sc">/</span>(n<span class="sc">-</span>k<span class="dv">-1</span>)))</span></code></pre></div>
<pre><code>## [1] 76.87317</code></pre>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="MLR.html#cb277-1" aria-hidden="true" tabindex="-1"></a>(Pval <span class="ot">&lt;-</span> <span class="fu">pf</span>(Fstat,m,n<span class="sc">-</span>k<span class="dv">-1</span>,<span class="at">lower.tail =</span> <span class="cn">FALSE</span>))</span></code></pre></div>
<pre><code>## [1] 3.405862e-41</code></pre>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="MLR.html#cb279-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="sc">-</span>Pval)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>Note that since our F-statistic is far from 0, we can reject the null with approximately 100% confidence (i.e. the p-value is essentially zero).</p>
<div id="what-can-we-conclude-from-this" class="section level5" number="8.4.3.1.1">
<h5><span class="header-section-number">8.4.3.1.1</span> What can we conclude from this?</h5>
<p>Since we rejected the null hypothesis, that means we have statistical evidence that the alternative hypothesis is true. However, take a look at the what the alternative hypothesis actually says. It says that <em>at least one</em> of the population coefficients are statistically different from zero. It doesn’t say which ones. It doesn’t say how many. That’s it…</p>
<p><strong>Is there a short cut?</strong></p>
<p>Remember that all regression results provide the simple hypothesis that each slope coefficient is equal to zero.</p>
<p><span class="math display">\[H_0: \beta=0 \quad \text{versus} \quad H_1: \beta \neq 0\]</span></p>
<p>All regression results also provide the joint hypothesis that all slope coefficients are equal to zero. You can see the result at the bottom of the summary page. The last line delivers the same F-statistic we calculated above as well as a p-value that is essentially zero.</p>
<p>Note that while this uninteresting joint hypothesis test is done by default. Other joint tests require a bit more work.</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="MLR.html#cb281-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(UREG)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = wage ~ educ + exper + tenure, data = wage1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.6068 -1.7747 -0.6279  1.1969 14.6536 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2.87273    0.72896  -3.941 9.22e-05 ***
## educ         0.59897    0.05128  11.679  &lt; 2e-16 ***
## exper        0.02234    0.01206   1.853   0.0645 .  
## tenure       0.16927    0.02164   7.820 2.93e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.084 on 522 degrees of freedom
## Multiple R-squared:  0.3064, Adjusted R-squared:  0.3024 
## F-statistic: 76.87 on 3 and 522 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<div id="application-2-constant-returns-to-scale" class="section level4" number="8.4.3.2">
<h4><span class="header-section-number">8.4.3.2</span> Application 2: Constant Returns to Scale</h4>
<p>Suppose you have data on the Gross Domestic Product (GDP) of a country as well as observations on two aggregate inputs to production: the nation’s capital stock (K) and aggregate labor supply (L). One popular regression to run in growth economics is to see if a nation’s aggregate production function possesses <em>constant returns to scale</em>. If it does, then if you scale up a nation’s inputs by a particular percentage, then you will get the exact same percentage increase in output (i.e., double the inputs results in double the outputs). This has implications for what the size an economy should be, but we won’t get into those details now.</p>
<p>The PRF is given by</p>
<p><span class="math display">\[lnGDP_i = \beta_0 + \beta_K \;lnK_i + \beta_L \;lnL_i + \varepsilon_i\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(lnGDP_i\)</span> is an observation of total output</p></li>
<li><p><span class="math inline">\(lnK_i\)</span> is an observation of total capital stock</p></li>
<li><p><span class="math inline">\(lnL_i\)</span> is an observation of total labor stock.</p></li>
</ul>
<p>These variables are actually in <em>logs</em>, but we will ignore that for now.</p>
<p>If we are testing for constant returns to scale, then we want to show that increasing all of the inputs by a certain amount will result in the same increase in output. Technical issues aside, this results in the following null hypothesis for a joint test:</p>
<p><span class="math display">\[H_0: \beta_K + \beta_L = 1 \quad \text{versus} \quad H_A: \beta_K + \beta_L \neq 1\]</span></p>
<p>We now have all we need to test for CRS:</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="MLR.html#cb283-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data...</span></span>
<span id="cb283-2"><a href="MLR.html#cb283-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb283-3"><a href="MLR.html#cb283-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readxl)</span>
<span id="cb283-4"><a href="MLR.html#cb283-4" aria-hidden="true" tabindex="-1"></a>CDdata <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;data/CDdata.xlsx&quot;</span>)</span>
<span id="cb283-5"><a href="MLR.html#cb283-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb283-6"><a href="MLR.html#cb283-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Run unrestricted model, get R^2...</span></span>
<span id="cb283-7"><a href="MLR.html#cb283-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb283-8"><a href="MLR.html#cb283-8" aria-hidden="true" tabindex="-1"></a>UREG <span class="ot">&lt;-</span> <span class="fu">lm</span>(lnGDP <span class="sc">~</span> lnK <span class="sc">+</span> lnL, <span class="at">data =</span> CDdata)</span>
<span id="cb283-9"><a href="MLR.html#cb283-9" aria-hidden="true" tabindex="-1"></a>(R2u <span class="ot">&lt;-</span> <span class="fu">summary</span>(UREG)<span class="sc">$</span>r.squared)</span></code></pre></div>
<pre><code>## [1] 0.9574247</code></pre>
<p>The unrestricted model can explain around 96% of the variation in the dependent variable. For us to determine how much the restricted model can explain, we first need to see exactly what the restriction does to our model. Starting from the unrestricted model, imposing the restriction delivers the following:</p>
<p><span class="math display">\[lnGDP_i = \beta_0 + \beta_K \; lnK_i + \beta_L \; lnL_i + \varepsilon_i\]</span>
<span class="math display">\[lnGDP_i = \beta_0 + (1 - \beta_L) \; lnK_i + \beta_L \; lnL_i + \varepsilon_i\]</span></p>
<p><span class="math display">\[(lnGDP_i - lnK_i) = \beta_0 + \beta_L \; (lnL_i - lnK_i) + \varepsilon_i\]</span>
<span class="math display">\[\tilde{Y}_i = \beta_0 + \beta_L \; \tilde{X}_i + \varepsilon_i\]</span>
where
<span class="math display">\[\tilde{Y}_i=lnGDP_i - lnK_i \quad \text{and} \quad \tilde{X}_i=lnL_i - lnK_i\]</span></p>
<p>Notice how these derivations deliver exactly how the variables of the model need to be transformed and what restricted model needs to be estimated.</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="MLR.html#cb285-1" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> CDdata<span class="sc">$</span>lnGDP <span class="sc">-</span> CDdata<span class="sc">$</span>lnK</span>
<span id="cb285-2"><a href="MLR.html#cb285-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> CDdata<span class="sc">$</span>lnL <span class="sc">-</span> CDdata<span class="sc">$</span>lnK</span>
<span id="cb285-3"><a href="MLR.html#cb285-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb285-4"><a href="MLR.html#cb285-4" aria-hidden="true" tabindex="-1"></a>RREG <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y<span class="sc">~</span>X)</span>
<span id="cb285-5"><a href="MLR.html#cb285-5" aria-hidden="true" tabindex="-1"></a>(R2r <span class="ot">&lt;-</span> <span class="fu">summary</span>(RREG)<span class="sc">$</span>r.squared)</span></code></pre></div>
<pre><code>## [1] 0.9370283</code></pre>
<p>The restricted model can explain roughly 94% of the variation in the dependent variable. To see if this reduction in <span class="math inline">\(R^2\)</span> is enough to reject the null hypothesis, we need to calculate an F-statistic. The numerator degrees of freedom is <span class="math inline">\(m=1\)</span> because there is technically only one restriction in the null. The denominator degrees of freedom uses <span class="math inline">\(n=24\)</span> and <span class="math inline">\(k=2\)</span>.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="MLR.html#cb287-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">=</span> <span class="dv">1</span>; n <span class="ot">=</span> <span class="dv">24</span>; k <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb287-2"><a href="MLR.html#cb287-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-3"><a href="MLR.html#cb287-3" aria-hidden="true" tabindex="-1"></a>(Fstat <span class="ot">&lt;-</span> ((R2u <span class="sc">-</span> R2r)<span class="sc">/</span>m)<span class="sc">/</span>((<span class="dv">1</span><span class="sc">-</span>R2u)<span class="sc">/</span>(n<span class="sc">-</span>k<span class="dv">-1</span>)))</span></code></pre></div>
<pre><code>## [1] 10.0604</code></pre>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="MLR.html#cb289-1" aria-hidden="true" tabindex="-1"></a>(Pval <span class="ot">&lt;-</span> <span class="fu">pf</span>(Fstat,m,n<span class="sc">-</span>k<span class="dv">-1</span>,<span class="at">lower.tail =</span> <span class="cn">FALSE</span>))</span></code></pre></div>
<pre><code>## [1] 0.004594084</code></pre>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="MLR.html#cb291-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="sc">-</span>Pval)</span></code></pre></div>
<pre><code>## [1] 0.9954059</code></pre>
<p>As in the previous application, we received a very high F-statistic and a very low p-value. This means we <em>reject</em> the hypothesis that this country has an aggregate production function that exhibits constant returns to scale with slightly over 99.5% confidence.</p>

</div>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>Bernanke, B., &amp; Blinder, A. (1992). The Federal Funds Rate and the Channels of Monetary Transmission. <em>The American Economic Review</em>, 82(4), 901-921.<a href="MLR.html#fnref9" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="SLR.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Advanced.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sjdres/MBA8350_Companion/edit/master/08-MLR.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/sjdres/MBA8350_Companion/blob/master/08-MLR.Rmd",
"text": null
},
"download": ["bookdownproj.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
